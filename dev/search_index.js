var documenterSearchIndex = {"docs":
[{"location":"Benchmark/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"Benchmark/#Vs-Torchdiffeq-1-million-and-less-ODEs","page":"Benchmarks","title":"Vs Torchdiffeq 1 million and less ODEs","text":"","category":"section"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"A raw ODE solver benchmark showcases >30x performance advantage for DifferentialEquations.jl for ODEs ranging in size from 3 to nearly 1 million.","category":"page"},{"location":"Benchmark/#Vs-Torchdiffeq-on-neural-ODE-training","page":"Benchmarks","title":"Vs Torchdiffeq on neural ODE training","text":"","category":"section"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"A training benchmark using the spiral ODE from the original neural ODE paper demonstrates a 100x performance advantage for DiffEqFlux in training neural ODEs.","category":"page"},{"location":"Benchmark/#Vs-torchsde-on-small-SDEs","page":"Benchmarks","title":"Vs torchsde on small SDEs","text":"","category":"section"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Using the code from torchsde's README we demonstrated a >70,000x performance advantage over torchsde. Further benchmarking is planned but was found to be computationally infeasible for the time being.","category":"page"},{"location":"Benchmark/#A-bunch-of-adjoint-choices-on-neural-ODEs","page":"Benchmarks","title":"A bunch of adjoint choices on neural ODEs","text":"","category":"section"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Quick summary:","category":"page"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"BacksolveAdjoint can be the fastest (but use with caution!); about 25% faster\nUsing ZygoteVJP is faster than other vjp choices with FastDense due to the overloads","category":"page"},{"location":"Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, DiffEqSensitivity,\n      Zygote, BenchmarkTools, Random\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nRandom.seed!(100)\np = initial_params(dudt2)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction loss_neuralode(p)\n    pred = Array(prob_neuralode(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode,p)\n# 2.709 ms (56506 allocations: 6.62 MiB)\n\nprob_neuralode_interpolating = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_interpolating(p)\n    pred = Array(prob_neuralode_interpolating(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating,p)\n# 5.501 ms (103835 allocations: 2.57 MiB)\n\nprob_neuralode_interpolating_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_interpolating_zygote(p)\n    pred = Array(prob_neuralode_interpolating_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating_zygote,p)\n# 2.899 ms (56150 allocations: 6.61 MiB)\n\nprob_neuralode_backsolve = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_backsolve(p)\n    pred = Array(prob_neuralode_backsolve(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve,p)\n# 4.871 ms (85855 allocations: 2.20 MiB)\n\nprob_neuralode_quad = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_quad(p)\n    pred = Array(prob_neuralode_quad(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_quad,p)\n# 11.748 ms (79549 allocations: 3.87 MiB)\n\nprob_neuralode_backsolve_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=TrackerVJP()))\n\nfunction loss_neuralode_backsolve_tracker(p)\n    pred = Array(prob_neuralode_backsolve_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_tracker,p)\n# 27.604 ms (186143 allocations: 12.22 MiB)\n\nprob_neuralode_backsolve_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_backsolve_zygote(p)\n    pred = Array(prob_neuralode_backsolve_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_zygote,p)\n# 2.091 ms (49883 allocations: 6.28 MiB)\n\nprob_neuralode_backsolve_false = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(false)))\n\nfunction loss_neuralode_backsolve_false(p)\n    pred = Array(prob_neuralode_backsolve_false(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_false,p)\n# 4.822 ms (9956 allocations: 1.03 MiB)\n\nprob_neuralode_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=TrackerAdjoint())\n\nfunction loss_neuralode_tracker(p)\n    pred = Array(prob_neuralode_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_tracker,p)\n# 12.614 ms (76346 allocations: 3.12 MiB)","category":"page"},{"location":"layers/CNFLayer/#CNF-Layer-Functions","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"DeterministicCNF\nFFJORD\nFFJORDDistribution","category":"page"},{"location":"layers/CNFLayer/#DiffEqFlux.DeterministicCNF","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.DeterministicCNF","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a direct computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\n!!!note     This layer has been deprecated in favour of FFJORD. Use FFJORD with monte_carlo=false instead.\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nDeterministicCNF(model, tspan, basedist=nothing, monte_carlo=false, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a stochastic approach [2] for the computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nFFJORD(model, basedist=nothing, monte_carlo=false, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORDDistribution","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by rand or estimate densities by pdf or logpdf (from Distributions.jl).\n\nArguments:\n\nmodel: A FFJORD instance\nregularize: Whether we use regularization (default: false)\nmonte_carlo: Whether we use monte carlo (default: true)\n\n\n\n\n\n","category":"type"},{"location":"examples/multiple_shooting/#Multiple-Shooting","page":"Multiple Shooting","title":"Multiple Shooting","text":"","category":"section"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"In Multiple Shooting, the training data is split into overlapping intervals. The solver is then trained on individual intervals. If the end conditions of any interval coincide with the initial conditions of the next immediate interval, then the joined/combined solution is same as solving on the whole dataset (without splitting).","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"To ensure that the overlapping part of two consecutive intervals coincide, we add a penalizing term, continuity_term * absolute_value_of(prediction of last point of group i - prediction of first point of group i+1), to the loss.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Note that the continuity_term should have a large positive value to add high penalties in case the solver predicts discontinuous values.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The following is a working demo, using Multiple Shooting","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"using DiffEqFlux, DifferentialEquations, Plots\nusing DiffEqFlux: group_ranges\n\n# Define initial conditions and time steps\ndatasize = 30\nu0 = Float32[2.0, 0.0]\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\n\n# Get the data\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\n# Define the Neural Network\nnn = FastChain((x, p) -> x.^3,\n                  FastDense(2, 16, tanh),\n                  FastDense(16, 2))\np_init = initial_params(nn)\n\nneuralode = NeuralODE(nn, tspan, Tsit5(), saveat = tsteps)\nprob_node = ODEProblem((u,p,t)->nn(u,p), u0, tspan, p_init)\n\n\nfunction plot_multiple_shoot(plt, preds, group_size)\n\tstep = group_size-1\n\tranges = group_ranges(datasize, group_size)\n\n\tfor (i, rg) in enumerate(ranges)\n\t\tplot!(plt, tsteps[rg], preds[i][1,:], markershape=:circle, label=\"Group $(i)\")\n\tend\nend\n\n# Animate training\nanim = Animation()\ncallback = function (p, l, preds; doplot = true)\n  display(l)\n  if doplot\n\t# plot the original data\n\tplt = scatter(tsteps, ode_data[1,:], label = \"Data\")\n\n\t# plot the different predictions for individual shoot\n\tplot_multiple_shoot(plt, preds, group_size)\n\n    frame(anim)\n    display(plot(plt))\n  end\n  return false\nend\n\n# Define parameters for Multiple Shooting\ngroup_size = 3\ncontinuity_term = 200\n\nfunction loss_function(data, pred)\n\treturn sum(abs2, data - pred)\nend\n\nfunction loss_multiple_shooting(p)\n    return multiple_shoot(p, ode_data, tsteps, prob_node, loss_function, Tsit5(),\n                          group_size; continuity_term)\nend\n\nres_ms = DiffEqFlux.sciml_train(loss_multiple_shooting, p_init,\n                                cb = callback)\ngif(anim, \"multiple_shooting.gif\", fps=15)\n","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here's the animation that we get from above","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic) The connected lines show the predictions of each group (Notice that there are overlapping points as well. These are the points we are trying to coincide.)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here is an output with group_size = 30 (which is same as solving on the whole interval without splitting also called single shooting)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic_single_shoot3)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"It is clear from the above picture, a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minima.","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Bayesian-Neural-ODEs:-SGLD","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"Recently, Neural Ordinary Differential Equations has emerged as a powerful framework for modeling physical simulations without explicitly defining the ODEs governing the system, but learning them via machine learning. However, the question: Can Bayesian learning frameworks be integrated with Neural ODEs to robustly quantify the uncertainty in the weights of a Neural ODE? remains unanswered.","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"In this tutorial, a working example of the Bayesian Neural ODE: SGLD sampler is shown. SGLD stands for Stochastic Langevin Gradient Descent.","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"For an introduction to SGLD, please refer to Introduction to SGLD in Julia","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"For more details regarding Bayesian Neural ODEs, please refer to Bayesian Neural Ordinary Differential Equations.","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Copy-Pasteable-Code","page":"Bayesian Neural ODEs: SGLD","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"Before getting to the explanation, here's some code to start with. We will follow with a full explanation of the definition and training process:","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"using DiffEqFlux, DifferentialEquations, Flux\nusing Plots, StatsPlots\n\nu0 = Float32[1., 1.]\np = [1.5, 1., 3., 1.]\ndatasize = 45\ntspan = (0.0f0, 14.f0)\ntsteps = tspan[1]:0.1:tspan[2]\n\nfunction lv(u, p, t)\n    x, y = u\n    α, β, γ, δ = p\n    dx = α*x - β*x*y\n    dy = δ*x*y - γ*y\n    du = [dx, dy]\nend\n\ntrueodeprob = ODEProblem(lv, u0, tspan, p)\node_data = Array(solve(trueodeprob, Tsit5(), saveat = tsteps))\ny_train = ode_data[:, 1:35]\n\ndudt = FastChain(FastDense(2, 50, tanh), FastDense(50, 2))\nprob_node = NeuralODE(dudt, (0., 14.), Tsit5(), saveat = tsteps)\ntrain_prob = NeuralODE(dudt, (0., 3.5), Tsit5(), saveat = tsteps[1:35])\n\nfunction predict(p)\n    Array(train_prob(u0, p))\nend\n\nfunction loss(p)\n    sum(abs2, y_train .- predict(p))\nend\n\nsgld(∇L, θᵢ, t, a = 2.5e-3, b = 0.05, γ = 0.35) = begin\n    ϵ = a*(b + t)^-γ\n    η = ϵ.*randn(size(θᵢ))\n    Δθᵢ = .5ϵ*∇L + η\n    θᵢ .-= Δθᵢ\nend\n\nparameters = []\nlosses = Float64[]\ngrad_norm = Float64[]\n\nθ = deepcopy(prob_node.p)\n@time for t in 1:45000\n    grad = gradient(loss, θ)[1]\n    sgld(grad, θ, t)\n    tmp = deepcopy(θ)\n    append!(losses, loss(θ))\n    append!(grad_norm, sum(abs2, grad))\n    append!(parameters, [tmp])\n    println(loss(θ))\nend\nplot(losses, yscale = :log10)\nplot(grad_norm, yscale =:log10)\n\nusing StatsPlots\nsampled_par = parameters[43000: 45000]\n\n\n##################### PLOTS: LOSSES ###############\n\nsampled_loss = [loss(p) for p in sampled_par]\ndensity(sampled_loss)\n\n#################### RETRODICTED PLOTS - TIME SERIES AND CONTOUR PLOTS ####################\n\n_, i_min = findmin(sampled_loss)\n\nplt = scatter(tsteps,ode_data[1,:], colour = :blue, label = \"Data: u1\", ylim = (-.5, 10.))\nscatter!(plt, tsteps, ode_data[2,:], colour = :red, label = \"Data: u2\")\nphase_plt = scatter(ode_data[1,:], ode_data[2,:], colour = :red, label = \"Data\", xlim = (-.25, 7.), ylim = (-2., 6.5))\n\nfor p in sampled_par\n    s = prob_node(u0, p)\n    plot!(plt, tsteps[1:35], s[1,1:35], colour = :blue, lalpha = 0.04, label =:none)\n    plot!(plt, tsteps[35:end], s[1, 35:end], colour =:purple, lalpha = 0.04, label =:none)\n    plot!(plt, tsteps[1:35], s[2,1:35], colour = :red, lalpha = 0.04, label=:none)\n    plot!(plt, tsteps[35:end], s[2,35:end], colour = :purple, lalpha = 0.04, label=:none)\n    plot!(phase_plt, s[1,1:35], s[2,1:35], colour =:red, lalpha = 0.04, label=:none)\n    plot!(phase_plt, s[1,35:end], s[2, 35:end], colour = :purple, lalpha = 0.04, label=:none)\nend\n\nplt\nphase_plt\nplot!(plt, [3.5], seriestype =:vline, colour = :green, linestyle =:dash,label = \"Training Data End\")\n\nbestfit = prob_node(u0, sampled_par[i_min])\nplot(bestfit)\n\n\nplot!(plt, tsteps[1:35], bestfit[2, 1:35], colour =:black, label = \"Training: Best fit prediction\")\nplot!(plt, tsteps[35:end], bestfit[2, 35:end], colour =:purple, label = \"Forecasting: Best fit prediction\")\nplot!(plt, tsteps[1:35], bestfit[1, 1:35], colour =:black, label = :none)\nplot!(plt, tsteps[35:end], bestfit[1, 35:end], colour =:purple, label = :none)\n\nplot!(phase_plt,bestfit[1,1:40], bestfit[2, 1:40], colour = :black, label = \"Training: Best fit prediction\")\nplot!(phase_plt,bestfit[1, 40:end], bestfit[2, 40:end], colour = :purple, label = \"Forecasting: Best fit prediction\")\n\nsavefig(plt, \"C:/Users/16174/Desktop/Julia Lab/MSML2021/BayesianNODE_SGLD_Plot1.png\")\nsavefig(phase_plt, \"C:/Users/16174/Desktop/Julia Lab/MSML2021/BayesianNODE_SGLD_Plot2.png\")\n","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"Time Series Plots:","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"Contour Plots:","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Explanation","page":"Bayesian Neural ODEs: SGLD","title":"Explanation","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/#Step1:-Get-the-data-from-the-Lotka-Volterra-ODE-example","page":"Bayesian Neural ODEs: SGLD","title":"Step1: Get the data from the Lotka Volterra ODE example","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"u0 = Float32[1., 1.]\np = [1.5, 1., 3., 1.]\ndatasize = 45\ntspan = (0.0f0, 14.f0)\ntsteps = tspan[1]:0.1:tspan[2]\n\nfunction lv(u, p, t)\n    x, y = u\n    α, β, γ, δ = p\n    dx = α*x - β*x*y\n    dy = δ*x*y - γ*y\n    du = [dx, dy]\nend\n\ntrueodeprob = ODEProblem(lv, u0, tspan, p)\node_data = Array(solve(trueodeprob, Tsit5(), saveat = tsteps))\ny_train = ode_data[:, 1:35]\n","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Step2:-Define-the-Neural-ODE-architecture.-Note-that-this-step-potentially-offers-a-lot-of-flexibility-in-the-number-of-layers/-number-of-units-in-each-layer.","page":"Bayesian Neural ODEs: SGLD","title":"Step2: Define the Neural ODE architecture. Note that this step potentially offers a lot of flexibility in the number of layers/ number of units in each layer.","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"dudt = FastChain(FastDense(2, 50, tanh), FastDense(50, 2))\nprob_node = NeuralODE(dudt, (0., 14.), Tsit5(), saveat = tsteps)\ntrain_prob = NeuralODE(dudt, (0., 3.5), Tsit5(), saveat = tsteps[1:35])","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Step3:-Define-the-loss-function-for-the-Neural-ODE.","page":"Bayesian Neural ODEs: SGLD","title":"Step3: Define the loss function for the Neural ODE.","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"function predict(p)\n    Array(train_prob(u0, p))\nend\n\nfunction loss(p)\n    sum(abs2, y_train .- predict(p))\nend","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Step4:-Now-we-start-integrating-the-Stochastic-Langevin-Gradient-Descent(SGLD)-framework.","page":"Bayesian Neural ODEs: SGLD","title":"Step4: Now we start integrating the Stochastic Langevin Gradient Descent(SGLD) framework.","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"The SGLD (Stochastic Langevin Gradient Descent) sampler is seen to have a better performance than NUTS whose tutorial is also shown in a separate document. Have a look at https://sebastiancallh.github.io/post/langevin/ for a quick introduction to SGLD.","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"Note that we sample from the last 2000 iterations.","category":"page"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"sgld(∇L, θᵢ, t, a = 2.5e-3, b = 0.05, γ = 0.35) = begin\n    ϵ = a*(b + t)^-γ\n    η = ϵ.*randn(size(θᵢ))\n    Δθᵢ = .5ϵ*∇L + η\n    θᵢ .-= Δθᵢ\nend\n\nparameters = []\nlosses = Float64[]\ngrad_norm = Float64[]\n\nθ = deepcopy(prob_node.p)\n@time for t in 1:45000\n    grad = gradient(loss, θ)[1]\n    sgld(grad, θ, t)\n    tmp = deepcopy(θ)\n    append!(losses, loss(θ))\n    append!(grad_norm, sum(abs2, grad))\n    append!(parameters, [tmp])\n    println(loss(θ))\nend\nplot(losses, yscale = :log10)\nplot(grad_norm, yscale =:log10)\n\nusing StatsPlots\nsampled_par = parameters[43000: 45000]","category":"page"},{"location":"examples/BayesianNODE_SGLD/#Step5:-Plot-Retrodicted-Plots-(Estimation-and-Forecasting).","page":"Bayesian Neural ODEs: SGLD","title":"Step5: Plot Retrodicted Plots (Estimation and Forecasting).","text":"","category":"section"},{"location":"examples/BayesianNODE_SGLD/","page":"Bayesian Neural ODEs: SGLD","title":"Bayesian Neural ODEs: SGLD","text":"################### RETRODICTED PLOTS - TIME SERIES AND CONTOUR PLOTS ####################\n\n_, i_min = findmin(sampled_loss)\n\nplt = scatter(tsteps,ode_data[1,:], colour = :blue, label = \"Data: u1\", ylim = (-.5, 10.))\nscatter!(plt, tsteps, ode_data[2,:], colour = :red, label = \"Data: u2\")\nphase_plt = scatter(ode_data[1,:], ode_data[2,:], colour = :red, label = \"Data\", xlim = (-.25, 7.), ylim = (-2., 6.5))\n\nfor p in sampled_par\n    s = prob_node(u0, p)\n    plot!(plt, tsteps[1:35], s[1,1:35], colour = :blue, lalpha = 0.04, label =:none)\n    plot!(plt, tsteps[35:end], s[1, 35:end], colour =:purple, lalpha = 0.04, label =:none)\n    plot!(plt, tsteps[1:35], s[2,1:35], colour = :red, lalpha = 0.04, label=:none)\n    plot!(plt, tsteps[35:end], s[2,35:end], colour = :purple, lalpha = 0.04, label=:none)\n    plot!(phase_plt, s[1,1:35], s[2,1:35], colour =:red, lalpha = 0.04, label=:none)\n    plot!(phase_plt, s[1,35:end], s[2, 35:end], colour = :purple, lalpha = 0.04, label=:none)\nend\n\nplt\nphase_plt\nplot!(plt, [3.5], seriestype =:vline, colour = :green, linestyle =:dash,label = \"Training Data End\")\n\nbestfit = prob_node(u0, sampled_par[i_min])\nplot(bestfit)\n\n\nplot!(plt, tsteps[1:35], bestfit[2, 1:35], colour =:black, label = \"Training: Best fit prediction\")\nplot!(plt, tsteps[35:end], bestfit[2, 35:end], colour =:purple, label = \"Forecasting: Best fit prediction\")\nplot!(plt, tsteps[1:35], bestfit[1, 1:35], colour =:black, label = :none)\nplot!(plt, tsteps[35:end], bestfit[1, 35:end], colour =:purple, label = :none)\n\nplot!(phase_plt,bestfit[1,1:40], bestfit[2, 1:40], colour = :black, label = \"Training: Best fit prediction\")\nplot!(phase_plt,bestfit[1, 40:end], bestfit[2, 40:end], colour = :purple, label = \"Forecasting: Best fit prediction\")\n","category":"page"},{"location":"examples/turing_bayesian/#Bayesian-Estimation-of-Differential-Equations-with-Probabilistic-Programming","page":"Bayesian Estimation of Differential Equations with Probabilistic Programming","title":"Bayesian Estimation of Differential Equations with Probabilistic Programming","text":"","category":"section"},{"location":"examples/turing_bayesian/","page":"Bayesian Estimation of Differential Equations with Probabilistic Programming","title":"Bayesian Estimation of Differential Equations with Probabilistic Programming","text":"For a good overview of how to use the tools of SciML in conjunction with the Turing.jl probabilistic programming language, see the Bayesian Differential Equation Tutorial.","category":"page"},{"location":"examples/neural_gde/#Neural-Graph-Differential-Equations","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"This tutorial has been adapted from here.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"In this tutorial we will use Graph Differential Equations (GDEs) to perform classification on the CORA Dataset. We shall be using the Graph Neural Networks primitives from the package GeometricFlux.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"# Load the packages\nusing GeometricFlux, JLD2, SparseArrays, DiffEqFlux, DifferentialEquations\nusing Flux: onehotbatch, onecold, throttle\nusing Flux.Losses: logitcrossentropy\nusing Statistics: mean\nusing LightGraphs: adjacency_matrix\n\n# Download the dataset\ndownload(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_features.jld2\", \"cora_features.jld2\")\ndownload(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_graph.jld2\", \"cora_graph.jld2\")\ndownload(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_labels.jld2\", \"cora_labels.jld2\")\n\n# Load the dataset\n@load \"./cora_features.jld2\" features\n@load \"./cora_labels.jld2\" labels\n@load \"./cora_graph.jld2\" g\n\n# Model and Data Configuration\nnum_nodes = 2708\nnum_features = 1433\nhidden = 16\ntarget_catg = 7\nepochs = 40\n\n# Preprocess the data and compute adjacency matrix\ntrain_X = Float32.(features)  # dim: num_features * num_nodes\ntrain_y = Float32.(labels)  # dim: target_catg * num_nodes\n\nadj_mat = FeaturedGraph(Matrix{Float32}(adjacency_matrix(g)))\n\n# Define the Neural GDE\ndiffeqarray_to_array(x) = reshape(cpu(x), size(x)[1:2])\n\nnode = NeuralODE(\n    GCNConv(adj_mat, hidden=>hidden),\n    (0.f0, 1.f0), Tsit5(), save_everystep = false,\n    reltol = 1e-3, abstol = 1e-3, save_start = false\n)\n\nmodel = Chain(GCNConv(adj_mat, num_features=>hidden, relu),\n              Dropout(0.5),\n              node,\n              diffeqarray_to_array,\n              GCNConv(adj_mat, hidden=>target_catg))\n\n# Loss\nloss(x, y) = logitcrossentropy(model(x), y)\naccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n\n# Training\n## Model Parameters\nps = Flux.params(model, node.p);\n\n## Training Data\ntrain_data = [(train_X, train_y)]\n\n## Optimizer\nopt = ADAM(0.01)\n\n## Callback Function for printing accuracies\nevalcb() = @show(accuracy(train_X, train_y))\n\n## Training Loop\nfor i = 1:epochs\n    Flux.train!(loss, ps, train_data, opt, cb=throttle(evalcb, 10))\nend","category":"page"},{"location":"examples/neural_gde/#Step-by-Step-Explanation","page":"Neural Graph Differential Equations","title":"Step by Step Explanation","text":"","category":"section"},{"location":"examples/neural_gde/#Load-the-Required-Packages","page":"Neural Graph Differential Equations","title":"Load the Required Packages","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"# Load the packages\nusing GeometricFlux, JLD2, SparseArrays, DiffEqFlux, DifferentialEquations\nusing Flux: onehotbatch, onecold, throttle\nusing Flux.Losses: crossentropy\nusing Statistics: mean\nusing LightGraphs: adjacency_matrix","category":"page"},{"location":"examples/neural_gde/#Load-the-Dataset","page":"Neural Graph Differential Equations","title":"Load the Dataset","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"The dataset is available in the desired format in the GeometricFlux repository. We shall download the dataset from there, and use the JLD2 package to load the data.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"download(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_features.jld2\", \"cora_features.jld2\")\ndownload(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_graph.jld2\", \"cora_graph.jld2\")\ndownload(\"https://rawcdn.githack.com/yuehhua/GeometricFlux.jl/a94ca7ce2ad01a12b23d68eb6cd991ee08569303/data/cora_labels.jld2\", \"cora_labels.jld2\")\n\n@load \"./cora_features.jld2\" features\n@load \"./cora_labels.jld2\" labels\n@load \"./cora_graph.jld2\" g","category":"page"},{"location":"examples/neural_gde/#Model-and-Data-Configuration","page":"Neural Graph Differential Equations","title":"Model and Data Configuration","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"The num_nodes, target_catg and num_features are defined by the data itself. We shall use a shallow GNN with only 16 hidden state dimension.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"num_nodes = 2708\nnum_features = 1433\nhidden = 16\ntarget_catg = 7\nepochs = 40","category":"page"},{"location":"examples/neural_gde/#Preprocessing-the-Data","page":"Neural Graph Differential Equations","title":"Preprocessing the Data","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Convert the data to float32 and use LightGraphs to get the adjacency matrix from the graph g.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"train_X = Float32.(features)  # dim: num_features * num_nodes\ntrain_y = Float32.(labels)  # dim: target_catg * num_nodes\n\nadj_mat = Matrix{Float32}(adjacency_matrix(g))","category":"page"},{"location":"examples/neural_gde/#Neural-Graph-Ordinary-Differential-Equations","page":"Neural Graph Differential Equations","title":"Neural Graph Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Let us now define the final model. We will use a single layer GNN for approximating the gradients for the neural ODE. We use two additional GCNConv layers, one to project the data to a latent space and the other to project it from the latent space to the predictions. Finally a softmax layer gives us the probability of the input belonging to each target category.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"diffeqarray_to_array(x) = reshape(cpu(x), size(x)[1:2])\n\nnode = NeuralODE(\n    GCNConv(adj_mat, hidden=>hidden),\n    (0.f0, 1.f0), Tsit5(), save_everystep = false,\n    reltol = 1e-3, abstol = 1e-3, save_start = false\n)\n\nmodel = Chain(GCNConv(adj_mat, num_features=>hidden, relu),\n              Dropout(0.5),\n              node,\n              diffeqarray_to_array,\n              GCNConv(adj_mat, hidden=>target_catg))","category":"page"},{"location":"examples/neural_gde/#Training-Configuration","page":"Neural Graph Differential Equations","title":"Training Configuration","text":"","category":"section"},{"location":"examples/neural_gde/#Loss-Function-and-Accuracy","page":"Neural Graph Differential Equations","title":"Loss Function and Accuracy","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"We shall be using the standard categorical crossentropy loss function which is used for multiclass classification tasks.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"loss(x, y) = logitcrossentropy(model(x), y)\naccuracy(x, y) = mean(onecold(model(x)) .== onecold(y))","category":"page"},{"location":"examples/neural_gde/#Model-Parameters","page":"Neural Graph Differential Equations","title":"Model Parameters","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Now we extract the model parameters which we want to learn.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"ps = Flux.params(model, node.p);","category":"page"},{"location":"examples/neural_gde/#Training-Data","page":"Neural Graph Differential Equations","title":"Training Data","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"GNNs operate on an entire graph, so we can't do any sort of minibatching here. We need to pass the entire data in a single pass. So our dataset is an array with a single tuple.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"train_data = [(train_X, train_y)]","category":"page"},{"location":"examples/neural_gde/#Optimizer","page":"Neural Graph Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"For this task we will be using the ADAM optimizer with a learning rate of 0.01.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"opt = ADAM(0.01)","category":"page"},{"location":"examples/neural_gde/#Callback-Function","page":"Neural Graph Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"We also define a utility function for printing the accuracy of the model over time.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"evalcb() = @show(accuracy(train_X, train_y))","category":"page"},{"location":"examples/neural_gde/#Training-Loop","page":"Neural Graph Differential Equations","title":"Training Loop","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Finally, with the configuration ready and all the utilities defined we can use the Flux.train! function to learn the parameters ps. We run the training loop for epochs number of iterations.","category":"page"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"for i = 1:epochs\n    Flux.train!(loss, ps, train_data, opt, cb=throttle(evalcb, 10))\nend","category":"page"},{"location":"examples/neural_gde/#Expected-Output","page":"Neural Graph Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"accuracy(train_X, train_y) = 0.12370753323485968\naccuracy(train_X, train_y) = 0.11632200886262925\naccuracy(train_X, train_y) = 0.1189069423929099\naccuracy(train_X, train_y) = 0.13404726735598227\naccuracy(train_X, train_y) = 0.15620384047267355\naccuracy(train_X, train_y) = 0.1776218611521418\naccuracy(train_X, train_y) = 0.19793205317577547\naccuracy(train_X, train_y) = 0.21122599704579026\naccuracy(train_X, train_y) = 0.22673559822747416\naccuracy(train_X, train_y) = 0.2429837518463811\naccuracy(train_X, train_y) = 0.25406203840472674\naccuracy(train_X, train_y) = 0.26809453471196454\naccuracy(train_X, train_y) = 0.2869276218611521\naccuracy(train_X, train_y) = 0.2961595273264402\naccuracy(train_X, train_y) = 0.30797636632200887\naccuracy(train_X, train_y) = 0.31831610044313147\naccuracy(train_X, train_y) = 0.3257016248153619\naccuracy(train_X, train_y) = 0.3378877400295421\naccuracy(train_X, train_y) = 0.3500738552437223\naccuracy(train_X, train_y) = 0.3629985228951256\naccuracy(train_X, train_y) = 0.37259970457902514\naccuracy(train_X, train_y) = 0.3777695716395864\naccuracy(train_X, train_y) = 0.3895864106351551\naccuracy(train_X, train_y) = 0.396602658788774\naccuracy(train_X, train_y) = 0.4010339734121123\naccuracy(train_X, train_y) = 0.40472673559822747\naccuracy(train_X, train_y) = 0.41285081240768096\naccuracy(train_X, train_y) = 0.422821270310192\naccuracy(train_X, train_y) = 0.43057607090103395\naccuracy(train_X, train_y) = 0.43833087149187594\naccuracy(train_X, train_y) = 0.44645494830132937\naccuracy(train_X, train_y) = 0.4538404726735598\naccuracy(train_X, train_y) = 0.45901033973412114\naccuracy(train_X, train_y) = 0.4630723781388479\naccuracy(train_X, train_y) = 0.46971935007385524\naccuracy(train_X, train_y) = 0.474519940915805\naccuracy(train_X, train_y) = 0.47858197932053176\naccuracy(train_X, train_y) = 0.4815361890694239\naccuracy(train_X, train_y) = 0.4804283604135894\naccuracy(train_X, train_y) = 0.4848596750369276","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Bayesian-Neural-ODEs:-NUTS","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"In this tutorial, we show how the DiffEqFlux.jl library in Julia can be seamlessly combined with Bayesian estimation libraries like AdvancedHMC.jl and Turing.jl. This enables converting Neural ODEs to Bayesian Neural ODEs, which enables us to estimate the error in the Neural ODE estimation and forecasting. In this tutorial, a working example of the Bayesian Neural ODE: NUTS sampler is shown.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"For more details, please refer to Bayesian Neural Ordinary Differential Equations.","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Copy-Pasteable-Code","page":"Bayesian Neural ODEs: NUTS","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Before getting to the explanation, here's some code to start with. We will follow wil a full explanation of the definition and training process:","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"using DiffEqFlux, DifferentialEquations, Plots, AdvancedHMC, MCMCChains\nusing JLD, StatsPlots\n\nu0 = [2.0; 0.0]\ndatasize = 40\ntspan = (0.0, 1)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n    Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\nl(θ) = -sum(abs2, ode_data .- predict_neuralode(θ)) - sum(θ .* θ)\n\n\nfunction dldθ(θ)\n    x,lambda = Flux.Zygote.pullback(l,θ)\n    grad = first(lambda(1))\n    return x, grad\nend\n\nmetric  = DiagEuclideanMetric(length(prob_neuralode.p))\n\nh = Hamiltonian(metric, l, dldθ)\n\n\nintegrator = Leapfrog(find_good_stepsize(h, Float64.(prob_neuralode.p)))\n\n\nprop = AdvancedHMC.NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.45, integrator))\n\nsamples, stats = sample(h, prop, Float64.(prob_neuralode.p), 500, adaptor, 500; progress=true)\n\n\nlosses = map(x-> x[1],[loss_neuralode(samples[i]) for i in 1:length(samples)])\n\n##################### PLOTS: LOSSES ###############\nscatter(losses, ylabel = \"Loss\",  yscale= :log, label = \"Architecture1: 500 warmup, 500 sample\")\n\n################### RETRODICTED PLOTS: TIME SERIES #################\npl = scatter(tsteps, ode_data[1,:], color = :red, label = \"Data: Var1\", xlabel = \"t\", title = \"Spiral Neural ODE\")\nscatter!(tsteps, ode_data[2,:], color = :blue, label = \"Data: Var2\")\n\nfor k in 1:300\n    resol = predict_neuralode(samples[100:end][rand(1:400)])\n    plot!(tsteps,resol[1,:], alpha=0.04, color = :red, label = \"\")\n    plot!(tsteps,resol[2,:], alpha=0.04, color = :blue, label = \"\")\nend\n\nidx = findmin(losses)[2]\nprediction = predict_neuralode(samples[idx])\n\nplot!(tsteps,prediction[1,:], color = :black, w = 2, label = \"\")\nplot!(tsteps,prediction[2,:], color = :black, w = 2, label = \"Best fit prediction\", ylims = (-2.5, 3.5))\n\n\n\n#################### RETRODICTED PLOTS - CONTOUR ####################\npl = scatter(ode_data[1,:], ode_data[2,:], color = :red, label = \"Data\",  xlabel = \"Var1\", ylabel = \"Var2\", title = \"Spiral Neural ODE\")\n\nfor k in 1:300\n    resol = predict_neuralode(samples[100:end][rand(1:400)])\n    plot!(resol[1,:],resol[2,:], alpha=0.04, color = :red, label = \"\")\nend\n\nplot!(prediction[1,:], prediction[2,:], color = :black, w = 2, label = \"Best fit prediction\", ylims = (-2.5, 3))\n","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Time Series Plots:","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Contour Plots:","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"######################## CHAIN DIAGNOSIS PLOTS#########################\nsamples = hcat(samples...)\n\nsamples_reduced = samples[1:5, :]\n\nsamples_reshape = reshape(samples_reduced, (500, 5, 1))\n\nChain_Spiral = Chains(samples_reshape)\n\nplot(Chain_Spiral)\n\nautocorplot(Chain_Spiral)\n","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Chain Mixing Plot:","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Auto-Correlation Plot:","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"(Image: )","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Explanation","page":"Bayesian Neural ODEs: NUTS","title":"Explanation","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/#Step-1:-Get-the-data-from-the-Spiral-ODE-example","page":"Bayesian Neural ODEs: NUTS","title":"Step 1: Get the data from the Spiral ODE example","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"u0 = [2.0; 0.0]\ndatasize = 40\ntspan = (0.0, 1)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Step-2:-Define-the-Neural-ODE-architecture.","page":"Bayesian Neural ODEs: NUTS","title":"Step 2: Define the Neural ODE architecture.","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"Note that this step potentially offers a lot of flexibility in the number of layers/ number of units in each layer. It may not necessarily be true that a 100 units architecture is better at prediction/forecasting than a 50 unit architecture. On the other hand, a complicated architecture can take a huge computational time without increasing performance.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Step-3:-Define-the-loss-function-for-the-Neural-ODE.","page":"Bayesian Neural ODEs: NUTS","title":"Step 3: Define the loss function for the Neural ODE.","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"function predict_neuralode(p)\n    Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Step-4:-Now-we-start-integrating-the-Bayesian-estimation-workflow-as-prescribed-by-the-AdvancedHMC-interface-with-the-NeuralODE-defined-above.","page":"Bayesian Neural ODEs: NUTS","title":"Step 4: Now we start integrating the Bayesian estimation workflow as prescribed by the AdvancedHMC interface with the NeuralODE defined above.","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"The Advanced HMC interface requires us to specify: (a) the hamiltonian log density and its gradient , (b) the sampler and (c) the step size adaptor function.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"For the hamiltonian log density, we use the loss function. The θ*θ term denotes the use of Gaussian priors.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"The user can make several modifications to Step 4. The user can try different acceptance ratios, warmup samples and posterior samples. One can also use the Variational Inference (ADVI) framework, which doesn't work quite as well as NUTS. The SGLD (Stochastic Langevin Gradient Descent) sampler is seen to have a better performance than NUTS. Have a look at https://sebastiancallh.github.io/post/langevin/ for a quick introduction to SGLD.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"l(θ) = -sum(abs2, ode_data .- predict_neuralode(θ)) - sum(θ .* θ)\n\n\nfunction dldθ(θ)\n    x,lambda = Flux.Zygote.pullback(l,θ)\n    grad = first(lambda(1))\n    return x, grad\nend\n\nmetric  = DiagEuclideanMetric(length(prob_neuralode.p))\n\nh = Hamiltonian(metric, l, dldθ)\n","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"We use the NUTS sampler with a acceptance ratio of δ= 0.45 in this example. In addition, we use Nesterov Dual Averaging for the Step Size adaptation.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"We sample using 500 warmup samples and 500 posterior samples.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"\nintegrator = Leapfrog(find_good_stepsize(h, Float64.(prob_neuralode.p)))\n\n\nprop = AdvancedHMC.NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.45, integrator))\n\nsamples, stats = sample(h, prop, Float64.(prob_neuralode.p), 500, adaptor, 500; progress=true)\n","category":"page"},{"location":"examples/BayesianNODE_NUTS/#Step-5:-Plot-diagnostics.","page":"Bayesian Neural ODEs: NUTS","title":"Step 5: Plot diagnostics.","text":"","category":"section"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"A: Plot chain object and auto-correlation plot of the first 5 parameters.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"samples = hcat(samples...)\n\nsamples_reduced = samples[1:5, :]\n\nsamples_reshape = reshape(samples_reduced, (500, 5, 1))\n\nChain_Spiral = Chains(samples_reshape)\n\nplot(Chain_Spiral)\n\nautocorplot(Chain_Spiral)","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"B: Plot retrodicted data.","category":"page"},{"location":"examples/BayesianNODE_NUTS/","page":"Bayesian Neural ODEs: NUTS","title":"Bayesian Neural ODEs: NUTS","text":"\n####################TIME SERIES PLOTS###################\npl = scatter(tsteps, ode_data[1,:], color = :red, label = \"Data: Var1\", xlabel = \"t\", title = \"Spiral Neural ODE\")\nscatter!(tsteps, ode_data[2,:], color = :blue, label = \"Data: Var2\")\n\nfor k in 1:300\n    resol = predict_neuralode(samples[100:end][rand(1:400)])\n    plot!(tsteps,resol[1,:], alpha=0.04, color = :red, label = \"\")\n    plot!(tsteps,resol[2,:], alpha=0.04, color = :blue, label = \"\")\nend\n\nidx = findmin(losses)[2]\nprediction = predict_neuralode(samples[idx])\n\nplot!(tsteps,prediction[1,:], color = :black, w = 2, label = \"\")\nplot!(tsteps,prediction[2,:], color = :black, w = 2, label = \"Best fit prediction\", ylims = (-2.5, 3.5))\n\n####################CONTOUR PLOTS#########################3\npl = scatter(ode_data[1,:], ode_data[2,:], color = :red, label = \"Data\",  xlabel = \"Var1\", ylabel = \"Var2\", title = \"Spiral Neural ODE\")\n\nfor k in 1:300\n    resol = predict_neuralode(samples[100:end][rand(1:400)])\n    plot!(resol[1,:],resol[2,:], alpha=0.04, color = :red, label = \"\")\nend\n\nplot!(prediction[1,:], prediction[2,:], color = :black, w = 2, label = \"Best fit prediction\", ylims = (-2.5, 3))\n","category":"page"},{"location":"examples/stiff_ode_fit/#Parameter-Estimation-on-Highly-Stiff-Systems","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"","category":"section"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"This tutorial goes into training a model on stiff chemical reaction system data.","category":"page"},{"location":"examples/stiff_ode_fit/#Copy-Pasteable-Code","page":"Parameter Estimation on Highly Stiff Systems","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"using DifferentialEquations, DiffEqFlux, LinearAlgebra\nusing ForwardDiff\nusing DiffEqBase: UJacobianWrapper\nusing Plots\nfunction rober(du,u,p,t)\n    y₁,y₂,y₃ = u\n    k₁,k₂,k₃ = p\n    du[1] = -k₁*y₁+k₃*y₂*y₃\n    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n    du[3] =  k₂*y₂^2\n    nothing\nend\n\np = [0.04,3e7,1e4]\nu0 = [1.0,0.0,0.0]\nprob = ODEProblem(rober,u0,(0.0,1e5),p)\nsol = solve(prob,Rosenbrock23())\nts = sol.t\nJs = map(u->I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)\n\nfunction predict_adjoint(p)\n    p = exp.(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(p)\n    prediction = predict_adjoint(p)\n    prediction = [prediction[:, i] for i in axes(prediction, 2)]\n    diff = map((J,u,data) -> J * (abs2.(u .- data)) , Js, prediction, sol.u)\n    loss = sum(abs, sum(diff)) |> sqrt\n    loss, prediction\nend\n\ncb = function (p,l,pred) #callback function to observe training\n    println(\"Loss: $l\")\n    println(\"Parameters: $(exp.(p))\")\n    # using `remake` to re-create our `prob` with current parameters `p`\n    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |> display\n    return false # Tell it to not halt the optimization. If return true, then optimization stops\nend\n\ninitp = ones(3)\n# Display the ODE with the initial parameter values.\ncb(initp,loss_adjoint(initp)...)\n\nres = DiffEqFlux.sciml_train(loss_adjoint, initp, ADAM(0.01), cb = cb, maxiters = 300)\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res.u, BFGS(), cb = cb, maxiters = 30, allow_f_increases=true)\nprintln(\"Ground truth: $(p)\\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%\")","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Output:","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Ground truth: [0.04, 3.0e7, 10000.0]\nFinal parameters: [0.040002, 3.0507e7, 10084.0]\nError: 1.69%","category":"page"},{"location":"examples/stiff_ode_fit/#Explanation","page":"Parameter Estimation on Highly Stiff Systems","title":"Explanation","text":"","category":"section"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"First, let's get a time series array from the Robertson's equation as data.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"using DifferentialEquations, DiffEqFlux, LinearAlgebra\nusing ForwardDiff\nusing DiffEqBase: UJacobianWrapper\nusing Plots\nfunction rober(du,u,p,t)\n    y₁,y₂,y₃ = u\n    k₁,k₂,k₃ = p\n    du[1] = -k₁*y₁+k₃*y₂*y₃\n    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n    du[3] =  k₂*y₂^2\n    nothing\nend\n\np = [0.04,3e7,1e4]\nu0 = [1.0,0.0,0.0]\nprob = ODEProblem(rober,u0,(0.0,1e5),p)\nsol = solve(prob,Rosenbrock23())\nts = sol.t\nJs = map(u->I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Note that we also computed a shifted and scaled Jacobian along with the solution. We will use this matrix to scale the loss later.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We fit the parameters in log space, so we need to compute exp.(p) to get back the original parameters.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"function predict_adjoint(p)\n    p = exp.(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(p)\n    prediction = predict_adjoint(p)\n    prediction = [prediction[:, i] for i in axes(prediction, 2)]\n    diff = map((J,u,data) -> J * (abs2.(u .- data)) , Js, prediction, sol.u)\n    loss = sum(abs, sum(diff)) |> sqrt\n    loss, prediction\nend","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"The difference between the data and the prediction is weighted by the transformed Jacobian to do a relative scaling of the loss.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We define a callback function.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"cb = function (p,l,pred) #callback function to observe training\n    println(\"Loss: $l\")\n    println(\"Parameters: $(exp.(p))\")\n    # using `remake` to re-create our `prob` with current parameters `p`\n    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |> display\n    return false # Tell it to not halt the optimization. If return true, then optimization stops\nend","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We then use a combination of ADAM and BFGS to minimize the loss function to accelerate the optimization. The initial guess of the parameters are chosen to be [1, 1, 1.0].","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"initp = ones(3)\n# Display the ODE with the initial parameter values.\ncb(initp,loss_adjoint(initp)...)\n\nres = DiffEqFlux.sciml_train(loss_adjoint, initp, ADAM(0.01), cb = cb, maxiters = 300)\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res.u, BFGS(), cb = cb, maxiters = 30, allow_f_increases=true)","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Finally, we can analyze the difference between the fitted parameters and the ground truth.","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"println(\"Ground truth: $(p)\\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%\")","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"It gives the output","category":"page"},{"location":"examples/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Ground truth: [0.04, 3.0e7, 10000.0]\nFinal parameters: [0.040002, 3.0507e7, 10084.0]\nError: 1.69%","category":"page"},{"location":"examples/exogenous_input/#Handling-Exogenous-Input-Signals","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"","category":"section"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"The key to using exogeneous input signals is the same as in the rest of the SciML universe: just use the function in the definition of the differential equation. For example, if it's a standard differential equation, you can use the form","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"I(t) = t^2\n\nfunction f(du,u,p,t)\n  du[1] = I(t)\n  du[2] = u[1]\nend","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"so that I(t) is an exogenous input signal into f. Another form that could be useful is a closure. For example:","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"function f(du,u,p,t,I)\n  du[1] = I(t)\n  du[2] = u[1]\nend\n\n_f(du,u,p,t) = f(du,u,p,t,x -> x^2)","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"which encloses an extra argument into f so that _f is now the interface-compliant differential equation definition.","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"Note that you can also learn what the exogenous equation is from data. For an example on how to do this, you can use the Optimal Control Example which shows how to parameterize a u(t) by a universal function and learn that from data.","category":"page"},{"location":"examples/exogenous_input/#Example-of-a-Neural-ODE-with-Exogenous-Input","page":"Handling Exogenous Input Signals","title":"Example of a Neural ODE with Exogenous Input","text":"","category":"section"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"In the following example, a discrete exogenous input signal ex is defined and used as an input into the neural network of a neural ODE system.","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"using DifferentialEquations, DiffEqFlux, Plots\n\ntspan = (0.1f0, Float32(10.0))\ntsteps = range(tspan[1], tspan[2], length = 100)\nt_vec = collect(tsteps)\nex = vec(ones(Float32,length(tsteps), 1))\nf(x) = (atan(8.0 * x - 4.0) + atan(4.0)) / (2.0 * atan(4.0))\n\nfunction hammerstein_system(u)\n    y= zeros(size(u))\n    for k in 2:length(u)\n        y[k] = 0.2 * f(u[k-1]) + 0.8 * y[k-1]\n    end\n    return y\nend\n\ny = Float32.(hammerstein_system(ex))\nplot(collect(tsteps), y, ticks=:native)\n\nnn_model = FastChain(FastDense(2,8, tanh), FastDense(8, 1))\np_model = initial_params(nn_model)\n\nu0 = Float32.([0.0])\n\nfunction dudt(u, p, t)\n    #input_val = u_vals[Int(round(t*10)+1)]\n    nn_model(vcat(u[1], ex[Int(round(10*0.1))]), p)\nend\n\nprob = ODEProblem(dudt,u0,tspan,nothing)\n\nfunction predict_neuralode(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6))\nend\n\nfunction loss(p)\n    sol = predict_neuralode(p)\n    N = length(sol)\n    return sum(abs2.(y[1:N] .- sol'))/N\nend\n\nres0 = DiffEqFlux.sciml_train(loss,p_model,maxiters=100)\n\nsol = predict_neuralode(res0.u)\nplot(tsteps,sol')\nN = length(sol)\nscatter!(tsteps,y[1:N])\n\nsavefig(\"trained.png\")","category":"page"},{"location":"examples/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"(Image: )","category":"page"},{"location":"examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Copy-Pasteable-Code","page":"Augmented Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend\n\ndiffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend\n\nfunction plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend\n\nloss_node(x, y) = mean((model(x) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)\n\ncb = function()\n    global iter += 1\n    if iter % 10 == 0\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend\n\nmodel, parameters = construct_model(1, 2, 64, 0)\nopt = ADAM(0.005)\niter = 0\n\nprintln(\"Training Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_node = plot_contour(model)\n\nmodel, parameters = construct_model(1, 2, 64, 1)\nopt = ADAM(0.005)\niter = 0\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_anode = plot_contour(model)","category":"page"},{"location":"examples/augmented_neural_ode/#Step-by-Step-Explanation","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loading-required-packages","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader","category":"page"},{"location":"examples/augmented_neural_ode/#Generating-a-toy-dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader(data |> gpu, labels |> gpu; batchsize=batch_size, shuffle=true,\n                      partial=false)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Models","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tutorial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on GPU, we need to manually transfer the models to GPU. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : (AugmentedNDELayer(node, augment_dim) |> gpu)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Plotting-the-Results","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define an utility to plot our model regression results as a heatmap.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Training-Parameters","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loss-Functions","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(x, y) = mean((model(x) .- y) .^ 2)","category":"page"},{"location":"examples/augmented_neural_ode/#Dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)","category":"page"},{"location":"examples/augmented_neural_ode/#Callback-Function","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"cb = function()\n    global iter += 1\n    if iter % 10 == 1\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Optimizer","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use ADAM as the optimizer with a learning rate of 0.005","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = ADAM(0.005)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which is returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 0)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be same as that of Neural ODE. Only in this case we have augmented the input with a single zero. This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs please refer to [1].","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 1)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"examples/augmented_neural_ode/#Expected-Output","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\nTraining Neural ODE\nIteration 10 || Loss = 0.9802582\nIteration 20 || Loss = 0.6727416\nIteration 30 || Loss = 0.5862373\nIteration 40 || Loss = 0.5278132\nIteration 50 || Loss = 0.4867624\nIteration 60 || Loss = 0.41630346\nIteration 70 || Loss = 0.3325938\nIteration 80 || Loss = 0.28235924\nIteration 90 || Loss = 0.24069068\nIteration 100 || Loss = 0.20503852\nIteration 110 || Loss = 0.17608969\nIteration 120 || Loss = 0.1491399\nIteration 130 || Loss = 0.12711425\nIteration 140 || Loss = 0.10686825\nIteration 150 || Loss = 0.089558244\n\nTraining Augmented Neural ODE\nIteration 10 || Loss = 1.3911372\nIteration 20 || Loss = 0.7694144\nIteration 30 || Loss = 0.5639633\nIteration 40 || Loss = 0.33187616\nIteration 50 || Loss = 0.14787851\nIteration 60 || Loss = 0.094676435\nIteration 70 || Loss = 0.07363529\nIteration 80 || Loss = 0.060333826\nIteration 90 || Loss = 0.04998395\nIteration 100 || Loss = 0.044843454\nIteration 110 || Loss = 0.042587914\nIteration 120 || Loss = 0.042706195\nIteration 130 || Loss = 0.040252227\nIteration 140 || Loss = 0.037686247\nIteration 150 || Loss = 0.036247417","category":"page"},{"location":"examples/augmented_neural_ode/#References","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.","category":"page"},{"location":"examples/multiple_nn/#Simultaneous-Fitting-of-Multiple-Neural-Networks","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"","category":"section"},{"location":"examples/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"In many cases users are interested in fitting multiple neural networks or parameters simultaneously. This tutorial addresses how to perform this kind of study.","category":"page"},{"location":"examples/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"The following is a fully working demo on the Fitzhugh-Nagumo ODE:","category":"page"},{"location":"examples/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"using DiffEqFlux, DifferentialEquations\n\nfunction fitz(du,u,p,t)\n  v,w = u\n  a,b,τinv,l = p\n  du[1] = v - v^3/3 -w + l\n  du[2] = τinv*(v +  a - b*w)\nend\n\np_ = Float32[0.7,0.8,1/12.5,0.5]\nu0 = [1f0;1f0]\ntspan = (0f0,10f0)\nprob = ODEProblem(fitz,u0,tspan,p_)\nsol = solve(prob, Tsit5(), saveat = 0.5 )\n\n# Ideal data\nX = Array(sol)\nXₙ = X + Float32(1e-3)*randn(eltype(X), size(X))  #noisy data\n\n# For xz term\nNN_1 = FastChain(FastDense(2, 16, tanh), FastDense(16, 1))\np1 = initial_params(NN_1)\n\n# for xy term\nNN_2 = FastChain(FastDense(3, 16, tanh), FastDense(16, 1))\np2 = initial_params(NN_2)\nscaling_factor = 1f0\n\np = [p1;p2;scaling_factor]\nfunction dudt_(u,p,t)\n    v,w = u\n    z1 = NN_1([v,w], p[1:length(p1)])\n    z2 = NN_2([v,w,t], p[(length(p1)+1):end-1])\n    [z1[1],p[end]*z2[1]]\nend\nprob_nn = ODEProblem(dudt_,u0, tspan, p)\nsol_nn = solve(prob_nn, Tsit5(),saveat = sol.t)\n\nfunction predict(θ)\n    Array(solve(prob_nn, Vern7(), p=θ, saveat = sol.t,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, Xₙ .- pred), pred\nend\nloss(p)\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\n\nres1_uode = DiffEqFlux.sciml_train(loss, p, ADAM(0.01), cb=callback, maxiters = 500)\nres2_uode = DiffEqFlux.sciml_train(loss, res1_uode.u, BFGS(initial_stepnorm=0.01), cb=callback, maxiters = 10000)","category":"page"},{"location":"examples/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"The key is that sciml_train acts on a single parameter vector p. Thus what we do here is concatenate all of the parameters into a single vector p = [p1;p2;scaling_factor] and then train on this parameter vector. Whenever we need to evaluate the neural networks, we cut the vector and grab the portion that corresponds to the neural network. For example, the p1 portion is p[1:length(p1)], which is why the first neural network's evolution is written like NN_1([v,w], p[1:length(p1)]).","category":"page"},{"location":"examples/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"This method is flexible to use with many optimizers and in fairly optimized ways. The allocations can be reduced by using @view p[1:length(p1)]. We can also see with the scaling_factor that we can grab parameters directly out of the vector and use them as needed.","category":"page"},{"location":"layers/TensorLayer/#Tensor-Product-Layer","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, B_1 B_2  B_n, a data point x, computes zi = Wi  B_1(x1)  B_2(x2)    B_n(xn), where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"layers/TensorLayer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"Constructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nTensorLayer(model,out,p=nothing)\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initialized to standard normal by default.\n\n\n\n\n\n","category":"type"},{"location":"examples/divergence/#Handling-Divergent-and-Unstable-Trajectories","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"","category":"section"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"It is not uncommon for a set of parameters in an ODE model to simply give a divergent trajectory. If the rate of growth compounds and outpaces the rate of decay, you will end up at infinity in finite time. This it is not uncommon to see divergent trajectories in the optimization of parameters, as many times an optimizer can take an excursion into a parameter regime which simply gives a model with an infinite solution.","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"This can be addressed by using the retcode system. In DifferentialEquations.jl, RetCodes detail the status of the returned solution. Thus if the retcode corresponds to a failure, we can use this to give an infinite loss and effectively discard the parameters. This is shown in the loss function:","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"function loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if tmp_sol.retcode == :Success\n    return sum(abs2,Array(tmp_sol) - dataset)\n  else\n    return Inf\n  end\nend","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"A full example making use of this trick is:","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"using DifferentialEquations, Plots\n\nfunction lotka_volterra!(du,u,p,t)\n    rab, wol = u\n    α,β,γ,δ=p\n    du[1] = drab = α*rab - β*rab*wol\n    du[2] = dwol = γ*rab*wol - δ*wol\n    nothing\nend\n\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(lotka_volterra!,u0,tspan,p)\nsol = solve(prob,saveat=0.1)\nplot(sol)\n\ndataset = Array(sol)\nscatter!(sol.t,dataset')\n\ntmp_prob = remake(prob, p=[1.2,0.8,2.5,0.8])\ntmp_sol = solve(tmp_prob)\nplot(tmp_sol)\nscatter!(sol.t,dataset')\n\nfunction loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if tmp_sol.retcode == :Success\n    return sum(abs2,Array(tmp_sol) - dataset)\n  else\n    return Inf\n  end\nend\n\nusing DiffEqFlux\n\npinit = [1.2,0.8,2.5,0.8]\nres = DiffEqFlux.sciml_train(loss,pinit,ADAM(), maxiters = 1000)\n\n# res = DiffEqFlux.sciml_train(loss,pinit,BFGS(), maxiters = 1000) ### errors!\n\n#try Newton method of optimization\nres = DiffEqFlux.sciml_train(loss,pinit,Newton(), GalacticOptim.AutoForwardDiff())","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"You might notice that AutoZygote (default) fails for the above sciml_train call with Optim's optimizers which happens because of Zygote's behaviour for zero gradients in which case it returns nothing. To avoid such issue you can just use a different version of the same check which compares the size of the obtained  solution and the data we have, shown below, which is easier to AD.","category":"page"},{"location":"examples/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"function loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if size(tmp_sol) == size(dataset)\n    return sum(abs2,Array(tmp_sol) .- dataset)\n  else\n    return Inf\n  end\nend","category":"page"},{"location":"examples/pde_constrained/#Partial-Differential-Equation-(PDE)-Constrained-Optimization","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"This example uses a prediction model to optimize the one-dimensional Heat Equation. (Step-by-step description below)","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DifferentialEquations, DiffEqFlux\n\n# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;\n\n## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend\n\n## ODE description of the Physics:\nfunction heat(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend\n\n# Testing Solver on linear PDE\nprob = ODEProblem(heat,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")\n\nps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend\n\n## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes\n\nLOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncb = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncb(ps,loss(ps)...) # Testing callback function\n\n# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")\n\nres = DiffEqFlux.sciml_train(loss, ps, cb = cb)\n@show res.u # returns [0.999999999613485, 0.9999999991343996]","category":"page"},{"location":"examples/pde_constrained/#Step-by-step-Description","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Step-by-step Description","text":"","category":"section"},{"location":"examples/pde_constrained/#Load-Packages","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Load Packages","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DifferentialEquations, DiffEqFlux","category":"page"},{"location":"examples/pde_constrained/#Parameters","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Parameters","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"First, we setup the 1-dimensional space over which our equations will be evaluated. x spans from 0.0 to 10.0 in steps of 0.01; t spans from 0.00 to 0.04 in steps of 4.0e-5.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"In plain terms, the quantities that were defined are:","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"x (to Lx) spans the specified 1D space\ndx = distance between two points\nNx = total size of space\nu0 = initial condition\np = true solution\nxtrs = convenient grouping of dx and Nx into Array\ndt = time distance between two points\nt (t0 to tMax) spans the specified time frame\ntspan = span of t","category":"page"},{"location":"examples/pde_constrained/#Auxiliary-Functions","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Auxiliary Functions","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We then define two functions to compute the derivatives numerically. The Central Difference is used in both the 1st and 2nd degree derivatives.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend","category":"page"},{"location":"examples/pde_constrained/#Heat-Differential-Equation","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Heat Differential Equation","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Next, we setup our desired set of equations in order to define our problem.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## ODE description of the Physics:\nfunction heat(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend","category":"page"},{"location":"examples/pde_constrained/#Solve-and-Plot-Ground-Truth","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Solve and Plot Ground Truth","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We then solve and plot our partial differential equation. This is the true solution which we will compare to further on.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Testing Solver on linear PDE\nprob = ODEProblem(heat,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")","category":"page"},{"location":"examples/pde_constrained/#Building-the-Prediction-Model","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Building the Prediction Model","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Now we start building our prediction model to try to obtain the values p. We make an initial guess for the parameters and name it ps here. The predict function is a non-linear transformation in one layer using solve. If unfamiliar with the concept, refer to here.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"ps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend","category":"page"},{"location":"examples/pde_constrained/#Train-Parameters","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Train Parameters","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Training our model requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"examples/pde_constrained/#Loss","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Loss","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We first make our predictions based on the current values of our parameters ps, then take the difference between the predicted solution and the truth above. For the loss, we use the Mean squared error.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes","category":"page"},{"location":"examples/pde_constrained/#Optimizer","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Optimizer","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The optimizers ADAM with a learning rate of 0.01 and BFGS are directly passed in training (see below)","category":"page"},{"location":"examples/pde_constrained/#Callback","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Callback","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The callback function displays the loss during training. We also keep a history of the loss, the previous predictions and the previous parameters with LOSS, PRED and PARS accumulators.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"LOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncb = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncb(ps,loss(ps)...) # Testing callback function","category":"page"},{"location":"examples/pde_constrained/#Plotting-Prediction-vs-Ground-Truth","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Plotting Prediction vs Ground Truth","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The scatter points plotted here are the ground truth obtained from the actual solution we solved for above. The solid line represents our prediction. The goal is for both to overlap almost perfectly when the PDE finishes its training and the loss is close to 0.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")","category":"page"},{"location":"examples/pde_constrained/#Train","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Train","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The parameters are trained using sciml_train and adjoint sensitivities. The resulting best parameters are stored in res and res.u returns the parameters that minimizes the cost function.","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"res = DiffEqFlux.sciml_train(loss, ps, cb = cb)\n@show res.u # returns [0.999999999613485, 0.9999999991343996]","category":"page"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We successfully predict the final ps to be equal to [0.999999999999975, 1.0000000000000213] vs the true solution of p = [1.0, 1.0]","category":"page"},{"location":"examples/pde_constrained/#Expected-Output","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Expected Output","text":"","category":"section"},{"location":"examples/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"153.74716386883014\n153.74716386883014\n150.31001476832154\n146.91327105278128\n143.55759898759374\n140.24363496931753\n136.97198347241257\n133.7432151677673\n130.55786524987215\n127.4164319720337\n124.31937540894337\n121.26711645161134\n118.26003603654628\n115.29847461603427\n112.3827318609633\n109.51306659138356\n106.68969692777314\n103.9128006498965\n101.18251574195561\n98.4989411191655\n95.8621374998964\n93.27212842357801\n90.7289013677808\n88.23240896985287\n85.7825703121191\n83.37927225399383\n81.02237079935475\n78.71169247246975\n76.44703568540336\n74.22817209335733\n72.05484791455291\n69.92678520204167\n67.84368308185877\n65.80521891873633\n63.81104944163126\n61.860811797059554\n59.95412455791812\n58.090588663826914\n56.26978832428055\n54.491291863817686\n52.75465253618253\n51.05940929392087\n49.405087540342564\n47.79119984816457\n46.217246667009626\n44.68271701552145\n43.18708916553295\n41.729831330086824\n40.310402328506555\n38.928252289762675\n37.58282331100446\n36.27355015737786\n34.99986094007708\n33.76117780641769\n32.55691762379305\n31.386492661205562\n30.249311268822595\n29.144778544729924\n28.07229699202965\n27.031267166855155\n26.0210883069299\n25.041158938495613\n24.09087747422764\n23.169642780270983\n22.276854715336583\n21.411914664407295\n20.57422602075309\n19.76319467338999\n18.978229434706996\n18.218742481097735\n17.48414972880479\n16.773871221320032\n16.087331469276343\n15.423959781047255\n14.78319057598673\n14.164463661389682\n13.567224508247984\n12.990924508800399\n12.435021204904853\n11.898978515303417\n11.382266943971572\n10.884363779196345\n10.404753276294088\n9.942926832732251\n9.49838314770057\n9.070628379941386\n8.659176278010788\n8.263548334737965\n7.883273889583058\n7.517890250788576\n7.1669427976429585\n6.829985075319055\n6.506578881124348\n6.19629433688754\n5.898709957062298\n5.613412692266443\n5.339997993203038\n5.078069839645422\n4.827240754206443\n4.587131834698446\n4.357372763056912\n4.357372763056912\n4.137601774726927\n1.5254536025963588\n0.0023707487489687726\n4.933077457357198e-7\n8.157805551380282e-14\n1.6648677430325974e-16\nres.u = [0.999999999999975, 1.0000000000000213]\n2-element Array{Float64,1}:\n 0.999999999999975\n 1.0000000000000213","category":"page"},{"location":"examples/tensor_layer/#Physics-Informed-Machine-Learning-with-TensorLayer","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"","category":"section"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"x = - kx - αx³ - βx -γx³","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in DifferentialEquations:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"using DiffEqFlux, DifferentialEquations, LinearAlgebra\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0,10.0)\n\nfunction dxdt_train(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\nend\n\nu0 = [1.0,0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train,u0,tspan,p=nothing)\ndata_train = Array(solve(prob_train,Tsit5(),saveat=ts))","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"and we also instantiate the model we are trying to learn, \"informing\" the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"f = x -> min(30one(x),x)\n\nfunction dxdt_pred(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\nend\n\nα = zeros(102)\n\nprob_pred = ODEProblem{true}(dxdt_pred,u0,tspan,p=nothing)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Note that we introduced a \"cap\" in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"\nfunction predict_adjoint(θ)\n  x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts))\nend\n\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  loss = sum(norm.(x - data_train))\n  return loss\nend\n\nfunction cb(θ,l)\n  @show θ, l\n  return false\nend","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"and we train the network using two rounds of ADAM:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"res1 = DiffEqFlux.sciml_train(loss_adjoint, α, ADAM(0.05), cb = cb, maxiters = 150)\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.u, ADAM(0.001), cb = cb,maxiters = 150)\nopt = res2.u","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"We plot the results and we obtain a fairly accurate learned model:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(opt)\nplot(ts, data_train[1,:], label = \"X (ODE)\")\nplot!(ts, data_train[2,:], label = \"V (ODE)\")\nplot!(ts, data_pred[1,:], label = \"X (NN)\")\nplot!(ts, data_pred[2,:],label = \"V (NN)\")","category":"page"},{"location":"examples/tensor_layer/","page":"Physics Informed Machine Learning with TensorLayer","title":"Physics Informed Machine Learning with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"examples/neural_sde/#Neural-Stochastic-Differential-Equations","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"With neural stochastic differential equations, there is once again a helper form neural_dmsde which can be used for the multiplicative noise case (consult the layers API documentation, or this full example using the layer function).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"However, since there are far too many possible combinations for the API to support, in many cases you will want to performantly define neural differential equations for non-ODE systems from scratch. For these systems, it is generally best to use TrackerAdjoint with non-mutating (out-of-place) forms. For example, the following defines a neural SDE with neural networks for both the drift and diffusion terms:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"dudt(u, p, t) = model(u)\ng(u, p, t) = model2(u)\nprob = SDEProblem(dudt, g, x, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"where model and model2 are different neural networks. The same can apply to a neural delay differential equation. Its out-of-place formulation is f(u,h,p,t). Thus for example, if we want to define a neural delay differential equation which uses the history value at p.tau in the past, we can define:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"dudt!(u, h, p, t) = model([u; h(t - p.tau)])\nprob = DDEProblem(dudt_, u0, h, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"First let's build training data from the same example as the neural ODE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"using Plots, Statistics\nusing DiffEqFlux, StochasticDiffEq, DiffEqBase.EnsembleAnalysis\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0, 1.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"function trueSDEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nmp = Float32[0.2, 0.2]\nfunction true_noise_func(du, u, p, t)\n    du .= mp.*u\nend\n\nprob_truesde = SDEProblem(trueSDEfunc, true_noise_func, u0, tspan)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"For our dataset we will use DifferentialEquations.jl's parallel ensemble interface to generate data from the average of 10,000 runs of the SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"# Take a typical sample from the mean\nensemble_prob = EnsembleProblem(prob_truesde)\nensemble_sol = solve(ensemble_prob, SOSRI(), trajectories = 10000)\nensemble_sum = EnsembleSummary(ensemble_sol)\n\nsde_data, sde_data_vars = Array.(timeseries_point_meanvar(ensemble_sol, tsteps))","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now we build a neural SDE. For simplicity we will use the NeuralDSDE neural SDE with diagonal noise layer function:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"drift_dudt = FastChain((x, p) -> x.^3,\n                       FastDense(2, 50, tanh),\n                       FastDense(50, 2))\ndiffusion_dudt = FastChain(FastDense(2, 2))\n\nneuralsde = NeuralDSDE(drift_dudt, diffusion_dudt, tspan, SOSRI(),\n                       saveat = tsteps, reltol = 1e-1, abstol = 1e-1)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Let's see what that looks like:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"# Get the prediction using the correct initial condition\nprediction0 = neuralsde(u0)\n\ndrift_(u, p, t) = drift_dudt(u, p[1:neuralsde.len])\ndiffusion_(u, p, t) = diffusion_dudt(u, p[(neuralsde.len+1):end])\n\nprob_neuralsde = SDEProblem(drift_, diffusion_, u0,(0.0f0, 1.2f0), neuralsde.p)\n\nensemble_nprob = EnsembleProblem(prob_neuralsde)\nensemble_nsol = solve(ensemble_nprob, SOSRI(), trajectories = 100,\n                      saveat = tsteps)\nensemble_nsum = EnsembleSummary(ensemble_nsol)\n\nplt1 = plot(ensemble_nsum, title = \"Neural SDE: Before Training\")\nscatter!(plt1, tsteps, sde_data', lw = 3)\n\nscatter(tsteps, sde_data[1,:], label = \"data\")\nscatter!(tsteps, prediction0[1,:], label = \"prediction\")","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now just as with the neural ODE we define a loss function that calculates the mean and variance from n runs at each time point and uses the distance from the data values:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"function predict_neuralsde(p, u = u0)\n  return Array(neuralsde(u, p))\nend\n\nfunction loss_neuralsde(p; n = 100)\n  u = repeat(reshape(u0, :, 1), 1, n)\n  samples = predict_neuralsde(p, u)\n  means = mean(samples, dims = 2)\n  vars = var(samples, dims = 2, mean = means)[:, 1, :]\n  means = means[:, 1, :]\n  loss = sum(abs2, sde_data - means) + sum(abs2, sde_data_vars - vars)\n  return loss, means, vars\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"list_plots = []\niter = 0\n\n# Callback function to observe training\ncallback = function (p, loss, means, vars; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  # loss against current data\n  display(loss)\n\n  # plot current prediction against data\n  plt = Plots.scatter(tsteps, sde_data[1,:], yerror = sde_data_vars[1,:],\n                     ylim = (-4.0, 8.0), label = \"data\")\n  Plots.scatter!(plt, tsteps, means[1,:], ribbon = vars[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n\n  if doplot\n    display(plt)\n  end\n  return false\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Now we train using this loss function. We can pre-train a little bit using a smaller n and then decrease it after it has had some time to adjust towards the right mean behavior:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"opt = ADAM(0.025)\n\n# First round of training with n = 10\nresult1 = DiffEqFlux.sciml_train((p) -> loss_neuralsde(p, n = 10),  \n                                 neuralsde.p, opt,\n                                 cb = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"We resume the training with a larger n. (WARNING - this step is a couple of orders of magnitude longer than the previous one).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"result2 = DiffEqFlux.sciml_train((p) -> loss_neuralsde(p, n = 100),\n                                 result1.u, opt,\n                                 cb = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"And now we plot the solution to an ensemble of the trained neural SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"_, means, vars = loss_neuralsde(result2.u, n = 1000)\n\nplt2 = Plots.scatter(tsteps, sde_data', yerror = sde_data_vars',\n                     label = \"data\", title = \"Neural SDE: After Training\",\n                     xlabel = \"Time\")\nplot!(plt2, tsteps, means', lw = 8, ribbon = vars', label = \"prediction\")\n\nplt = plot(plt1, plt2, layout = (2, 1))\nsavefig(plt, \"NN_sde_combined.png\"); nothing # sde","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations","title":"Neural Stochastic Differential Equations","text":"Try this with GPUs as well!","category":"page"},{"location":"examples/neural_ode_galacticoptim/#Neural-Ordinary-Differential-Equations-with-GalacticOptim.jl","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"","category":"section"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"DiffEqFlux.jl defines sciml_train which is a high level utility that automates a lot of the choices, using heuristics to determine a potentially efficient method. However, in some cases you may want more control over the optimization process. The underlying optimization package behind sciml_train is GalacticOptim.jl. In this tutorial we will show how to more deeply interact with the optimization library to tweak its processes.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"We can use a neural ODE as our example. A neural ODE is an ODE where a neural network defines its derivative function. Thus for example, with the multilayer perceptron neural network FastChain(FastDense(2, 50, tanh), FastDense(50, 2)), we obtain  the following results.","category":"page"},{"location":"examples/neural_ode_galacticoptim/#Copy-Pasteable-Code","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, Plots, GalacticOptim\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\ncallback = function (p, l, pred; doplot = true)\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\n# use GalacticOptim.jl to solve the problem\nadtype = GalacticOptim.AutoZygote()\noptf = GalacticOptim.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = GalacticOptim.OptimizationProblem(optf, prob_neuralode.p)\n\nresult_neuralode = GalacticOptim.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = GalacticOptim.solve(optprob2,\n                                        LBFGS(),\n                                        callback = callback,\n                                        allow_f_increases = false)","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"(Image: Neural ODE)","category":"page"},{"location":"examples/neural_ode_galacticoptim/#Explanation","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Explanation","text":"","category":"section"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"Let's get a time series array from the Lotka-Volterra equation as data:","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"Now let's define a neural network with a NeuralODE layer. First we define the layer. Here we're going to use FastChain, which is a faster neural network structure for NeuralODEs:","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"Note that we can directly use Chains from Flux.jl as well, for example:","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"dudt2 = Chain(x -> x.^3,\n              Dense(2, 50, tanh),\n              Dense(50, 2))","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"In our model we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"We define a callback function.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"# Callback function to observe training\ncallback = function (p, l, pred; doplot = false)\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"We then train the neural network to learn the ODE.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we are able to fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting). You can easily incorporate the procedure below to set up custom optimization problems. For more information on the usage of GalacticOptim.jl, please consult this documentation.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"The x and p variables in the optimization function are different than x and p above. The optimization function runs over the space of parameters of the original problem, so x_optimization == p_original.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"# use GalacticOptim.jl to solve the problem\nadtype = GalacticOptim.AutoZygote()\noptf = GalacticOptim.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = GalacticOptim.OptimizationProblem(optf, prob_neuralode.p)\n\nresult_neuralode = GalacticOptim.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\n# output\n* Status: success\n\n* Candidate solution\n   u: [4.38e-01, -6.02e-01, 4.98e-01,  ...]\n   Minimum:   8.691715e-02\n\n* Found with\n   Algorithm:     ADAM\n   Initial Point: [-3.02e-02, -5.40e-02, 2.78e-01,  ...]","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"We then complete the training using a different optimizer starting from where ADAM stopped. We do allow_f_increases=false to make the optimization automatically halt when near the minimum.","category":"page"},{"location":"examples/neural_ode_galacticoptim/","page":"Neural Ordinary Differential Equations with GalacticOptim.jl","title":"Neural Ordinary Differential Equations with GalacticOptim.jl","text":"# Retrain using the LBFGS optimizer\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = GalacticOptim.solve(optprob2,\n                                        LBFGS(),\n                                        callback = callback,\n                                        allow_f_increases = false)\n# output\n* Status: success\n\n* Candidate solution\n   u: [4.23e-01, -6.24e-01, 4.41e-01,  ...]\n   Minimum:   1.429496e-02\n\n* Found with\n   Algorithm:     L-BFGS\n   Initial Point: [4.38e-01, -6.02e-01, 4.98e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 1.46e-11 ≰ 0.0e+00\n   |x - x'|/|x'|          = 1.26e-11 ≰ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 4.28e-02 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   4  (vs limit Inf)\n   Iterations:    35\n   f(x) calls:    336\n   ∇f(x) calls:   336","category":"page"},{"location":"examples/collocation/#Smoothed-Collocation-for-Fast-Two-Stage-Training","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data. First the example and then an explanation.","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)\nsavefig(\"colloc.png\")\nplot(tsteps,du')\nsavefig(\"colloc_du.png\")\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du = dudt2(@view(u[:,i]),p)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit = initial_params(dudt2)\ncallback = function (p, l)\n  return false\nend\n\nresult_neuralode = DiffEqFlux.sciml_train(loss, pinit,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol = prob_neuralode(u0, result_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol)\nsavefig(\"colloc_trained.png\")\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\n@time numerical_neuralode = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.u,\n                                                ADAM(0.05), cb = callback,\n                                                maxiters = 300)\n\nnn_sol = prob_neuralode(u0, numerical_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)\nsavefig(\"post_trained.png\")","category":"page"},{"location":"examples/collocation/#Generating-the-Collocation","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Generating the Collocation","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"The smoothed collocation is a spline fit of the datapoints which allows us to get a an estimate of the approximate noiseless dynamics:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"We can then differentiate the smoothed function to get estimates of the derivative at each datapoint:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"plot(tsteps,du')","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Because we have (u',u) pairs, we can write a loss function that calculates the squared difference between f(u,p,t) and u' at each point, and find the parameters which minimize this difference:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du = dudt2(@view(u[:,i]),p)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit = initial_params(dudt2)\ncallback = function (p, l)\n  return false\nend\n\nresult_neuralode = DiffEqFlux.sciml_train(loss, pinit,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol = prob_neuralode(u0, result_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"While this doesn't look great, it has the characteristics of the full solution all throughout the timeseries, but it does have a drift. We can continue to optimize like this, or we can use this as the initial condition to the next phase of our fitting:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\n@time numerical_neuralode = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.u,\n                                                ADAM(0.05), cb = callback,\n                                                maxiters = 300)\n\nnn_sol = prob_neuralode(u0, numerical_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"This method then has a good global starting position, making it less prone to local minima and is thus a great method to mix in with other fitting methods for neural ODEs.","category":"page"},{"location":"controlling_AD/#Controlling-Automatic-Differentiation","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"","category":"section"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"One of the key features of DiffEqFlux.jl is the fact that it has many modes of differentiation which are available, allowing neural differential equations and universal differential equations to be fit in the manner that is most appropriate.","category":"page"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"To use the automatic differentiation overloads, the differential equation just needs to be solved with solve. Thus, for example,","category":"page"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"using DiffEqSensitivity, OrdinaryDiffEq, Zygote\r\n\r\nfunction fiip(du,u,p,t)\r\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\r\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\r\nend\r\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\r\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\r\nsol = solve(prob,Tsit5())\r\nloss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\r\ndu0,dp = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"will compute the gradient of the loss function \"sum of the values of the solution to the ODE at timepoints dt=0.1\" using an adjoint method, where du0 is the derivative of the loss function with respect to the initial condition and dp is the derivative of the loss function with respect to the parameters.","category":"page"},{"location":"controlling_AD/#Choosing-a-Differentiation-Method","page":"Controlling Automatic Differentiation","title":"Choosing a Differentiation Method","text":"","category":"section"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"The choice of the method for calculating the gradient is made by passing the keyword argument sensealg to solve. The default choice is dependent on the type of differential equation and the choice of neural network architecture.","category":"page"},{"location":"controlling_AD/","page":"Controlling Automatic Differentiation","title":"Controlling Automatic Differentiation","text":"The full listing of differentiation methods is described in the DifferentialEquations.jl documentation. That page also has guidelines on how to make the right choice.","category":"page"},{"location":"examples/delay_diffeq/#Delay-Differential-Equations","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"","category":"section"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Other differential equation problem types from DifferentialEquations.jl are supported. For example, we can build a layer with a delay differential equation like:","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using DifferentialEquations, DiffEqFlux\n\n\n# Define the same LV equation, but including a delay parameter\nfunction delay_lotka_volterra!(du, u, h, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = (α   - β*y) * h(p, t-0.1)[1]\n  du[2] = dy = (δ*x - γ)   * y\nend\n\n# Initial parameters\np = [2.2, 1.0, 2.0, 0.4]\n\n# Define a vector containing delays for each variable (although only the first\n# one is used)\nh(p, t) = ones(eltype(p), 2)\n\n# Initial conditions\nu0 = [1.0, 1.0]\n\n# Define the problem as a delay differential equation\nprob_dde = DDEProblem(delay_lotka_volterra!, u0, h, (0.0, 10.0),\n                      constant_lags = [0.1])\n\nfunction predict_dde(p)\n  return Array(solve(prob_dde, MethodOfSteps(Tsit5()),\n                              u0=u0, p=p, saveat = 0.1,\n                              sensealg = ReverseDiffAdjoint()))\nend\n\nloss_dde(p) = sum(abs2, x-1 for x in predict_dde(p))\n\n#using Plots\ncb = function (p,l...)\n  display(loss_dde(p))\n  #display(plot(solve(remake(prob_dde,p=p),MethodOfSteps(Tsit5()),saveat=0.1),ylim=(0,6)))\n  return false\nend\n\ncb(p,loss_dde(p))\n\nresult_dde = DiffEqFlux.sciml_train(loss_dde, p, cb = cb)","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Notice that we chose sensealg = ReverseDiffAdjoint() to utilize the ReverseDiff.jl reverse-mode to handle the delay differential equation.","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We define a callback to display the solution at the current parameters for each step of the training:","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"#using Plots\ncb = function (p,l...)\n  display(loss_dde(p))\n  #display(plot(solve(remake(prob_dde,p=p),MethodOfSteps(Tsit5()),saveat=0.1),ylim=(0,6)))\n  return false\nend\n\ncb(p,loss_dde(p))","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We use sciml_train to optimize the parameters for our loss function:","category":"page"},{"location":"examples/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"result_dde = DiffEqFlux.sciml_train(loss_dde, p, cb = cb)","category":"page"},{"location":"examples/SDE_control/#Controlling-Stochastic-Differential-Equations","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"In this tutorial, we show how to use DiffEqFlux to control the time evolution of a system described by a stochastic differential equations (SDE). Specifically, we consider a continuously monitored qubit described by an SDE in the Ito sense with multiplicative scalar noise (see [1] for a reference):","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"dψ = b(ψ(t) Ω(t))ψ(t) dt + σ(ψ(t))ψ(t) dW_t ","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use a predictive model to map the quantum state of the qubit, ψ(t), at each time to the control parameter Ω(t) which rotates the quantum state about the x-axis of the Bloch sphere to ultimately prepare and stabilize the qubit in the excited state.","category":"page"},{"location":"examples/SDE_control/#Copy-Pasteable-Code","page":"Controlling Stochastic Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# load packages\nusing DiffEqFlux\nusing StochasticDiffEq, DiffEqCallbacks, DiffEqNoiseProcess\nusing Statistics, LinearAlgebra, Random\nusing Plots\n\n\n#################################################\nlr = 0.01f0\nepochs = 100\n\nnumtraj = 16 # number of trajectories in parallel simulations for training\nnumtrajplot = 32 # .. for plotting\n\n# time range for the solver\ndt = 0.0005f0\ntinterval = 0.05f0\ntstart = 0.0f0\nNintervals = 20 # total number of intervals, total time = t_interval*Nintervals\ntspan = (tstart,tinterval*Nintervals)\nts = Array(tstart:dt:(Nintervals*tinterval+dt)) # time array for noise grid\n\n# Hamiltonian parameters\nΔ = 20.0f0\nΩmax = 10.0f0 # control parameter (maximum amplitude)\nκ = 0.3f0\n\n# loss hyperparameters\nC1 = Float32(1.0)  # evolution state fidelity\n\nstruct Parameters{flType,intType,tType}\n  lr::flType\n  epochs::intType\n  numtraj::intType\n  numtrajplot::intType\n  dt::flType\n  tinterval::flType\n  tspan::tType\n  Nintervals::intType\n  ts::Vector{flType}\n  Δ::flType\n  Ωmax::flType\n  κ::flType\n  C1::flType\nend\n\nmyparameters = Parameters{typeof(dt),typeof(numtraj), typeof(tspan)}(\n  lr, epochs, numtraj, numtrajplot, dt, tinterval, tspan, Nintervals, ts,\n  Δ, Ωmax, κ, C1)\n\n################################################\n# Define Neural Network\n\n# state-aware\nnn = FastChain(\n  FastDense(4, 32, relu),\n  FastDense(32, 1, tanh))\n\np_nn = initial_params(nn) # random initial parameters\n\n\n###############################################\n# initial state anywhere on the Bloch sphere\nfunction prepare_initial(dt, n_par)\n  # shape 4 x n_par\n  # input number of parallel realizations and dt for type inference\n  # random position on the Bloch sphere\n  theta = acos.(2*rand(typeof(dt),n_par).-1)  # uniform sampling for cos(theta) between -1 and 1\n  phi = rand(typeof(dt),n_par)*2*pi  # uniform sampling for phi between 0 and 2pi\n  # real and imaginary parts ceR, cdR, ceI, cdI\n  u0 = [cos.(theta/2), sin.(theta/2).*cos.(phi), false*theta, sin.(theta/2).*sin.(phi)]\n  return vcat(transpose.(u0)...) # build matrix\nend\n\n# target state\n# ψtar = |up>\n\nu0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n\n###############################################\n# Define SDE\n\nfunction qubit_drift!(du,u,p,t)\n  # expansion coefficients |Ψ> = ce |e> + cd |d>\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  # Δ: atomic frequency\n  # Ω: Rabi frequency for field in x direction\n  # κ: spontaneous emission\n  Δ, Ωmax, κ = p[end-2:end]\n  nn_weights = p[1:end-3]\n  Ω = (nn(u, nn_weights).*Ωmax)[1]\n\n  @inbounds begin\n    du[1] = 1//2*(ceI*Δ-ceR*κ+cdI*Ω)\n    du[2] = -cdI*Δ/2 + 1*ceR*(cdI*ceI+cdR*ceR)*κ+ceI*Ω/2\n    du[3] = 1//2*(-ceR*Δ-ceI*κ-cdR*Ω)\n    du[4] = cdR*Δ/2 + 1*ceI*(cdI*ceI+cdR*ceR)*κ-ceR*Ω/2\n  end\n  return nothing\nend\n\nfunction qubit_diffusion!(du,u,p,t)\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  κ = p[end]\n\n  du .= false\n\n  @inbounds begin\n    #du[1] = zero(ceR)\n    du[2] += sqrt(κ)*ceR\n    #du[3] = zero(ceR)\n    du[4] += sqrt(κ)*ceI\n  end\n  return nothing\nend\n\n# normalization callback\ncondition(u,t,integrator) = true\nfunction affect!(integrator)\n  integrator.u=integrator.u/norm(integrator.u)\nend\ncb = DiscreteCallback(condition,affect!,save_positions=(false,false))\n\nCreateGrid(t,W1) = NoiseGrid(t,W1)\nZygote.@nograd CreateGrid #avoid taking grads of this function\n\n# set scalar random process\nW = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\nW1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\nNG = CreateGrid(myparameters.ts,W1)\n\n# get control pulses\np_all = [p_nn; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n# define SDE problem\nprob = SDEProblem{true}(qubit_drift!, qubit_diffusion!, vec(u0[:,1]), myparameters.tspan, p_all,\n   callback=cb, noise=NG\n   )\n\n#########################################\n# compute loss\nfunction g(u,p,t)\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  p[1]*mean((cdR.^2 + cdI.^2) ./ (ceR.^2 + cdR.^2 + ceI.^2 + cdI.^2))\nend\n\n\nfunction loss(p, u0, prob::SDEProblem, myparameters::Parameters;\n\t alg=EM(), sensealg = BacksolveAdjoint()\n\t )\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:,i]))\n    W = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts,W1)\n\n    remake(prob,\n      p = pars,\n      u0 = u0tmp,\n      callback = cb,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n   prob_func = prob_func,\n   safetycopy = true\n   )\n\n  _sol = solve(ensembleprob, alg, EnsembleThreads(),\n    sensealg=sensealg,\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false,\n    trajectories=myparameters.numtraj, batch_size=myparameters.numtraj)\n  A = convert(Array,_sol)\n\n  loss = g(A,[myparameters.C1],nothing)\n\n  return loss\nend\n\n#########################################\n# visualization -- run for new batch\nfunction visualize(p, u0, prob::SDEProblem, myparameters::Parameters;\n   alg=EM(),\n   )\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:,i]))\n    W = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts,W1)\n\n    remake(prob,\n      p = pars,\n      u0 = u0tmp,\n      callback = cb,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n   prob_func = prob_func,\n   safetycopy = true\n   )\n\n  u = solve(ensembleprob, alg, EnsembleThreads(),\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false, #abstol=1e-6, reltol=1e-6,\n    trajectories=myparameters.numtrajplot, batch_size=myparameters.numtrajplot)\n\n\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  infidelity = @. (cdR^2 + cdI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n  meaninfidelity = mean(infidelity)\n  loss = myparameters.C1*meaninfidelity\n\n  @info \"Loss: \" loss\n\n  fidelity = @. (ceR^2 + ceI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n\n  mf = mean(fidelity, dims=2)[:]\n  sf = std(fidelity, dims=2)[:]\n\n  pl1 = plot(0:myparameters.Nintervals, mf,\n    ribbon = sf,\n    ylim = (0,1), xlim = (0,myparameters.Nintervals),\n    c=1, lw = 1.5, xlabel = \"steps i\", ylabel=\"Fidelity\", legend=false)\n\n  pl = plot(pl1, legend = false, size=(400,360))\n  return pl, loss\nend\n\n###################################\n# training loop\n@info \"Start Training..\"\n\n# optimize the parameters for a few epochs with ADAM on time span Nint\nopt = ADAM(myparameters.lr)\nlist_plots = []\nlosses = []\nfor epoch in 1:myparameters.epochs\n  println(\"epoch: $epoch / $(myparameters.epochs)\")\n  local u0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n  _dy, back = @time Zygote.pullback(p -> loss(p, u0, prob, myparameters,\n    sensealg=BacksolveAdjoint()\n  ), p_nn)\n  @show _dy\n  gs = @time back(one(_dy))[1]\n  # store loss\n  push!(losses, _dy)\n  if (epoch % myparameters.epochs == 0) || (epoch == 1)\n    # plot/store every xth epoch\n    @info \"plotting..\"\n    local u0 = prepare_initial(myparameters.dt, myparameters.numtrajplot)\n    pl, test_loss = visualize(p_nn, u0, prob, myparameters)\n    println(\"Loss (epoch: $epoch): $test_loss\")\n    display(pl)\n    push!(list_plots, pl)\n  end\n  Flux.Optimise.update!(opt, p_nn, gs)\n  println(\"\")\nend\n\n# plot training loss\npl = plot(losses, lw = 1.5, xlabel = \"some epochs\", ylabel=\"Loss\", legend=false)\n\nsavefig(display(list_plots[end], \"fidelity.png\")","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"Output:","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"[ Info: Start Training..\nepoch: 1 / 100\n38.519219 seconds (85.38 M allocations: 4.316 GiB, 3.37% gc time)\n_dy = 0.63193643f0\n 26.232970 seconds (122.33 M allocations: 5.899 GiB, 7.26% gc time)\n ...\n\n[ Info: plotting..\n┌ Info: Loss:\n└   loss = 0.11777343f0\nLoss (epoch: 100): 0.11777343","category":"page"},{"location":"examples/SDE_control/#Step-by-step-description","page":"Controlling Stochastic Differential Equations","title":"Step-by-step description","text":"","category":"section"},{"location":"examples/SDE_control/#Load-packages","page":"Controlling Stochastic Differential Equations","title":"Load packages","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"using DiffEqFlux\nusing StochasticDiffEq, DiffEqCallbacks, DiffEqNoiseProcess\nusing Statistics, LinearAlgebra, Random\nusing Plots","category":"page"},{"location":"examples/SDE_control/#Parameters","page":"Controlling Stochastic Differential Equations","title":"Parameters","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We define the parameters of the qubit and hyper-parameters of the training process.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"lr = 0.01f0\nepochs = 100\n\nnumtraj = 16 # number of trajectories in parallel simulations for training\nnumtrajplot = 32 # .. for plotting\n\n# time range for the solver\ndt = 0.0005f0\ntinterval = 0.05f0\ntstart = 0.0f0\nNintervals = 20 # total number of intervals, total time = t_interval*Nintervals\ntspan = (tstart,tinterval*Nintervals)\nts = Array(tstart:dt:(Nintervals*tinterval+dt)) # time array for noise grid\n\n# Hamiltonian parameters\nΔ = 20.0f0\nΩmax = 10.0f0 # control parameter (maximum amplitude)\nκ = 0.3f0\n\n# loss hyperparameters\nC1 = Float32(1.0)  # evolution state fidelity\n\nstruct Parameters{flType,intType,tType}\n  lr::flType\n  epochs::intType\n  numtraj::intType\n  numtrajplot::intType\n  dt::flType\n  tinterval::flType\n  tspan::tType\n  Nintervals::intType\n  ts::Vector{flType}\n  Δ::flType\n  Ωmax::flType\n  κ::flType\n  C1::flType\nend\n\nmyparameters = Parameters{typeof(dt),typeof(numtraj), typeof(tspan)}(\n  lr, epochs, numtraj, numtrajplot, dt, tinterval, tspan, Nintervals, ts,\n  Δ, Ωmax, κ, C1)","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"In plain terms, the quantities that were defined are:","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"lr = learning rate of the optimizer\nepochs = number of epochs in the training process\nnumtraj = number of simulated trajectories in the training process\nnumtrajplot = number of simulated trajectories to visualize the performance\ndt = time step for solver (initial dt if adaptive)\ntinterval = time spacing between checkpoints\ntspan = time span\nNintervals = number of checkpoints\nts = discretization of the entire time interval, used for NoiseGrid\nΔ = detuning between the qubit and the laser\nΩmax = maximum frequency of the control laser\nκ = decay rate\nC1 = loss function hyper-parameter","category":"page"},{"location":"examples/SDE_control/#Controller","page":"Controlling Stochastic Differential Equations","title":"Controller","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use a neural network to control the parameter Ω(t). Alternatively, one could also, e.g., use tensor layers.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# state-aware\nnn = FastChain(\n  FastDense(4, 32, relu),\n  FastDense(32, 1, tanh))\n\np_nn = initial_params(nn) # random initial parameters","category":"page"},{"location":"examples/SDE_control/#Initial-state","page":"Controlling Stochastic Differential Equations","title":"Initial state","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We prepare n_par initial states, uniformly distributed over the Bloch sphere. To avoid complex numbers in our simulations, we split the state of the qubit","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"  ψ(t) = c_e(t) (10) + c_d(t) (01)","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"into its real and imaginary part.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# initial state anywhere on the Bloch sphere\nfunction prepare_initial(dt, n_par)\n  # shape 4 x n_par\n  # input number of parallel realizations and dt for type inference\n  # random position on the Bloch sphere\n  theta = acos.(2*rand(typeof(dt),n_par).-1)  # uniform sampling for cos(theta) between -1 and 1\n  phi = rand(typeof(dt),n_par)*2*pi  # uniform sampling for phi between 0 and 2pi\n  # real and imaginary parts ceR, cdR, ceI, cdI\n  u0 = [cos.(theta/2), sin.(theta/2).*cos.(phi), false*theta, sin.(theta/2).*sin.(phi)]\n  return vcat(transpose.(u0)...) # build matrix\nend\n\n# target state\n# ψtar = |e>\n\nu0 = prepare_initial(myparameters.dt, myparameters.numtraj)","category":"page"},{"location":"examples/SDE_control/#Defining-the-SDE","page":"Controlling Stochastic Differential Equations","title":"Defining the SDE","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We define the drift and diffusion term of the qubit. The SDE doesn't preserve the norm of the quantum state. To ensure the normalization of the state, we add a DiscreteCallback after each time step. Further, we use a NoiseGrid from the DiffEqNoiseProcess package, as one possibility to simulate a 1D Brownian motion. Note that the NN is placed directly into the drift function, thus the control parameter Ω is continuously updated.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# Define SDE\nfunction qubit_drift!(du,u,p,t)\n  # expansion coefficients |Ψ> = ce |e> + cd |d>\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  # Δ: atomic frequency\n  # Ω: Rabi frequency for field in x direction\n  # κ: spontaneous emission\n  Δ, Ωmax, κ = p[end-2:end]\n  nn_weights = p[1:end-3]\n  Ω = (nn(u, nn_weights).*Ωmax)[1]\n\n  @inbounds begin\n    du[1] = 1//2*(ceI*Δ-ceR*κ+cdI*Ω)\n    du[2] = -cdI*Δ/2 + 1*ceR*(cdI*ceI+cdR*ceR)*κ+ceI*Ω/2\n    du[3] = 1//2*(-ceR*Δ-ceI*κ-cdR*Ω)\n    du[4] = cdR*Δ/2 + 1*ceI*(cdI*ceI+cdR*ceR)*κ-ceR*Ω/2\n  end\n  return nothing\nend\n\nfunction qubit_diffusion!(du,u,p,t)\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  κ = p[end]\n\n  du .= false\n\n  @inbounds begin\n    #du[1] = zero(ceR)\n    du[2] += sqrt(κ)*ceR\n    #du[3] = zero(ceR)\n    du[4] += sqrt(κ)*ceI\n  end\n  return nothing\nend\n\n# normalization callback\ncondition(u,t,integrator) = true\nfunction affect!(integrator)\n  integrator.u=integrator.u/norm(integrator.u)\nend\ncb = DiscreteCallback(condition,affect!,save_positions=(false,false))\n\nCreateGrid(t,W1) = NoiseGrid(t,W1)\nZygote.@nograd CreateGrid #avoid taking grads of this function\n\n# set scalar random process\nW = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\nW1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\nNG = CreateGrid(myparameters.ts,W1)\n\n# get control pulses\np_all = [p_nn; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n# define SDE problem\nprob = SDEProblem{true}(qubit_drift!, qubit_diffusion!, vec(u0[:,1]), myparameters.tspan, p_all,\n   callback=cb, noise=NG\n   )","category":"page"},{"location":"examples/SDE_control/#Compute-loss-function","page":"Controlling Stochastic Differential Equations","title":"Compute loss function","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We'd like to prepare the excited state of the qubit. An appropriate choice for the loss function is the infidelity of the state ψ(t) with respect to the excited state. We create a parallelized EnsembleProblem, where the prob_func creates a new NoiseGrid for every trajectory and loops over the initial states. The number of parallel trajectories and the used batch size can be tuned by the kwargs trajectories=.. and batchsize=.. in the solve call. See also the parallel ensemble simulation docs for a description of the available ensemble algorithms. To optimize only the parameters of the neural network, we use pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# compute loss\nfunction g(u,p,t)\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  p[1]*mean((cdR.^2 + cdI.^2) ./ (ceR.^2 + cdR.^2 + ceI.^2 + cdI.^2))\nend\n\n\nfunction loss(p, u0, prob::SDEProblem, myparameters::Parameters;\n\t alg=EM(), sensealg = BacksolveAdjoint()\n\t )\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:,i]))\n    W = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts,W1)\n\n    remake(prob,\n      p = pars,\n      u0 = u0tmp,\n      callback = cb,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n   prob_func = prob_func,\n   safetycopy = true\n   )\n\n  _sol = solve(ensembleprob, alg, EnsembleThreads(),\n    sensealg=sensealg,\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false,\n    trajectories=myparameters.numtraj, batch_size=myparameters.numtraj)\n  A = convert(Array,_sol)\n\n  loss = g(A,[myparameters.C1],nothing)\n\n  return loss\nend","category":"page"},{"location":"examples/SDE_control/#Visualization","page":"Controlling Stochastic Differential Equations","title":"Visualization","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"To visualize the performance of the controller, we plot the mean value and standard deviation of the fidelity of a bunch of trajectories (myparameters.numtrajplot) as a function of the time steps at which loss values are computed.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"function visualize(p, u0, prob::SDEProblem, myparameters::Parameters;\n   alg=EM(),\n   )\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:,i]))\n    W = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts,W1)\n\n    remake(prob,\n      p = pars,\n      u0 = u0tmp,\n      callback = cb,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n   prob_func = prob_func,\n   safetycopy = true\n   )\n\n  u = solve(ensembleprob, alg, EnsembleThreads(),\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false, #abstol=1e-6, reltol=1e-6,\n    trajectories=myparameters.numtrajplot, batch_size=myparameters.numtrajplot)\n\n\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  infidelity = @. (cdR^2 + cdI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n  meaninfidelity = mean(infidelity)\n  loss = myparameters.C1*meaninfidelity\n\n  @info \"Loss: \" loss\n\n  fidelity = @. (ceR^2 + ceI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n\n  mf = mean(fidelity, dims=2)[:]\n  sf = std(fidelity, dims=2)[:]\n\n  pl1 = plot(0:myparameters.Nintervals, mf,\n    ribbon = sf,\n    ylim = (0,1), xlim = (0,myparameters.Nintervals),\n    c=1, lw = 1.5, xlabel = \"steps i\", ylabel=\"Fidelity\", legend=false)\n\n  pl = plot(pl1, legend = false, size=(400,360))\n  return pl, loss\nend","category":"page"},{"location":"examples/SDE_control/#Training","page":"Controlling Stochastic Differential Equations","title":"Training","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use the ADAM optimizer to optimize the parameters of the neural network. In each epoch, we draw new initial quantum states, compute the forward evolution, and, subsequently, the gradients of the loss function with respect to the parameters of the neural network. sensealg allows one to switch between the different sensitivity modes. InterpolatingAdjoint and BacksolveAdjoint are the two possible continuous adjoint sensitivity methods. The necessary correction between Ito and Stratonovich integrals is computed under the hood in the DiffEqSensitivity package.","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# optimize the parameters for a few epochs with ADAM on time span Nint\nopt = ADAM(myparameters.lr)\nlist_plots = []\nlosses = []\nfor epoch in 1:myparameters.epochs\n  println(\"epoch: $epoch / $(myparameters.epochs)\")\n  local u0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n  _dy, back = @time Zygote.pullback(p -> loss(p, u0, prob, myparameters,\n    sensealg=BacksolveAdjoint()\n  ), p_nn)\n  @show _dy\n  gs = @time back(one(_dy))[1]\n  # store loss\n  push!(losses, _dy)\n  if (epoch % myparameters.epochs == 0) || (epoch == 1)\n    # plot/store every xth epoch\n    @info \"plotting..\"\n    local u0 = prepare_initial(myparameters.dt, myparameters.numtrajplot)\n    pl, test_loss = visualize(p_nn, u0, prob, myparameters)\n    println(\"Loss (epoch: $epoch): $test_loss\")\n    display(pl)\n    push!(list_plots, pl)\n  end\n  Flux.Optimise.update!(opt, p_nn, gs)\n  println(\"\")\nend","category":"page"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"(Image: Evolution of the fidelity as a function of time)","category":"page"},{"location":"examples/SDE_control/#References","page":"Controlling Stochastic Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"[1] Schäfer, Frank, Pavel Sekatski, Martin Koppenhöfer, Christoph Bruder, and Michal Kloc. \"Control of stochastic quantum dynamics by differentiable programming.\" Machine Learning: Science and Technology 2, no. 3 (2021): 035004.","category":"page"},{"location":"examples/bouncing_ball/#Bouncing-Ball-Hybrid-ODE-Optimization","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"","category":"section"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"The bouncing ball is a classic hybrid ODE which can be represented in the DifferentialEquations.jl event handling system. This can be applied to ODEs, SDEs, DAEs, DDEs, and more. Let's now add the DiffEqFlux machinery to this problem in order to optimize the friction that's required to match data. Assume we have data for the ball's height after 15 seconds. Let's first start by implementing the ODE:","category":"page"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"using DiffEqFlux, DifferentialEquations\n\nfunction f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]\nend\n\nfunction condition(u,t,integrator) # Event when event_f(u,t) == 0\n  u[1]\nend\n\nfunction affect!(integrator)\n  integrator.u[2] = -integrator.p[2]*integrator.u[2]\nend\n\ncb = ContinuousCallback(condition,affect!)\nu0 = [50.0,0.0]\ntspan = (0.0,15.0)\np = [9.8, 0.8]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"Here we have a friction coefficient of 0.8. We want to refine this coefficient to find the value so that the predicted height of the ball at the endpoint is 20. We do this by minimizing a loss function against the value 20:","category":"page"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"function loss(θ)\n  sol = solve(prob,Tsit5(),p=[9.8,θ[1]],callback=cb)\n  target = 20.0\n  abs2(sol[end][1] - target)\nend\n\nloss([0.8])\n@time res = DiffEqFlux.sciml_train(loss,[0.8])\n@show res.u # [0.866554105436901]","category":"page"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"This runs in about 0.091215 seconds (533.45 k allocations: 80.717 MiB) and finds an optimal drag coefficient.","category":"page"},{"location":"examples/bouncing_ball/#Note-on-Sensitivity-Methods","page":"Bouncing Ball Hybrid ODE Optimization","title":"Note on Sensitivity Methods","text":"","category":"section"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"The continuous adjoint sensitivities BacksolveAdjoint, InterpolatingAdjoint, and QuadratureAdjoint are compatible with events for ODEs. BacksolveAdjoint and InterpolatingAdjoint can also handle events for SDEs. Use BacksolveAdjoint if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.","category":"page"},{"location":"examples/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"All methods based on discrete sensitivity analysis via automatic differentiation, like ReverseDiffAdjoint, TrackerAdjoint, or ForwardDiffSensitivity are the methods to use (and ReverseDiffAdjoint is demonstrated above), are compatible with events. This applies to SDEs, DAEs, and DDEs as well.","category":"page"},{"location":"examples/universal_diffeq/#Universal-Ordinary,-Stochastic,-and-Partial-Differential-Equation-Examples","page":"Universal Ordinary, Stochastic, and Partial Differential Equation Examples","title":"Universal Ordinary, Stochastic, and Partial Differential Equation Examples","text":"","category":"section"},{"location":"examples/universal_diffeq/","page":"Universal Ordinary, Stochastic, and Partial Differential Equation Examples","title":"Universal Ordinary, Stochastic, and Partial Differential Equation Examples","text":"For examples of using universal ordinary and stochastic differential equations, along with universal partial differential equations, see the Universal Differential Equations for Scientific Machine Learning paper along with its examples repository","category":"page"},{"location":"examples/neural_ode_sciml/#Neural-Ordinary-Differential-Equations-with-sciml_train","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"","category":"section"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"DiffEqFlux.jl defines sciml_train which is a high level utility that automates a lot of the choices, using heuristics to determine a potentially efficient method. However, in some cases you may want more control over the optimization process. In this example we will use this utility to train a neural ODE to some generated data. A neural ODE is an ODE where a neural network defines its derivative function. Thus for example, with the multilayer perceptron neural network FastChain(FastDense(2, 50, tanh), FastDense(50, 2)), we obtain  the following results.","category":"page"},{"location":"examples/neural_ode_sciml/#Copy-Pasteable-Code","page":"Neural Ordinary Differential Equations with sciml_train","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"using DiffEqFlux, DifferentialEquations, Plots, GalacticOptim\n\nu0 = Float32[2.0; 0.0] # Initial condition\ndatasize = 30 # Number of data points\ntspan = (0.0f0, 1.5f0) # Time range\ntsteps = range(tspan[1], tspan[2], length = datasize) # Split time range into equal steps for each data point\n\n# Function that will generate the data we are trying to fit\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)' # Need transposes to make the matrix multiplication work\nend\n\n# Define the problem with the function above\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Solve and take just the solution array\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n# Make a neural net with a NeuralODE layer\ndudt2 = FastChain((x, p) -> x.^3, # Guess a cubic function\n                  FastDense(2, 50, tanh), # Multilayer perceptron for the part we don't know\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\n# Array of predictions from NeuralODE with parameters p starting at initial condition u0\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred) # Just sum of squared error\n    return loss, pred\nend\n\n# Callback function to observe training\ncallback = function (p, l, pred; doplot = true)\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\n# Parameters are prob_neuralode.p\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          cb = callback)","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"(Image: Neural ODE)","category":"page"},{"location":"examples/neural_ode_sciml/#Explanation","page":"Neural Ordinary Differential Equations with sciml_train","title":"Explanation","text":"","category":"section"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Let's generate a time series array from a cubic equation as data:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Now let's define a neural network with a NeuralODE layer. First we define the layer. Here we're going to use FastChain, which is a faster neural network structure for NeuralODEs:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Note that we can directly use Chains from Flux.jl as well, for example:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"dudt2 = Chain(x -> x.^3,\n              Dense(2, 50, tanh),\n              Dense(50, 2))","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"In our model we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but we need a good guess!","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"From here, we build a loss function around our NeuralODE. NeuralODE has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We define a callback function.","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"# Callback function to observe training\ncallback = function (p, l, pred; doplot = false)\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"We then train the neural network to learn the ODE. sciml_train chooses heuristics that train quickly and simply:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"result_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          cb = callback)","category":"page"},{"location":"examples/neural_ode_sciml/#Usage-Without-the-Layer-Function","page":"Neural Ordinary Differential Equations with sciml_train","title":"Usage Without the Layer Function","text":"","category":"section"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"Note that you can equivalently define the NeuralODE by hand instead of using the NeuralODE. With FastChain this would look like:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"dudt!(u, p, t) = dudt2(u, p)\nu0 = rand(2)\nprob = ODEProblem(dudt!, u0, tspan, p)\nmy_neural_ode_prob = solve(prob, Tsit5(), args...; kwargs...)","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"and with Chain this would look like:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"p, re = Flux.destructure(dudt2)\nneural_ode_f(u, p, t) = re(p)(u)\nu0 = rand(2)\nprob = ODEProblem(neural_ode_f, u0, tspan, p)\nmy_neural_ode_prob = solve(prob, Tsit5(), args...; kwargs...)","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"and then one would use solve for the prediction like in other tutorials.","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"In total, the 'from-scratch' form looks like:","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"using DiffEqFlux, DifferentialEquations, Plots, GalacticOptim\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\ndudt!(u, p, t) = dudt2(u, p)\nu0 = rand(2)\nprob_neuralode = ODEProblem(dudt!, u0, tspan, initial_params(dudt2))\nsol_node = solve(prob_neuralode, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  tmp_prob = remake(prob_neuralode, p = p)\n  Array(solve(tmp_prob, Tsit5(), saveat = tsteps))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\ncallback = function (p, l, pred; doplot = true)\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p, cb = callback)","category":"page"},{"location":"examples/neural_ode_sciml/","page":"Neural Ordinary Differential Equations with sciml_train","title":"Neural Ordinary Differential Equations with sciml_train","text":"(Image: neural ode result)","category":"page"},{"location":"#DiffEqFlux:-Generalized-Physics-Informed-and-Scientific-Machine-Learning-(SciML)","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"DiffEqFlux.jl is a parameter estimation system for the SciML ecosystem. It is a high level interface that pulls together all of the tools with heuristics and helper functions to make solving inverse problems and inferring models as easy as possible without losing efficiency.","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"DiffEqFlux.jl is for universal differential equations, where these can include delays, physical constraints, stochasticity, events, and all other kinds of interesting behavior that shows up in scientific simulations. Neural networks can be all or part of the model. They can be around the differential equation, in the cost function, or inside of the differential equation. Neural networks representing unknown portions of the model or functions can go anywhere you have uncertainty in the form of the scientific simulator. Forward sensitivity and adjoint equations are automatically generated with checkpointing and stabilization to ensure it works for large stiff equations, while specializations on static objects allows for high efficiency on small equations. For an overview of the topic with applications, consult the paper Universal Differential Equations for Scientific Machine Learning.","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"You can efficiently use the package for:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Parameter estimation of scientific models (ODEs, SDEs, DDEs, DAEs, etc.)\nNeural ODEs, Neural SDE, etc.\nNonlinear optimal control, including training neural controllers\n(Stiff) universal ordinary differential equations (universal ODEs)\nUniversal stochastic differential equations (universal SDEs)\nUniversal delay differential equations (universal DDEs)\nUniversal partial differential equations (universal PDEs)\nUniversal jump stochastic differential equations (universal jump diffusions)\nHybrid universal differential equations (universal DEs with event handling)","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"with high order, adaptive, implicit, GPU-accelerated, Newton-Krylov, etc. methods. For examples, please refer to the release blog post (which we try to keep updated for changes to the libraries). Additional demonstrations, like neural PDEs and neural jump SDEs, can be found at this blog post (among many others!). All of these features are only part of the advantage, as this library routinely benchmarks orders of magnitude faster than competing libraries like torchdiffeq. Use with GPUs is highly optimized by recompiling the solvers to GPUs to remove all CPU-GPU data transfers, while use with CPUs uses specialized kernels for accelerating differential equation solves.","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Many different training techniques are supported by this package, including:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Optimize-then-discretize (backsolve adjoints, checkpointed adjoints, quadrature adjoints)\nDiscretize-then-optimize (forward and reverse mode discrete sensitivity analysis)\nThis is a generalization of ANODE and ANODEv2 to all DifferentialEquations.jl ODE solvers\nHybrid approaches (adaptive time stepping + AD for adaptive discretize-then-optimize)\nCollocation approaches (two-stage methods, multiple shooting, etc.)\nO(1) memory backprop of ODEs via BacksolveAdjoint, and Virtual Brownian Trees for O(1) backprop of SDEs\nContinuous adjoints for integral loss functions\nProbabilistic programming and variational inference on ODEs/SDEs/DAEs/DDEs/hybrid equations etc. is provided by integration with Turing.jl and Gen.jl. Reproduce variational loss functions by plugging composible libraries together.","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"all while mixing forward mode and reverse mode approaches as appropriate for the most speed. For more details on the adjoint sensitivity analysis methods for computing fast gradients, see the Adjoints page.","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"With this package, you can explore various ways to integrate the two methodologies:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Neural networks can be defined where the “activations” are nonlinear functions described by differential equations\nNeural networks can be defined where some layers are ODE solves\nODEs can be defined where some terms are neural networks\nCost functions on ODEs can define neural networks","category":"page"},{"location":"#Basics","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"Basics","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"The basics are all provided by the DifferentialEquations.jl package. Specifically, the solve function is automatically compatible with AD systems like Zygote.jl and thus there is no machinery that is necessary to use DifferentialEquations.jl package. For example, the following computes the solution to an ODE and computes the gradient of a loss function (the sum of the ODE's output at each timepoint with dt=0.1) via the adjoint method:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"using DiffEqSensitivity, OrdinaryDiffEq, Zygote\n\nfunction fiip(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5())\nloss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\ndu01,dp1 = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Thus, what DiffEqFlux.jl provides is:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"A bunch of tutorials, documentation, and test cases for this combination with neural network libraries and GPUs\nPre-built layer functions for common use cases, like neural ODEs\nSpecialized layer functions (FastDense) to improve neural differential equation training performance\nCompatibility with a multifunctional optimization package GalacticOptim.jl with a training loop that allows non-machine learning libraries to be easily utilized","category":"page"},{"location":"#Applications","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"Applications","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"The approach of this package is the efficient training of Universal Differential Equations. Since this is a fairly general class of problems, the following applications are readily available as specific instances of this methodology, and are showcased in tutorials and layer functions:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Neural ODEs\nNeural SDEs\nNeural DAEs\nNeural DDEs\nAugmented Neural ODEs\nGraph Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nLagrangian Neural Networks\nContinuous Normalizing Flows (CNF) and FFJORD\nGalerkin Neural ODEs","category":"page"},{"location":"#Modularity-and-Composability","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"Modularity and Composability","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Note that DiffEqFlux.jl purely built on composable and modular infrastructure. In fact, DiffEqFlux.jl's functions are not even directly required for performing many of these operations! DiffEqFlux provides high level helper functions and documentation for the user, but the code generation stack is modular and composes in many different ways. For example, one can use and swap out the ODE solver between any common interface compatible library, like:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"Sundials.jl\nOrdinaryDiffEq.jl\nLSODA.jl\nIRKGaussLegendre.jl\nSciPyDiffEq.jl\n... etc. many other choices!","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"In addition, due to the composability of the system, none of the components are directly tied to the Flux.jl machine learning framework. For example, you can use DiffEqFlux.jl to generate TensorFlow graphs and train the neural network with TensorFlow.jl, utilize PyTorch arrays via Torch.jl, and more all with single line code changes by utilizing the underlying code generation. The tutorials shown here are thus mostly a guide on how to use the ecosystem as a whole, only showing a small snippet of the possible ways to compose the thousands of differentiable libraries together! Swap out ODEs for SDEs, DDEs, DAEs, etc., put quadrature libraries or Tullio.jl in the loss function, the world is your oyster!","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"As a proof of composability, note that the implementation of Bayesian neural ODEs required zero code changes to the library, and instead just relied on the composability with other Julia packages.","category":"page"},{"location":"#Citation","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"Citation","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"","page":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","title":"DiffEqFlux.jl: Generalized Physics-Informed and Scientific Machine Learning (SciML)","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"layers/SplineLayer/#Spline-Layer","page":"Spline Layer","title":"Spline Layer","text":"","category":"section"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Constructs a Spline Layer. At a high-level, it performs the following:","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Takes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"SplineLayer","category":"page"},{"location":"layers/SplineLayer/#DiffEqFlux.SplineLayer","page":"Spline Layer","title":"DiffEqFlux.SplineLayer","text":"Constructs a Spline Layer. At a high-level, it performs the following:\n\nTakes as input a one-dimensional training dataset, a time span, a time step and\n\nan interpolation method.\n\nDuring training, adjusts the values of the function at multiples of the time-step\n\nsuch that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.\n\nSplineLayer(time_span,time_step,spline_basis,saved_points=nothing)\n\nArguments:\n\ntime_span: Tuple of real numbers corresponding to the time span.\ntime_step: Real number corresponding to the time step.\nspline_basis: Interpolation method to be used yb the basis (current supported interpolation methods: ConstantInterpolation, LinearInterpolation, QuadraticInterpolation, QuadraticSpline, CubicSpline).\n'saved_points': values of the function at multiples of the time step. Initialized by default\n\nto a random vector sampled from the unit normal.\n\n\n\n\n\n","category":"type"},{"location":"examples/second_order_adjoints/#Newton-and-Hessian-Free-Newton-Krylov-with-Second-Order-Adjoint-Sensitivity-Analysis","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"","category":"section"},{"location":"examples/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"In many cases it may be more optimal or more stable to fit using second order Newton-based optimization techniques. Since DiffEqSensitivity.jl provides second order sensitivity analysis for fast Hessians and Hessian-vector products (via forward-over-reverse), we can utilize these in our neural/universal differential equation training processes.","category":"page"},{"location":"examples/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"sciml_train is setup to automatically use second order sensitivity analysis methods if a second order optimizer is requested via Optim.jl. Thus Newton and NewtonTrustRegion optimizers will use a second order Hessian-based optimization, while KrylovTrustRegion will utilize a Krylov-based method with Hessian-vector products (never forming the Hessian) for large parameter optimizations.","category":"page"},{"location":"examples/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"using DiffEqFlux, DifferentialEquations, Plots\r\n\r\nu0 = Float32[2.0; 0.0]\r\ndatasize = 30\r\ntspan = (0.0f0, 1.5f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\ndudt2 = FastChain((x, p) -> x.^3,\r\n                  FastDense(2, 50, tanh),\r\n                  FastDense(50, 2))\r\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\r\np = prob_neuralode.p\r\n\r\nfunction predict_neuralode(p)\r\n  Array(prob_neuralode(u0, p))\r\nend\r\n\r\nfunction loss_neuralode(p)\r\n    pred = predict_neuralode(p)\r\n    loss = sum(abs2, ode_data .- pred)\r\n    return loss, pred\r\nend\r\n\r\n# Callback function to observe training\r\nlist_plots = []\r\niter = 0\r\ncb = function (p, l, pred; doplot = false)\r\n  global list_plots, iter\r\n\r\n  if iter == 0\r\n    list_plots = []\r\n  end\r\n  iter += 1\r\n\r\n  display(l)\r\n\r\n  # plot current prediction against data\r\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\r\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\r\n  push!(list_plots, plt)\r\n  if doplot\r\n    display(plot(plt))\r\n  end\r\n\r\n  return l < 0.01\r\nend\r\n\r\npstart = DiffEqFlux.sciml_train(loss_neuralode, p, ADAM(0.01), cb=cb, maxiters = 100).u\r\npmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, NewtonTrustRegion(), cb=cb, maxiters = 200)\r\npmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, Optim.KrylovTrustRegion(), cb=cb, maxiters = 200)","category":"page"},{"location":"examples/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"Note that we do not demonstrate Newton() because we have not found a single case where it is competitive with the other two methods. KrylovTrustRegion() is generally the fastest due to its use of Hessian-vector products.","category":"page"},{"location":"examples/neural_ode_flux/#Neural-Ordinary-Differential-Equations-with-Flux.train!","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"","category":"section"},{"location":"examples/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"The following is the same neural ODE example as before, but now using Flux.jl directly with Flux.train!. Notice that the only difference is that we have to make the neural network be a Chain and use Flux.jl's Flux.params implicit parameter system.","category":"page"},{"location":"examples/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"using DiffEqFlux, DifferentialEquations, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restructure for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\r\nend\r\n\r\nfunction loss_n_ode()\r\n    pred = predict_n_ode()\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss\r\nend\r\n\r\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\r\n\r\ncb = function (;doplot=false) # callback function to observe training\r\n  pred = predict_n_ode()\r\n  display(sum(abs2,ode_data .- pred))\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb()\r\n\r\ndata = Iterators.repeated((), 1000)\r\nFlux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)","category":"page"},{"location":"examples/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux.train!","title":"Neural Ordinary Differential Equations with Flux.train!","text":"(Image: )","category":"page"},{"location":"layers/NeuralDELayers/#Neural-Differential-Equation-Layer-Functions","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"Constructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nNeuralODE(model,tspan,alg=nothing,args...;kwargs...)\nNeuralODE(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"Constructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nNeuralDSDE(model1,model2,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\nNeuralDSDE(model1::FastChain,model2::FastChain,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"Constructs a neural stochastic differential equation (neural SDE).\n\nNeuralSDE(model1,model2,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralSDE(model1::FastChain,model2::FastChain,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x,1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralCDDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralCDDE","text":"Constructs a neural delay differential equation (neural DDE) with constant delays.\n\nNeuralCDDE(model,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralCDDE(model::FastChain,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size [x;x(t-lag_1);...;x(t-lag_n)] and produce and output shaped like x.\ntspan: The timespan to be solved on.\nhist: Defines the history function h(t) for values before the start of the integration.\nlags: Defines the lagged values that should be utilized in the neural network.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"Constructs a neural differential-algebraic equation (neural DAE).\n\nNeuralDAE(model,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralDAE(model::FastChain,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"Constructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nNeuralODEMM(model,constraints_model,tspan,mass_matrix,alg=nothing,args...;kwargs...)\nNeuralODEMM(model::FastChain,tspan,mass_matrix,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nmass_matrix: The mass matrix associated with the DAE\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.AugmentedNDELayer","page":"Neural Differential Equation Layers","title":"DiffEqFlux.AugmentedNDELayer","text":"Constructs an Augmented Neural Differential Equation Layer.\n\nAugmentedNDELayer(nde, adim::Int)\n\nArguments:\n\nnde: Any Neural Differential Equation Layer\nadim: The number of dimensions the initial conditions should be lifted\n\nReferences:\n\n[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.\n\n\n\n\n\n","category":"type"},{"location":"examples/prediction_error_method/#Prediction-error-method-(PEM)","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"","category":"section"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"When identifying linear systems from noisy data, the prediction-error method [Ljung] is close to a gold standard when it comes to the quality of the models it produces, but is also one of the computationally more expensive methods due to its reliance on iterative, gradient-based estimation. When we are identifying nonlinear models, we typically do not have the luxury of closed-form, non-iterative solutions, while PEM is easier to adopt to the nonlinear setting.[Larsson]","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Fundamentally, PEM changes the problem from minimizing a loss based on the simulation performance, to minimizing a loss based on shorter-term predictions. There are several benefits of doing so, and this example will highlight two:","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The loss is often easier to optimize.\nIn addition to an accurate simulator, you also obtain a prediction for the system.\nWith PEM, it's possible to estimate disturbance models.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The last point will not be illustrated in this tutorial, but we will briefly expand upon it here. Gaussian, zero-mean measurement noise is usually not very hard to handle. Disturbances that affect the state of the system may, however, cause all sorts of havoc on the estimate. Consider wind affecting an aircraft, deriving a statistical and dynamical model of the wind may be doable, but unless you measure the exact wind affecting the aircraft, making use of the model during parameter estimation is impossible. The wind is an unmeasured load disturbance that affects the state of the system through its own dynamics model. Using the techniques illustrated in this tutorial, it's possible to estimate the influence of the wind during the experiment that generated the data and reduce or eliminate the bias it otherwise causes in the parameter estimates. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We will start by illustrating a common problem with simulation-error minimization. Imagine a pendulum with unknown length that is to be estimated. A small error in the pendulum length causes the frequency of oscillation to change. Over sufficiently large horizon, two sinusoidal signals with different frequencies become close to orthogonal to each other. If some form of squared-error loss is used, the loss landscape will be horribly non-convex in this case, indeed, we will illustrate exactly this below.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Another case that poses a problem for simulation-error estimation is when the system is unstable or chaotic. A small error in either the initial condition or the parameters may cause the simulation error to diverge and its gradient to become meaningless.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"In both of these examples, we may make use of measurements we have of the evolution of the system to prevent the simulation error from diverging. For instance, if we have measured the angle of the pendulum, we can make use of this measurement to adjust the angle during the simulation to make sure it stays close to the measured angle. Instead of performing a pure simulation, we instead say that we predict the state a while forward in time, given all the measurements up until the current time point. By minimizing this prediction rather than the pure simulation, we can often prevent the model error from diverging even though we have a poor initial guess. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We start by defining a model of the pendulum. The model takes a parameter L corresponding to the length of the pendulum. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"using DifferentialEquations, DiffEqFlux, Plots, Statistics, DataInterpolations\n\ntspan = (0.1f0, Float32(20.0))\ntsteps = range(tspan[1], tspan[2], length = 1000)\n\nu0 = [0f0, 3f0] # Initial angle and angular velocity\n\nfunction simulator(du,u,p,t) # Pendulum dynamics\n    g = 9.82f0 # Gravitational constant\n    L = p isa Number ? p : p[1] # Length of the pendulum\n    gL = g/L\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -gL * sin(θ)\nend","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We assume that the true length of the pendulum is L = 1, and generate some data from this system.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"prob = ODEProblem(simulator,u0,tspan,1.0) # Simulate with L = 1\nsol = solve(prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)\ny = sol[1,:] # This is the data we have available for parameter estimation\nplot(y, title=\"Pendulum simulation\", label=\"angle\")","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"(Image: img1)","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We also define functions that simulate the system and calculate the loss, given a parameter p corresponding to the length.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"function simulate(p)\n    _prob = remake(prob,p=p)\n    solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)[1,:]\nend\n\nfunction simloss(p)\n    yh = simulate(p)\n    e2 = yh\n    e2 .= abs2.(y .- yh)\n    return mean(e2)\nend","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We now look at the loss landscape as a function of the pendulum length:","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Ls = 0.01:0.01:2\nsimlosses = simloss.(Ls)\nfig_loss = plot(Ls, simlosses, title = \"Loss landscape\", xlabel=\"Pendulum length\", ylabel = \"MSE loss\", lab=\"Simulation loss\")","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"(Image: img2)","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"This figure is interesting, the loss is of course 0 for the true value L=1, but for values L  1, the overall slope actually points in the wrong direction! Moreover, the loss is oscillatory, indicating that this is a terrible function to optimize, and that we would need a very good initial guess for a local search to converge to the true value. Note, this example is chosen to be one-dimensional in order to allow these kinds of visualizations, and one-dimensional problems are typically not hard to solve, but the reasoning extends to higher-dimensional and harder problems.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We will now move on to defining a predictor model. Our predictor will be very simple, each time step, we will calculate the error e between the simulated angle theta and the measured angle y. A part of this error will be used to correct the state of the pendulum. The correction we use is linear and looks like Ke = K(y - theta). We have formed what is commonly referred to as a (linear) observer. The Kalman filter is a particular kind of linear observer, where K is calculated based on a statistical model of the disturbances that act on the system. We will stay with a simple, fixed-gain observer here for simplicity. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"To feed the sampled data into the continuous-time simulation, we make use of an interpolator. We also define new functions, predictor that contains the pendulum dynamics with the observer correction, a prediction function that performs the rollout (we're not using the word simulation to not confuse with the setting above) and a loss function.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"y_int = LinearInterpolation(y,tsteps)\n\nfunction predictor(du,u,p,t)\n    g = 9.82f0\n    L, K, y = p # pendulum length, observer gain and measurements\n    gL = g/L\n    θ  = u[1]\n    dθ = u[2]\n    yt = y(t)\n    e = yt - θ\n    du[1] = dθ + K*e\n    du[2] = -gL * sin(θ) \nend\n\npredprob = ODEProblem(predictor,u0,tspan,nothing)\n\nfunction prediction(p)\n    p_full = (p..., y_int)\n    _prob = remake(predprob,p=p_full)\n    solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)[1,:]\nend\n\nfunction predloss(p)\n    yh = prediction(p)\n    e2 = yh\n    e2 .= abs2.(y .- yh)\n    return mean(e2)\nend\n\npredlosses = map(Ls) do L\n    p = (L, 1) # use K = 1\n    predloss(p)\nend\n\nplot!(Ls, predlosses, lab=\"Prediction loss\")","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"(Image: img3)","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Once gain we look at the loss as a function of the parameter, and this time it looks a lot better. The loss is not convex, but the gradient points in the right direction over a much larger interval. Here, we arbitrarily set the observer gain to K=1, we will later let the optimizer learn this parameter.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"For completeness, we also perform estimation using both losses. We choose an initial guess we know will be hard for the simulation-error minimization just to drive home the point:","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"L0 = [0.7] # Initial guess of pendulum length\nressim = DiffEqFlux.sciml_train(simloss,L0,maxiters=5000)\nysim = simulate(ressim.u)\n\nplot(tsteps, [y ysim], label=[\"Data\" \"Simulation model\"])\n\np0 = [0.7, 1.0] # Initial guess of length and observer gain K\nrespred = DiffEqFlux.sciml_train(predloss,p0,maxiters=5000)\nypred = simulate(respred.u)\n\nplot!(tsteps, ypred, label=\"Prediction model\")","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"(Image: img4)","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The estimated parameters (L K) are","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"respred.u","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Now, we might ask ourselves why we used a correct on the form Ke and didn't instead set the angle in the simulation equal to the measurement. The reason is twofold","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"If our prediction of the angle is 100% based on the measurements, the model parameters do not matter for the prediction and we can thus not hope to learn their values.\nThe measurement is usually noisy, and we thus want to fuse the predictive power of the model with the information of the measurements. The Kalman filter is an optimal approach to this information fusion under special circumstances (linear model, Gaussian noise).","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We thus let the optimization learn the best value of the observer gain in order to make the best predictions. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"As a last step, we perform the estimation also with some measurement noise to verify that it does something reasonable:","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"yn = y .+ 0.1f0 .* randn.(Float32)\ny_int = LinearInterpolation(yn,tsteps) # redefine the interpolator to contain noisy measurements\n\nresprednoise = DiffEqFlux.sciml_train(predloss,p0,maxiters=5000)\nyprednoise = prediction(resprednoise.u)\nplot!(tsteps, yprednoise, label=\"Prediction model with noisy measurements\")","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"(Image: img5)","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"resprednoise.u","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"This example has illustrated basic use of the prediction-error method for parameter estimation. In our example, the measurement we had corresponded directly to one of the states, and coming up with an observer/predictor that worked was not too hard. For more difficult cases, we may opt to use a nonlinear observer, such as an extended Kalman filter (EKF) or design a Kalman filter based on a linearization of the system around some operating point.","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"As a last note, there are several other methods available to improve the loss landscape and avoid local minima, such as multiple-shooting. The prediction-error method can easily be combined with most of those methods. ","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"References:","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"[Ljung]: Ljung, Lennart. \"System identification–-Theory for the user\".","category":"page"},{"location":"examples/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"[Larsson]: Larsson, Roger, et al. \"Direct prediction-error identification of unstable nonlinear systems applied to flight test data.\"","category":"page"},{"location":"Collocation/#Smoothed-Collocation","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Smoothed collocation, also referred to as the two-stage method, allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true (u',u) pairs, at which point u'-f(u,p,t) can be directly estimated as a loss to determine the correct parameters p. This method can be extremely fast and robust to noise, though, because it does not accumulate through time, is not as exact as other methods.","category":"page"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"collocate_data","category":"page"},{"location":"Collocation/#DiffEqFlux.collocate_data","page":"Smoothed Collocation","title":"DiffEqFlux.collocate_data","text":"u′,u = collocate_data(data,tpoints,kernel=SigmoidKernel())\nu′,u = collocate_data(data,tpoints,tpoints_sample,interp,args...)\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nFor kernels, the following exist:\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/\n\nAdditionally, we can use interpolation methods from DataInterpolations.jl to generate data from intermediate timesteps. In this case, pass any of the methods like QuadraticInterpolation as interp, and the timestamps to sample from as tpoints_sample.\n\n\n\n\n\n","category":"function"},{"location":"Collocation/#Kernel-Choice","page":"Smoothed Collocation","title":"Kernel Choice","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Note that the kernel choices of DataInterpolations.jl, such as CubicSpline(), are exact, i.e. go through the data points, while the smoothed kernels are regression splines. Thus CubicSpline() is preferred if the data is not too noisy or is relatively sparse. If data is sparse and very noisy, a BSpline()  can be the best regression spline, otherwise one of the other kernels such as as EpanechnikovKernel.","category":"page"},{"location":"Collocation/#Non-Allocating-Forward-Mode-L2-Collocation-Loss","page":"Smoothed Collocation","title":"Non-Allocating Forward-Mode L2 Collocation Loss","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation:","category":"page"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"using PreallocationTools\ndu = PreallocationTools.dualcache(similar(prob.u0))\npreview_est_sol = [@view estimated_solution[:,i] for i in 1:size(estimated_solution,2)]\npreview_est_deriv = [@view estimated_derivative[:,i] for i in 1:size(estimated_solution,2)]\n\nfunction construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)\n  function (p)\n      _du = PreallocationTools.get_tmp(du,p)\n      vecdu = vec(_du)\n      cost = zero(first(p))\n      for i in 1:length(preview_est_sol)\n        est_sol = preview_est_sol[i]\n        f(_du,est_sol,p,tpoints[i])\n        vecdu .= vec(preview_est_deriv[i]) .- vec(_du)\n        cost += sum(abs2,vecdu)\n      end\n      sqrt(cost)\n  end\nend\ncost_function = construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)","category":"page"},{"location":"Flux/#Use-with-Flux.jl","page":"Use with Flux Chain and train!","title":"Use with Flux.jl","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"All of the tools of DiffEqFlux.jl can be used with Flux.jl. A lot of the examples have been written to use FastChain and sciml_train, but in all cases this can be changed to the Chain and Flux.train! workflow.","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-Flux.train!","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with Flux.train!","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"This should work almost automatically by using solve. Here is an example of optimizing u0 and p.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\r\nend\r\n\r\nfunction loss_n_ode()\r\n    pred = predict_n_ode()\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss\r\nend\r\n\r\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\r\n\r\ncb = function (;doplot=false) #callback function to observe training\r\n  pred = predict_n_ode()\r\n  display(sum(abs2,ode_data .- pred))\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb()\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = Flux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-sciml_train","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with sciml_train","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"While for simple neural networks we recommend using FastChain-based neural networks for speed and simplicity, Flux neural networks can be used with sciml_train by utilizing the Flux.destructure function. In this case, if dudt is a Flux chain, then:","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"p,re = Flux.destructure(chain)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"returns p which is the vector of parameters for the chain and re which is a function re(p) that reconstructs the neural network with new parameters p. Using this function we can thus build our neural differential equations in an explicit parameter style. For example, the neural ordinary differential equation example written out without using the NeuralODE helper would look like. Notice that in this example we will optimize both the neural network parameters p and the input initial condition u0. Notice that sciml_train works on a vector input, so we have to concatenate u0 and p and then in the loss function split to the pieces.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nθ = [u0;p] # the parameter vector to optimize\r\n\r\nfunction predict_n_ode(θ)\r\n  Array(solve(prob,Tsit5(),u0=θ[1:2],p=θ[3:end],saveat=t))\r\nend\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss,pred\r\nend\r\n\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ,l,pred;doplot=false) #callback function to observe training\r\n  display(l)\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb(θ,loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05), cb = cb, maxiters=100)\r\ncb(res1.minimizer,loss_n_ode(res1.minimizer)...;doplot=true)\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(), cb = cb)\r\ncb(res2.minimizer,loss_n_ode(res2.minimizer)...;doplot=true)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Notice that the advantage of this format is that we can use Optim's optimizers, like LBFGS with a full Chain object for all of Flux's neural networks, like convolutional neural networks.","category":"page"},{"location":"Flux/#Using-ComponentArrays-for-neural-network-layers","page":"Use with Flux Chain and train!","title":"Using ComponentArrays for neural network layers","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"We can also create the dense layers from scratch using ComponentArrays.jl. Flux is used here just for the glorot_uniform function and the ADAM optimizer.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using ComponentArrays, DiffEqFlux, Optim, OrdinaryDiffEq, Plots, UnPack\r\nusing Flux: glorot_uniform, ADAM","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Again, let's create the truth data.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"u0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0, 1.5f0)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nt = range(tspan[1], tspan[2], length = datasize)\r\nprob = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob, Tsit5(), saveat = t))","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Now, we'll make a function that creates dense neural layer components. It is similar to Flux.Dense, except it doesn't handle the activation function. We'll do that separately.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"dense_layer(in, out) = ComponentArray{Float32}(weight=glorot_uniform(out, in), bias=zeros(out))","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Our parameter vector will be a ComponentArray that holds the ODE initial conditions and the dense neural layers. This enables it to pass through the solver as a flat array while giving us the convenience of struct-like access to the components.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"layers = (L1=dense_layer(2, 50), L2=dense_layer(50, 2))\r\nθ = ComponentArray(u=u0, p=layers)\r\n\r\nfunction dudt(u, p, t)\r\n    @unpack L1, L2 = p\r\n    return L2.weight * tanh.(L1.weight * u.^3 .+ L1.bias) .+ L2.bias\r\nend\r\n\r\nprob = ODEProblem(dudt, u0, tspan)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"As before, we'll define prediction and loss functions as well as a callback function to observe training.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"predict_n_ode(θ) = Array(solve(prob, Tsit5(), u0=θ.u, p=θ.p, saveat=t))\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2, ode_data .- pred)\r\n    return loss, pred\r\nend\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ, loss, pred; doplot=false)\r\n    display(loss)\r\n    # plot current prediction against data\r\n    pl = scatter(t, ode_data[1,:], label = \"data\")\r\n    scatter!(pl, t, pred[1,:], label = \"prediction\")\r\n    display(plot(pl))\r\n    return false\r\nend\r\n\r\ncb(θ, loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\n\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05); cb=cb, maxiters=100)\r\ncb(res1.minimizer, loss_n_ode(res1.minimizer)...; doplot=true)\r\n\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(); cb=cb)\r\ncb(res2.minimizer, loss_n_ode(res2.minimizer)...; doplot=true)","category":"page"},{"location":"examples/normalizing_flows/#Continuous-Normalizing-Flows-with-GalacticOptim.jl","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"examples/normalizing_flows/#Copy-Pasteable-Code","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, GalacticOptim, Distributions\n\nnn = Chain(\n    Dense(1, 3, tanh),\n    Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\nffjord_mdl = FFJORD(nn, tspan, Tsit5())\n\n# Training\ndata_dist = Normal(6.0f0, 0.7f0)\ntrain_data = rand(data_dist, 1, 100)\n\nfunction loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend\n\nadtype = GalacticOptim.AutoZygote()\nres1 = DiffEqFlux.sciml_train(loss, ffjord_mdl.p, ADAM(0.1), adtype; maxiters=100)\nres2 = DiffEqFlux.sciml_train(loss, res1.u, LBFGS(), adtype; allow_f_increases=false)\n\n# Evaluation\nusing Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)\n\n# Data Generation\nffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#Step-by-Step-Explanation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, GalacticOptim, Distributions\n\nnn = Chain(\n    Dense(1, 3, tanh),\n    Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\nffjord_mdl = FFJORD(nn, tspan, Tsit5())","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"examples/normalizing_flows/#Training","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Training","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"First, let's get an array from a normal distribution as the training data","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"data_dist = Normal(6.0f0, 0.7f0)\ntrain_data = rand(data_dist, 1, 100)","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Now we define a loss function that we wish to minimize","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"function loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"adtype = GalacticOptim.AutoZygote()\nres1 = DiffEqFlux.sciml_train(loss, ffjord_mdl.p, ADAM(0.1), adtype; maxiters=100)\n\n# output\n* Status: success\n\n* Candidate solution\n   u: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n   Minimum:   1.240627e+00\n\n* Found with\n   Algorithm:     ADAM\n   Initial Point: [9.33e-01, 1.13e+00, 2.92e-01,  ...]\n","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We then complete the training using a different optimizer starting from where ADAM stopped.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"res2 = DiffEqFlux.sciml_train(loss, res1.u, LBFGS(), adtype; allow_f_increases=false)\n\n# output\n* Status: success\n\n* Candidate solution\n   u: [-1.06e+00, 2.24e+00, 8.77e-01,  ...]\n   Minimum:   1.157672e+00\n\n* Found with\n   Algorithm:     L-BFGS\n   Initial Point: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 0.00e+00 ≰ 0.0e+00\n   |x - x'|/|x'|          = 0.00e+00 ≰ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 4.09e-03 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   514  (vs limit Inf)\n   Iterations:    44\n   f(x) calls:    244\n   ∇f(x) calls:   244","category":"page"},{"location":"examples/normalizing_flows/#Evaluation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Evaluation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"For evaluating the result, we can use totalvariation function from Distances.jl. First, we compute densities using actual distribution and FFJORD model. then we use a distance function.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)","category":"page"},{"location":"examples/normalizing_flows/#Data-Generation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Data Generation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"What's more, we can generate new data by using FFJORD as a distribution in rand.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"ffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#References","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"References","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"[1] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).","category":"page"},{"location":"examples/hamiltonian_nn/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Networks introduced in [1] allow models to \"learn and respect exact conservation laws in an unsupervised manner\". In this example, we will train a model to learn the Hamiltonian for a 1D Spring mass system. This system is described by the equation:","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"mddot(x) + kx = 0","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Now we make some simplifying assumptions, and assign m = 1 and k = 1. Analytically solving this equation, we get x = sin(t). Hence, q = sin(t), and p = cos(t). Using these solutions we generate our dataset and fit the NeuralHamiltonianDE to learn the dynamics of this system.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using DiffEqFlux, DifferentialEquations, Statistics, Plots\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader(data, target; batchsize=256, shuffle=true)\n\nhnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()\n\nmodel = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/#Step-by-Step-Explanation","page":"Hamiltonian Neural Network","title":"Step by Step Explanation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/#Data-Generation","page":"Hamiltonian Neural Network","title":"Data Generation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"The HNN predicts the gradients (dot(q) dot(p)) given (q p). Hence, we generate the pairs (q p) using the equations given at the top. Additionally to supervise the training we also generate the gradients. Next we use use Flux DataLoader for automatically batching our dataset.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"t = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader(data, target; batchsize=256, shuffle=true)","category":"page"},{"location":"examples/hamiltonian_nn/#Training-the-HamiltonianNN","page":"Hamiltonian Neural Network","title":"Training the HamiltonianNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We parameterize the HamiltonianNN with a small MultiLayered Perceptron (HNN also works with the Fast* Layers provided in DiffEqFlux). HNNs are trained by optimizing the gradients of the Neural Network. Zygote currently doesn't support nesting itself, so we will be using ReverseDiff in the training loop to compute the gradients of the HNN Layer for Optimization.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"hnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()","category":"page"},{"location":"examples/hamiltonian_nn/#Solving-the-ODE-using-trained-HNN","page":"Hamiltonian Neural Network","title":"Solving the ODE using trained HNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In order to visualize the learned trajectories, we need to solve the ODE. We will use the NeuralHamiltonianDE layer which is essentially a wrapper over HamiltonianNN layer and solves the ODE.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"model = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"(Image: HNN Prediction)","category":"page"},{"location":"examples/hamiltonian_nn/#Expected-Output","page":"Hamiltonian Neural Network","title":"Expected Output","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Loss Neural Hamiltonian DE = 18.768814\nLoss Neural Hamiltonian DE = 0.022630047\nLoss Neural Hamiltonian DE = 0.015060622\nLoss Neural Hamiltonian DE = 0.013170851\nLoss Neural Hamiltonian DE = 0.011898238\nLoss Neural Hamiltonian DE = 0.009806873","category":"page"},{"location":"examples/hamiltonian_nn/#References","page":"Hamiltonian Neural Network","title":"References","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.","category":"page"},{"location":"layers/HamiltonianNN/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system.","category":"page"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"HamiltonianNN\nNeuralHamiltonianDE","category":"page"},{"location":"layers/HamiltonianNN/#DiffEqFlux.HamiltonianNN","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.HamiltonianNN","text":"Constructs a Hamiltonian Neural Network [1]. This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories. It takes as input a concatenated vector of length 2n containing the position (of size n) and momentum (of size n) of the particles. It then returns the time derivatives for position and momentum.\n\nnote: Note\nThis doesn't solve the Hamiltonian Problem. Use NeuralHamiltonianDE for such applications.\n\nnote: Note\nThis layer currently doesn't support GPU. The support will be added in future with some AD fixes.\n\nTo obtain the gradients to train this network, ReverseDiff.gradient is supposed to be used. This prevents the usage of DiffEqFlux.sciml_train or Flux.train. Follow this tutorial to see how to define a training loop to circumvent this issue.\n\nHamiltonianNN(model; p = nothing)\nHamiltonianNN(model::FastChain; p = initial_params(model))\n\nArguments:\n\nmodel: A Chain or FastChain neural network that returns the Hamiltonian of the          system.\np: The initial parameters of the neural network.\n\nReferences:\n\n[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.\n\n\n\n\n\n","category":"type"},{"location":"layers/HamiltonianNN/#DiffEqFlux.NeuralHamiltonianDE","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.NeuralHamiltonianDE","text":"Contructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network HamiltonianNN.\n\nNeuralHamiltonianDE(model, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain, FastChain or Hamiltonian Neural Network that predicts the          Hamiltonian of the system.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the           Common Solver Arguments           documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"examples/physical_constraints/#Enforcing-Physical-Constraints-via-Universal-Differential-Algebraic-Equations","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"As shown in the stiff ODE tutorial, differential-algebraic equations (DAEs) can be used to impose physical constraints. One way to define a DAE is through an ODE with a singular mass matrix. For example, if we make Mu' = f(u) where the last row of M is all zeros, then we have a constraint defined by the right hand side. Using NeuralODEMM, we can use this to define a neural ODE where the sum of all 3 terms must add to one. An example of this is as follows:","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nfunction f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend\n\nu₀ = [1.0, 0, 0]\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\np = [0.04, 3e7, 1e4]\n\nstiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)\n\nnn_dudt2 = FastChain(FastDense(3, 64, tanh),\n                     FastDense(64, 2))\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(autodiff=false), saveat = 0.1)\nmodel_stiff_ndae(u₀)\n\nfunction predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p)\nend\n\nfunction loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, Array(sol_stiff) .- pred)\n    return loss, pred\nend\n\ncallback = function (p, l, pred) #callback function to observe training\n  display(l)\n  return false\nend\n\nl1 = first(loss_stiff_ndae(model_stiff_ndae.p))\nresult_stiff = DiffEqFlux.sciml_train(loss_stiff_ndae, model_stiff_ndae.p,\n                                      BFGS(initial_stepnorm = 0.001),\n                                      cb = callback, maxiters = 100)","category":"page"},{"location":"examples/physical_constraints/#Step-by-Step-Description","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/physical_constraints/#Load-Packages","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Load Packages","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using DiffEqFlux, DifferentialEquations, Plots","category":"page"},{"location":"examples/physical_constraints/#Differential-Equation","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Differential Equation","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"First, we define our differential equations as a highly stiff problem which makes the fitting difficult.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend","category":"page"},{"location":"examples/physical_constraints/#Parameters","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Parameters","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = [1.0, 0, 0]\n\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\n\np = [0.04, 3e7, 1e4]","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = Initial Conditions\nM = Semi-explicit Mass Matrix (last row is the constraint equation and are therefore","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"all zeros)","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"tspan = Time span over which to evaluate\np = parameters k1, k2 and k3 of the differential equation above","category":"page"},{"location":"examples/physical_constraints/#ODE-Function,-Problem-and-Solution","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"ODE Function, Problem and Solution","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We define and solve our ODE problem to generate the \"labeled\" data which will be used to train our Neural Network.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"stiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a DAE we need to make sure to use a compatible solver. Rodas5 works well for this example.","category":"page"},{"location":"examples/physical_constraints/#Neural-Network-Layers","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Neural Network Layers","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Next, we create our layers using FastChain. We use this instead of Chain because it reduces the overhead making it faster for smaller NNs of <200 layers (similarly for FastDense). The input to our network will be the initial conditions fed in as u₀.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"nn_dudt2 = FastChain(FastDense(3, 64, tanh),\n                     FastDense(64, 2))\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(autodiff = false), saveat = 0.1)\nmodel_stiff_ndae(u₀)","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a stiff problem, we have manually imposed that sum constraint via (u,p,t) -> [u[1] + u[2] + u[3] - 1], making the fitting easier.","category":"page"},{"location":"examples/physical_constraints/#Prediction-Function","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Prediction Function","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"For simplicity, we define a wrapper function that only takes in the model's parameters to make predictions.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p)\nend","category":"page"},{"location":"examples/physical_constraints/#Train-Parameters","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train Parameters","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Training our network requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"examples/physical_constraints/#Loss","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Loss","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We first make our predictions based on the current parameters, then calculate the loss from these predictions. In this case, we use least squares as our loss.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, sol_stiff .- pred)\n    return loss, pred\nend\n\nl1 = first(loss_stiff_ndae(model_stiff_ndae.p))","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Notice that we are feeding the parameters of model_stiff_ndae to the loss_stiff_ndae function. model_stiff_node.p are the weights of our NN and is of size 386 (4 * 64 + 65 * 2) including the biases.","category":"page"},{"location":"examples/physical_constraints/#Optimizer","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The optimizer BFGS is directly passed in the training step (see below).","category":"page"},{"location":"examples/physical_constraints/#Callback","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Callback","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The callback function displays the loss during training.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"callback = function (p, l, pred) #callback function to observe training\n  display(l)\n  return false\nend","category":"page"},{"location":"examples/physical_constraints/#Train","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train","text":"","category":"section"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Finally, training with sciml_train by passing: loss function, model parameters, optimizer, callback and maximum iteration.","category":"page"},{"location":"examples/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"result_stiff = DiffEqFlux.sciml_train(loss_stiff_ndae, model_stiff_ndae.p,\n                                      BFGS(initial_stepnorm = 0.001),\n                                      cb = callback, maxiters = 100)","category":"page"},{"location":"examples/physical_constraints/#Expected-Output","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/jump/#Neural-Jump-Diffusions-(Neural-Jump-SDE)-and-Neural-Partial-Differential-Equations-(Neural-PDEs)","page":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","title":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","text":"","category":"section"},{"location":"examples/jump/","page":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","title":"Neural Jump Diffusions (Neural Jump SDE) and Neural Partial Differential Equations (Neural PDEs)","text":"For the sake of not having a never-ending documentation of every single combination of CPU/GPU with every layer and every neural differential equation, we will end here. But you may want to consult this blog post which showcases defining neural jump diffusions and neural partial differential equations.","category":"page"},{"location":"sciml_train/#sciml_train","page":"sciml_train and GalacticOptim.jl","title":"sciml_train and GalacticOptim.jl","text":"","category":"section"},{"location":"sciml_train/","page":"sciml_train and GalacticOptim.jl","title":"sciml_train and GalacticOptim.jl","text":"sciml_train is a heuristic-based training function built using GalacticOptim.jl. It incorporates the knowledge of many high level benchmarks to attempt and do the right thing.","category":"page"},{"location":"sciml_train/","page":"sciml_train and GalacticOptim.jl","title":"sciml_train and GalacticOptim.jl","text":"GalacticOptim.jl is a package with a scope that is beyond your normal global optimization package. GalacticOptim.jl seeks to bring together all of the optimization packages it can find, local and global, into one unified Julia interface. This means, you learn one package and you learn them all! GalacticOptim.jl adds a few high-level features, such as integrating with automatic differentiation, to make its usage fairly simple for most cases, while allowing all of the options in a single unified interface.","category":"page"},{"location":"sciml_train/#sciml_train-API","page":"sciml_train and GalacticOptim.jl","title":"sciml_train API","text":"","category":"section"},{"location":"sciml_train/","page":"sciml_train and GalacticOptim.jl","title":"sciml_train and GalacticOptim.jl","text":"DiffEqFlux.sciml_train","category":"page"},{"location":"sciml_train/#DiffEqFlux.sciml_train","page":"sciml_train and GalacticOptim.jl","title":"DiffEqFlux.sciml_train","text":"sciml_train\n\nUnconstrained Optimization\n\nfunction sciml_train(loss, _θ, opt = DEFAULT_OPT, adtype = DEFAULT_AD,\n                     _data = DEFAULT_DATA, args...;\n                     callback = (args...) -> false, maxiters = get_maxiters(data),\n                     kwargs...)\n\nBox Constrained Optimization\n\nfunction sciml_train(loss, θ, opt = DEFAULT_OPT, adtype = DEFAULT_AD,\n                     data = DEFAULT_DATA, args...;\n                     lower_bounds, upper_bounds,\n                     callback = (args...) -> (false), maxiters = get_maxiters(data),\n                     kwargs...)\n\nOptimizer Choices and Arguments\n\nFor a full definition of the allowed optimizers and arguments, please see the GalacticOptim.jl documentation. As sciml_train is an interface over GalacticOptim.jl, all of its optimizers and arguments can be used from here.\n\nLoss Functions and Callbacks\n\nLoss functions in sciml_train treat the first returned value as the return. For example, if one returns (1.0, [2.0]), then the value the optimizer will see is 1.0. The other values are passed to the callback function. The callback function is callback(p, args...) where the arguments are the extra returns from the loss. This allows for reusing instead of recalculating. The callback function must return a boolean where if true, then the optimizer will prematurely end the optimization. It is called after every successful step, something that is defined in an optimizer-dependent manner.\n\nDefault AD Choice\n\nThe current default AD choice is dependent on the number of parameters. For <50 parameters both ForwardDiff.jl and Zygote.jl gradients are evaluated and the fastest is used. If both methods fail, finite difference method is used as a fallback. For ≥50 parameters Zygote.jl is used. More refinements to the techniques are planned.\n\nDefault Optimizer Choice\n\nBy default, if the loss function is deterministic than an optimizer chain of ADAM -> BFGS is used, otherwise ADAM is used (and a choice of maxiters is required).\n\n\n\n\n\n","category":"function"},{"location":"examples/mnist_conv_neural_ode/#Convolutional-Neural-ODE-MNIST-Classifier-on-GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Training a Convolutional Neural Net Classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend\n\n# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split);\n\ndown = Chain(Conv((3, 3), 1=>64, relu, stride = 1), GroupNorm(64, 64),\n             Conv((4, 4), 64=>64, relu, stride = 2, pad=1), GroupNorm(64, 64),\n             Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Chain(Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Chain(GroupNorm(64, 64), x -> relu.(x), MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend\n\n# Build our over-all model topology\nmodel = Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncb() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = cb)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Step-by-Step-Description","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/#Load-Packages","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs","category":"page"},{"location":"examples/mnist_conv_neural_ode/#GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Layers","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down = Chain(Conv((3, 3), 1=>64, relu, stride = 1), GroupNorm(64, 64),\n             Conv((4, 4), 64=>64, relu, stride = 2, pad=1), GroupNorm(64, 64),\n             Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Chain(Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Chain(GroupNorm(64, 64), x -> relu.(x), MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down: This layer downsamples our images into 6 x 6 x 64 dimensional features.         It takes a 28 x 28 image, and passes it through a convolutional neural network         layer with relu activation","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn: A 2 layer Convolutional Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"fc: The final fully connected layer which maps our learned features to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"gpu: A utility function which transfers our model to GPU, if one is available","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Array-Conversion","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"When using NeuralODE, we can use the following function as a cheap conversion of DiffEqArray from the ODE solver into a Matrix that can be used in the following layer:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) with Array(x).","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Build-Topology","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Build our over-all model topology\nmodel = Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Chain(down, nn, fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Prediction","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Accuracy","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Training-Parameters","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Loss","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Optimizer","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"opt = ADAM(0.05)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#CallBack","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"cb() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Train","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Train","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = cb)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Expected-Output","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Iter:   1 || Train Accuracy: 8.453 || Test Accuracy: 8.883\nIter:  11 || Train Accuracy: 14.773 || Test Accuracy: 14.967\nIter:  21 || Train Accuracy: 24.383 || Test Accuracy: 24.433\nIter:  31 || Train Accuracy: 38.820 || Test Accuracy: 38.000\nIter:  41 || Train Accuracy: 30.852 || Test Accuracy: 31.350\nIter:  51 || Train Accuracy: 29.852 || Test Accuracy: 29.433\nIter:  61 || Train Accuracy: 45.195 || Test Accuracy: 45.217\nIter:  71 || Train Accuracy: 70.336 || Test Accuracy: 68.850\nIter:  81 || Train Accuracy: 76.250 || Test Accuracy: 75.783\nIter:  91 || Train Accuracy: 80.867 || Test Accuracy: 81.017\nIter: 101 || Train Accuracy: 86.398 || Test Accuracy: 85.317\nIter: 111 || Train Accuracy: 90.852 || Test Accuracy: 90.650\nIter: 121 || Train Accuracy: 93.477 || Test Accuracy: 92.550\nIter: 131 || Train Accuracy: 93.320 || Test Accuracy: 92.483\nIter: 141 || Train Accuracy: 94.273 || Test Accuracy: 93.567\nIter: 151 || Train Accuracy: 94.531 || Test Accuracy: 93.583\nIter: 161 || Train Accuracy: 94.992 || Test Accuracy: 94.067\nIter: 171 || Train Accuracy: 95.398 || Test Accuracy: 94.883\nIter: 181 || Train Accuracy: 96.945 || Test Accuracy: 95.633\nIter: 191 || Train Accuracy: 96.430 || Test Accuracy: 95.750\nIter: 201 || Train Accuracy: 96.859 || Test Accuracy: 95.983\nIter: 211 || Train Accuracy: 97.359 || Test Accuracy: 96.500\nIter: 221 || Train Accuracy: 96.586 || Test Accuracy: 96.133\nIter: 231 || Train Accuracy: 96.992 || Test Accuracy: 95.833\nIter: 241 || Train Accuracy: 97.148 || Test Accuracy: 95.950\nIter: 251 || Train Accuracy: 96.422 || Test Accuracy: 95.950\nIter: 261 || Train Accuracy: 96.094 || Test Accuracy: 95.633\nIter: 271 || Train Accuracy: 96.719 || Test Accuracy: 95.767\nIter: 281 || Train Accuracy: 96.719 || Test Accuracy: 96.000\nIter: 291 || Train Accuracy: 96.609 || Test Accuracy: 95.817\nIter: 301 || Train Accuracy: 96.656 || Test Accuracy: 96.033\nIter: 311 || Train Accuracy: 97.594 || Test Accuracy: 96.500\nIter: 321 || Train Accuracy: 97.633 || Test Accuracy: 97.083\nIter: 331 || Train Accuracy: 98.008 || Test Accuracy: 97.067\nIter: 341 || Train Accuracy: 98.070 || Test Accuracy: 97.150\nIter: 351 || Train Accuracy: 97.875 || Test Accuracy: 97.050\nIter: 361 || Train Accuracy: 96.922 || Test Accuracy: 96.500\nIter: 371 || Train Accuracy: 97.188 || Test Accuracy: 96.650\nIter: 381 || Train Accuracy: 97.820 || Test Accuracy: 96.783\nIter: 391 || Train Accuracy: 98.156 || Test Accuracy: 97.567\nIter: 401 || Train Accuracy: 98.250 || Test Accuracy: 97.367\nIter: 411 || Train Accuracy: 97.969 || Test Accuracy: 97.267\nIter: 421 || Train Accuracy: 96.555 || Test Accuracy: 95.667","category":"page"},{"location":"ControllingAdjoints/#adjoints","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"DiffEqFlux is capable of training neural networks embedded inside of differential equations with many different techniques. For all of the details, see the DifferentialEquations.jl local sensitivity analysis documentation. Here we will summarize these methodologies in the context of neural differential equations and scientific machine learning.","category":"page"},{"location":"ControllingAdjoints/#Choosing-a-sensealg-in-a-Nutshell","page":"Controlling Choices of Adjoints","title":"Choosing a sensealg in a Nutshell","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"By default, a stable adjoint with an auto-adapting vjp choice is used. In many cases, a user can optimize the choice to compute more than an order of magnitude faster than the default. However, given the vast space to explore, use the following decision tree to help guide the choice:","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"If you have 100 parameters or less, consider using forward-mode sensititivites. If the f function is not ForwardDiff-compatible, use ForwardSensitivty, otherwise use ForwardDiffSensitivty as its more efficient.\nFor larger equations, give BacksolveAdjoint and InterpolatingAdjoint a try. If the gradient of BacksolveAdjoint is correct, many times it's the faster choice so choose that (but it's not always faster!). If your equation is stiff or a DAE, skip this step as BacksolveAdjoint is almost certainly unstable.\nIf your equation does not use much memory and you're using a stiff solver, consider using QuadratureAdjoint as it is asymtopically more computationally efficient by trading off memory cost.\nIf the other methods are all unstable (check the gradients against each other!), then ReverseDiffAdjoint is a good fallback on CPU, while TrackerAdjoint is a good fallback on GPUs.\nAfter choosing a general sensealg, if the choice is InterpolatingAdjoint, QuadratureAdjoint, or BacksolveAdjoint, then optimize the choice of vjp calculation next:\nIf your function has no branching (no if statements), use ReverseDiffVJP(true).\nIf you're on the CPU and your function is very scalarized in operations but has branches, choose ReverseDiffVJP().\nIf your on the CPU or GPU and your function is very vectorized, choose ZygoteVJP().\nElse fallback to TrackerVJP() if Zygote does not support the function.","category":"page"},{"location":"ControllingAdjoints/#Additional-Details","page":"Controlling Choices of Adjoints","title":"Additional Details","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"A sensitivity analysis method can be passed to a solver via the sensealg keyword argument. For example:","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"solve(prob,Tsit5(),sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"sets the adjoint sensitivity analysis so that, when this call is encountered in the gradient calculation of any of the Julia reverse-mode AD frameworks, the differentiation will be replaced with the BacksolveAdjoint method where internal vector-Jacobian products are performed using Zygote.jl. From the DifferentialEquations.jl local sensitivity analysis page, we note that the following choices for sensealg exist:","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"BacksolveAdjoint\nInterpolatingAdjoint (with checkpoints)\nQuadratureAdjoint\nTrackerAdjoint\nReverseDiffAdjoint (currently requires using DistributionsAD)\nZygoteAdjoint (currently limited to special solvers)","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"Additionally, there are methodologies for forward sensitivity analysis:","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"ForwardSensitivty\nForwardDiffSensitivty","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"These methods have very low overhead compared to adjoint methods but have poor scaling with respect to increased numbers of parameters. Our benchmarks demonstrate a cutoff of around 100 parameters, where for models with less than 100 parameters these techniques are more efficient, but when there are more than 100 parameters (like in neural ODEs) these methods are less efficient than the adjoint methods.","category":"page"},{"location":"ControllingAdjoints/#Choices-of-Vector-Jacobian-Products-(autojacvec)","page":"Controlling Choices of Adjoints","title":"Choices of Vector-Jacobian Products (autojacvec)","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"With each of these solvers, autojacvec can be utilized to choose how the internal vector-Jacobian products of the f function are computed. The choices are:","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"ReverseDiffVJP(compile::Bool): Usually the fastest when scalarized operations exist in the f function (like in scientific machine learning applications like Universal Differential Equations) and the boolean compile (i.e. ReverseDiffVJP(true)) is the absolute fastest but requires that the f function of the ODE/DAE/SDE/DDE has no branching. Does not support GPUs. \nTrackerVJP: Not as efficient as ReverseDiffVJP, but supports GPUs. \nZygoteVJP: Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized functions (like neural networks and other layers from Flux.jl). Bear in mind that Zygote does not allow mutation, making the solve more memory expensive and therefore slow.\nnothing: Default choice given characteristics of the types in your model.\ntrue: Forward-mode AD Jacobian-vector products. Should only be used on sufficiently small equations\nfalse: Numerical Jacobian-vector products. Should only be used if the f function is not differentiable (i.e. is a Fortran code).","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"As other vector-Jacobian product systems become available in Julia they will be added to this system so that no user code changes are required to interface with these methodologies. ","category":"page"},{"location":"ControllingAdjoints/#Manual-VJPs","page":"Controlling Choices of Adjoints","title":"Manual VJPs","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"Note that when defining your differential equation the vjp can be manually overwritten by providing a vjp(u,p,t) that returns a tuple f(u,p,t),v->J*v in the form of ChainRules.jl. When this is done, the choice of ZygoteVJP will utilize your VJP function during the internal steps of the adjoint. This is useful for models where automatic differentiation may have trouble producing optimal code. This can be paired with ModelingToolkit.jl for producing hyper-optimized, sparse, and parallel VJP functions utilizing the automated symbolic conversions.","category":"page"},{"location":"ControllingAdjoints/#Optimize-then-Discretize","page":"Controlling Choices of Adjoints","title":"Optimize-then-Discretize","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"The original neural ODE paper popularized optimize-then-discretize with O(1) adjoints via backsolve. This is the methodology BacksolveAdjoint When training non-stiff neural ODEs, BacksolveAdjoint with ZygoteVJP is generally the fastest method. Additionally, this method does not require storing the values of any intermediate points and is thus the most memory efficient. However, BacksolveAdjoint is prone to instabilities whenever the Lipschitz constant is sufficiently large, like in stiff equations, PDE discretizations, and many other contexts, so it is not used by default. When training a neural ODE for machine learning applications, the user should try BacksolveAdjoint and see if it is sufficiently accurate on their problem.","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"Note that DiffEqFlux's implementation of BacksolveAdjoint includes an extra feature BacksolveAdjoint(checkpointing=true) which mixes checkpointing with BacksolveAdjoint. What this method does is that, at saveat points, values from the forward pass are saved. Since the reverse solve should numerically be the same as the forward pass, issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the saveat value from the forward pass. This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass. This can stabilize the adjoint in some applications, but for highly stiff applications the divergence can be too fast for this to work in practice.","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"To avoid the issues of backwards solving the ODE, InterpolatingAdjoint and QuadratureAdjoint utilize information from the forward pass. By default these methods utilize the continuous solution provided by DifferentialEquations.jl in the calculations of the adjoint pass. QuadratureAdjoint uses this to build a continuous function for the solution of adjoint equation and then performs an adaptive quadrature via Quadrature.jl, while InterpolatingAdjoint appends the integrand to the ODE so it's computed simultaneously to the Lagrange multiplier. When memory is not an issue, we find that the QuadratureAdjoint approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast, but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems. The InterpolatingAdjoint is thus a compromise between memory efficiency and compute efficiency, and is in the same spirit as CVODES.","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"However, if the memory cost of the InterpolatingAdjoint is too high, checkpointing can be used via InterpolatingAdjoint(checkpointing=true). When this is used, the checkpoints default to sol.t of the forward pass (i.e. the saved timepoints usually set by saveat). Then in the adjoint, intervals of sol.t[i-1] to sol.t[i] are re-solved in order to obtain a short interpolation which can be utilized in the adjoints. This at most results in two full solves of the forward pass, but dramatically reduces the computational cost while being a low-memory format. This is the preferred method for highly stiff equations when memory is an issue, i.e. stiff PDEs or large neural DAEs.","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"For forward-mode, the ForwardSensitivty is the version that performs the optimize-then-discretize approach. In this case, autojacvec corresponds to the method for computing J*v within the forward sensitivity equations, which is either true or false for whether to use Jacobian-free forward-mode AD (via ForwardDiff.jl) or Jacobian-free numerical differentiation.","category":"page"},{"location":"ControllingAdjoints/#Discretize-then-Optimize","page":"Controlling Choices of Adjoints","title":"Discretize-then-Optimize","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"In this approach the discretization is done first and then optimization is done on the discretized system. While traditionally this can be done discrete sensitivity analysis, this is can be equivalently done by automatic differentiation on the solver itself. ReverseDiffAdjoint performs reverse-mode automatic differentiation on the solver via ReverseDiff.jl, ZygoteAdjoint performs reverse-mode automatic differentiation on the solver via Zygote.jl, and TrackerAdjoint performs reverse-mode automatic differentiation on the solver via Tracker.jl. In addition, ForwardDiffSensitivty performs forward-mode automatic differentiation on the solver via ForwardDiff.jl.","category":"page"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"We note that many studies have suggested that this approach produces more accurate gradients than the optimize-than-discretize approach","category":"page"},{"location":"ControllingAdjoints/#Special-Notes-on-Equation-Types","page":"Controlling Choices of Adjoints","title":"Special Notes on Equation Types","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"While all of the choices are compatible with ordinary differential equations, specific notices apply to other forms:","category":"page"},{"location":"ControllingAdjoints/#Differential-Algebraic-Equations","page":"Controlling Choices of Adjoints","title":"Differential-Algebraic Equations","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"We note that while all 3 are compatible with index-1 DAEs via the derivation in the universal differential equations paper (note the reinitialization), we do not recommend BacksolveAdjoint one DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables.","category":"page"},{"location":"ControllingAdjoints/#Stochastic-Differential-Equations","page":"Controlling Choices of Adjoints","title":"Stochastic Differential Equations","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"We note that all of the adjoints except QuadratureAdjoint are applicable to stochastic differential equations.","category":"page"},{"location":"ControllingAdjoints/#Delay-Differential-Equations","page":"Controlling Choices of Adjoints","title":"Delay Differential Equations","text":"","category":"section"},{"location":"ControllingAdjoints/","page":"Controlling Choices of Adjoints","title":"Controlling Choices of Adjoints","text":"We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques.","category":"page"},{"location":"examples/hybrid_diffeq/#Training-Neural-Networks-in-Hybrid-Differential-Equations","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"","category":"section"},{"location":"examples/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"Hybrid differential equations are differential equations with implicit or explicit discontinuities as specified by callbacks. In the following example, explicit dosing times are given for a pharmacometric model and the universal differential equation is trained to uncover the missing dynamical equations.","category":"page"},{"location":"examples/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"using DiffEqFlux, DifferentialEquations, Plots\nu0 = Float32[2.; 0.]\ndatasize = 100\ntspan = (0.0f0,10.5f0)\ndosetimes = [1.0,2.0,4.0,8.0]\n\nfunction affect!(integrator)\n    integrator.u = integrator.u.+1\nend\ncb_ = PresetTimeCallback(dosetimes,affect!,save_positions=(false,false))\nfunction trueODEfunc(du,u,p,t)\n    du .= -u\nend\nt = range(tspan[1],tspan[2],length=datasize)\n\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),callback=cb_,saveat=t))\ndudt2 = Chain(Dense(2,50,tanh),\n             Dense(50,2))\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\n\nfunction dudt(du,u,p,t)\n    du[1:2] .= -u[1:2]\n    du[3:end] .= re(p)(u[1:2]) #re(p)(u[3:end])\nend\nz0 = Float32[u0;u0]\nprob = ODEProblem(dudt,z0,tspan)\n\naffect!(integrator) = integrator.u[1:2] .= integrator.u[3:end]\ncb = PresetTimeCallback(dosetimes,affect!,save_positions=(false,false))\n\nfunction predict_n_ode()\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Tsit5(),u0=z0,p=p,callback=cb,saveat=t,sensealg=ReverseDiffAdjoint()))[1:2,:]\n    #Array(solve(prob,Tsit5(),u0=z0,p=p,saveat=t))[1:2,:]\nend\n\nfunction loss_n_ode()\n    pred = predict_n_ode()\n    loss = sum(abs2,ode_data .- pred)\n    loss\nend\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\n\ncba = function (;doplot=false) #callback function to observe training\n  pred = predict_n_ode()\n  display(sum(abs2,ode_data .- pred))\n  # plot current prediction against data\n  pl = scatter(t,ode_data[1,:],label=\"data\")\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\n  display(plot(pl))\n  return false\nend\ncba()\n\nps = Flux.params(p)\ndata = Iterators.repeated((), 200)\nFlux.train!(loss_n_ode, ps, data, ADAM(0.05), cb = cba)","category":"page"},{"location":"examples/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"(Image: Hybrid Universal Differential Equation)","category":"page"},{"location":"examples/hybrid_diffeq/#Note-on-Sensitivity-Methods","page":"Training Neural Networks in Hybrid Differential Equations","title":"Note on Sensitivity Methods","text":"","category":"section"},{"location":"examples/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"The continuous adjoint sensitivities BacksolveAdjoint, InterpolatingAdjoint, and QuadratureAdjoint are compatible with events for ODEs. BacksolveAdjoint and InterpolatingAdjoint can also handle events for SDEs. Use BacksolveAdjoint if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.","category":"page"},{"location":"examples/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"All methods based on discrete sensitivity analysis via automatic differentiation, like ReverseDiffAdjoint, TrackerAdjoint, or ForwardDiffSensitivity are the methods to use (and ReverseDiffAdjoint is demonstrated above), are compatible with events. This applies to SDEs, DAEs, and DDEs as well.","category":"page"},{"location":"examples/local_minima/#Strategies-to-Avoid-Local-Minima","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"","category":"section"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Local minima can be an issue with fitting neural differential equations. However, there are many strategies to avoid local minima:","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Insert stochasticity into the loss function through minibatching\nWeigh the loss function to allow for fitting earlier portions first\nChanging the optimizers to allow_f_increases\nIteratively grow the fit\nTraining the initial conditions and the parameters to start","category":"page"},{"location":"examples/local_minima/#allow_f_increasestrue","page":"Strategies to Avoid Local Minima","title":"allow_f_increases=true","text":"","category":"section"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"With Optim.jl optimizers, you can set allow_f_increases=true in order to let increases in the loss function not cause an automatic halt of the optimization process. Using a method like BFGS or NewtonTrustRegion is not guaranteed to have monotonic convergence and so this can stop early exits which can result in local minima. This looks like:","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"pmin = DiffEqFlux.sciml_train(loss_neuralode, pstart, NewtonTrustRegion(), cb=cb,\r\n                              maxiters = 200, allow_f_increases = true)","category":"page"},{"location":"examples/local_minima/#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima","page":"Strategies to Avoid Local Minima","title":"Iterative Growing Of Fits to Reduce Probability of Bad Local Minima","text":"","category":"section"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"In this example we will show how to use strategy (4) in order to increase the robustness of the fit. Let's start with the same neural ODE example we've used before except with one small twist: we wish to find the neural ODE that fits on (0,5.0). Naively, we use the same training strategy as before:","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"using DiffEqFlux, DifferentialEquations, Plots\r\n\r\nu0 = Float32[2.0; 0.0]\r\ndatasize = 30\r\ntspan = (0.0f0, 5.0f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\ndudt2 = FastChain((x, p) -> x.^3,\r\n                  FastDense(2, 16, tanh),\r\n                  FastDense(16, 2))\r\nprob_neuralode = NeuralODE(dudt2, tspan, Vern7(), saveat = tsteps, abstol=1e-6, reltol=1e-6)\r\n\r\nfunction predict_neuralode(p)\r\n  Array(prob_neuralode(u0, p))\r\nend\r\n\r\nfunction loss_neuralode(p)\r\n    pred = predict_neuralode(p)\r\n    loss = sum(abs2, (ode_data[:,1:size(pred,2)] .- pred))\r\n    return loss, pred\r\nend\r\n\r\niter = 0\r\ncallback = function (p, l, pred; doplot = true)\r\n  global iter\r\n  iter += 1\r\n\r\n  display(l)\r\n  if doplot\r\n    # plot current prediction against data\r\n    plt = scatter(tsteps[1:size(pred,2)], ode_data[1,1:size(pred,2)], label = \"data\")\r\n    scatter!(plt, tsteps[1:size(pred,2)], pred[1,:], label = \"prediction\")\r\n    display(plot(plt))\r\n  end\r\n\r\n  return false\r\nend\r\n\r\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\r\n                                          ADAM(0.05), cb = callback,\r\n                                          maxiters = 300)\r\n\r\ncallback(result_neuralode.u,loss_neuralode(result_neuralode.u)...;doplot=true)\r\nsavefig(\"local_minima.png\")","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"However, we've now fallen into a trap of a local minima. If the optimizer changes the parameters so it dips early, it will increase the loss because there will be more error in the later parts of the time series. Thus it tends to just stay flat and never fit perfectly. This thus suggests strategies (2) and (3): do not allow the later parts of the time series to influence the fit until the later stages. Strategy (3) seems to be more robust, so this is what will be demonstrated.","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Let's start by reducing the timespan to (0,1.5):","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,1.5), Tsit5(), saveat = tsteps[tsteps .<= 1.5])\r\n\r\nresult_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\r\n                                           ADAM(0.05), cb = callback,\r\n                                           maxiters = 300)\r\n\r\ncallback(result_neuralode2.u,loss_neuralode(result_neuralode2.u)...;doplot=true)\r\nsavefig(\"shortplot1.png\")","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"This fits beautifully. Now let's grow the timespan and utilize the parameters from our (0,1.5) fit as the initial condition to our next fit:","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,3.0), Tsit5(), saveat = tsteps[tsteps .<= 3.0])\r\n\r\nresult_neuralode3 = DiffEqFlux.sciml_train(loss_neuralode,\r\n                                           result_neuralode2.u,\r\n                                           ADAM(0.05), maxiters = 300,\r\n                                           cb = callback)\r\ncallback(result_neuralode3.u,loss_neuralode(result_neuralode3.u)...;doplot=true)\r\nsavefig(\"shortplot2.png\")","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Once again a great fit. Now we utilize these parameters as the initial condition to the full fit:","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0,5.0), Tsit5(), saveat = tsteps)\r\n\r\nresult_neuralode4 = DiffEqFlux.sciml_train(loss_neuralode,\r\n                                           result_neuralode3.u,\r\n                                           ADAM(0.01), maxiters = 300,\r\n                                           cb = callback)\r\ncallback(result_neuralode4.u,loss_neuralode(result_neuralode4.u)...;doplot=true)\r\nsavefig(\"fullplot.png\")","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"examples/local_minima/#Training-both-the-initial-conditions-and-the-parameters-to-start","page":"Strategies to Avoid Local Minima","title":"Training both the initial conditions and the parameters to start","text":"","category":"section"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"In this example we will show how to use strategy (5) in order to accomplish the same goal, except rather than growing the trajectory iteratively, we can train on the whole trajectory. We do this by allowing the neural ODE to learn both the initial conditions and parameters to start, and then reset the initial conditions back and train only the parameters. Note: this strategy is demonstrated for the (0, 5) time span and (0, 10), any longer and more iterations will be required. Alternatively, one could use a mix of (4) and (5), or breaking up the trajectory into chunks and just (5).","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"\r\nusing DiffEqFlux, Plots, DifferentialEquations\r\n\r\n\r\n#Starting example with tspan (0, 5)\r\nu0 = Float32[2.0; 0.0]\r\ndatasize = 30\r\ntspan = (0.0f0, 5.0f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\n#Using flux here to easily demonstrate the idea, but this can be done with sciml_train!\r\ndudt2 = Chain(Dense(2,16, tanh),\r\n             Dense(16,2))\r\n\r\n\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n    Array(solve(prob,u0=u0,p=p, saveat=tsteps))\r\nend\r\n\r\nfunction loss_n_ode()\r\n      pred = predict_n_ode()\r\n      sqnorm(x) = sum(abs2, x)\r\n      loss = sum(abs2,ode_data .- pred)\r\n      loss\r\nend\r\n\r\nfunction cb(;doplot=false) #callback function to observe training\r\n    pred = predict_n_ode()\r\n    display(sum(abs2,ode_data .- pred))\r\n    if doplot\r\n      # plot current prediction against data\r\n      pl = plot(tsteps,ode_data[1,:],label=\"data\")\r\n      plot!(pl,tsteps,pred[1,:],label=\"prediction\")\r\n      display(plot(pl))\r\n    end\r\n    return false\r\nend\r\npredict_n_ode()\r\nloss_n_ode()\r\ncb(;doplot=true)\r\n\r\ndata = Iterators.repeated((), 1000)\r\n\r\n#Specify to flux to include both the initial conditions (IC) and parameters of the NODE to train\r\nFlux.train!(loss_n_ode, Flux.params(u0, p), data,\r\n                    Flux.Optimise.ADAM(0.05), cb = cb)\r\n\r\n#Here we reset the IC back to the original and train only the NODE parameters\r\nu0 = Float32[2.0; 0.0]\r\nFlux.train!(loss_n_ode, Flux.params(p), data,\r\n            Flux.Optimise.ADAM(0.05), cb = cb)\r\n\r\ncb(;doplot=true)\r\n\r\n#Now use the same technique for a longer tspan (0, 10)\r\ndatasize = 30\r\ntspan = (0.0f0, 10.0f0)\r\ntsteps = range(tspan[1], tspan[2], length = datasize)\r\n\r\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\r\n\r\ndudt2 = Chain(Dense(2,16, tanh),\r\n             Dense(16,2))\r\n\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\n\r\n\r\ndata = Iterators.repeated((), 1500)\r\n\r\nFlux.train!(loss_n_ode, Flux.params(u0, p), data,\r\n                    Flux.Optimise.ADAM(0.05), cb = cb)\r\n\r\n\r\n\r\nu0 = Float32[2.0; 0.0]\r\nFlux.train!(loss_n_ode, Flux.params(p), data,\r\n            Flux.Optimise.ADAM(0.05), cb = cb)\r\n\r\ncb(;doplot=true)\r\n","category":"page"},{"location":"examples/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"And there we go, a set of robust strategies for fitting an equation that would otherwise get stuck in a local optima.","category":"page"},{"location":"examples/optimization_sde/#Optimization-of-Stochastic-Differential-Equations","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"","category":"section"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Here we demonstrate sensealg = ForwardDiffSensitivity() (provided by DiffEqSensitivity.jl) for forward-mode automatic differentiation of a small stochastic differential equation. For large parameter equations, like neural stochastic differential equations, you should use reverse-mode automatic differentiation. However, forward-mode can be more efficient for low numbers of parameters (<100). (Note: the default is reverse-mode AD which is more suitable for things like neural SDEs!)","category":"page"},{"location":"examples/optimization_sde/#Example-1:-Fitting-Data-with-SDEs-via-Method-of-Moments-and-Parallelism","page":"Optimization of Stochastic Differential Equations","title":"Example 1: Fitting Data with SDEs via Method of Moments and Parallelism","text":"","category":"section"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's do the most common scenario: fitting data. Let's say our ecological system is a stochastic process. Each time we solve this equation we get a different solution, so we need a sensible data source.","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using DiffEqFlux, DifferentialEquations, Plots\nfunction lotka_volterra!(du,u,p,t)\n  x,y = u\n  α,β,γ,δ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = δ*x*y - γ*y\nend\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\n\nfunction multiplicative_noise!(du,u,p,t)\n  x,y = u\n  du[1] = p[5]*x\n  du[2] = p[6]*y\nend\np = [1.5,1.0,3.0,1.0,0.3,0.3]\n\nprob = SDEProblem(lotka_volterra!,multiplicative_noise!,u0,tspan,p)\nsol = solve(prob)\nplot(sol)","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's assume that we are observing the seasonal behavior of this system and have 10,000 years of data, corresponding to 10,000 observations of this timeseries. We can utilize this to get the seasonal means and variances. To simulate that scenario, we will generate 10,000 trajectories from the SDE to build our dataset:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using Statistics\nensembleprob = EnsembleProblem(prob)\n@time sol = solve(ensembleprob,SOSRI(),saveat=0.1,trajectories=10_000)\ntruemean = mean(sol,dims=3)[:,:]\ntruevar  = var(sol,dims=3)[:,:]","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"From here, we wish to utilize the method of moments to fit the SDE's parameters. Thus our loss function will be to solve the SDE a bunch of times and compute moment equations and use these as our loss against the original series. We then plot the evolution of the means and variances to verify the fit. For example:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"function loss(p)\n  tmp_prob = remake(prob,p=p)\n  ensembleprob = EnsembleProblem(tmp_prob)\n  tmp_sol = solve(ensembleprob,SOSRI(),saveat=0.1,trajectories=1000)\n  arrsol = Array(tmp_sol)\n  sum(abs2,truemean - mean(arrsol,dims=3)) + 0.1sum(abs2,truevar - var(arrsol,dims=3)),arrsol\nend\n\nfunction cb2(p,l,arrsol)\n  @show p,l\n  means = mean(arrsol,dims=3)[:,:]\n  vars = var(arrsol,dims=3)[:,:]\n  p1 = plot(sol[1].t,means',lw=5)\n  scatter!(p1,sol[1].t,truemean')\n  p2 = plot(sol[1].t,vars',lw=5)\n  scatter!(p2,sol[1].t,truevar')\n  p = plot(p1,p2,layout = (2,1))\n  display(p)\n  false\nend","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"We can then use sciml_train to fit the SDE:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"pinit = [1.2,0.8,2.5,0.8,0.1,0.1]\n@time res = DiffEqFlux.sciml_train(loss,pinit,ADAM(0.05),cb=cb2,maxiters = 100)","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"The final print out was:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(p, l) = ([1.5242134195974462, 1.019859938499017, 2.9120928257869227, 0.9840408090733335, 0.29427123791721765, 0.3334393815923646], 1.7046719990657184)","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Notice that both the parameters of the deterministic drift equations and the stochastic portion (the diffusion equation) are fit through this process! Also notice that the final fit of the moment equations is close:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"The time for the full fitting process was:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"250.654845 seconds (4.69 G allocations: 104.868 GiB, 11.87% gc time)","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"approximately 4 minutes.","category":"page"},{"location":"examples/optimization_sde/#Example-2:-Fitting-SDEs-via-Bayesian-Quasi-Likelihood-Approaches","page":"Optimization of Stochastic Differential Equations","title":"Example 2: Fitting SDEs via Bayesian Quasi-Likelihood Approaches","text":"","category":"section"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"An inference method which can be much more efficient in many cases is the quasi-likelihood approach. This approach matches the random likelihood of the SDE output with the random sampling of a Bayesian inference problem to more efficiently directly estimate the posterior distribution. For more information, please see the Turing.jl Bayesian Differential Equations tutorial","category":"page"},{"location":"examples/optimization_sde/#Example-3:-Controlling-SDEs-to-an-objective","page":"Optimization of Stochastic Differential Equations","title":"Example 3: Controlling SDEs to an objective","text":"","category":"section"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"In this example, we will find the parameters of the SDE that force the solution to be close to the constant 1.","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using DifferentialEquations, DiffEqFlux, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\nfunction lotka_volterra_noise!(du, u, p, t)\n  du[1] = 0.1u[1]\n  du[2] = 0.1u[2]\nend\n\nu0 = [1.0,1.0]\ntspan = (0.0, 10.0)\np = [2.2, 1.0, 2.0, 0.4]\nprob_sde = SDEProblem(lotka_volterra!, lotka_volterra_noise!, u0, tspan)\n\n\nfunction predict_sde(p)\n  return Array(solve(prob_sde, SOSRI(), p=p,\n               sensealg = ForwardDiffSensitivity(), saveat = 0.1))\nend\n\nloss_sde(p) = sum(abs2, x-1 for x in predict_sde(p))","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"For this training process, because the loss function is stochastic, we will use the ADAM optimizer from Flux.jl. The sciml_train function is the same as before. However, to speed up the training process, we will use a global counter so that way we only plot the current results every 10 iterations. This looks like:","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"callback = function (p, l)\n  display(l)\n  remade_solution = solve(remake(prob_sde, p = p), SOSRI(), saveat = 0.1)\n  plt = plot(remade_solution, ylim = (0, 6))\n  display(plt)\n  return false\nend","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's optimize","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"result_sde = DiffEqFlux.sciml_train(loss_sde, p, ADAM(0.1),\n                                    cb = callback, maxiters = 100)","category":"page"},{"location":"examples/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/optimal_control/#optcontrol","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"","category":"section"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Here we will solve a classic optimal control problem with a universal differential equation. Let","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"x^ = u^3(t)","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where we want to optimize our controller u(t) such that the following is minimized:","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert x^prime(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where i is measured on (0,8) at 0.01 intervals. To do this, we rewrite the ODE in first order form:","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"beginaligned\r\nx^prime = v \r\nv^ = u^3(t) \r\nendaligned","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"and thus","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert v(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"is our loss function on the first order system. We thus choose a neural network form for u and optimize the equation with respect to this loss. Note that we will first reduce control cost (the last term) by 10x in order to bump the network out of a local minimum. This looks like:","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"using DiffEqFlux, DifferentialEquations, Plots, Statistics\r\ntspan = (0.0f0,8.0f0)\r\nann = FastChain(FastDense(1,32,tanh), FastDense(32,32,tanh), FastDense(32,1))\r\nθ = initial_params(ann)\r\nfunction dxdt_(dx,x,p,t)\r\n    x1, x2 = x\r\n    dx[1] = x[2]\r\n    dx[2] = ann([t],p)[1]^3\r\nend\r\nx0 = [-4f0,0f0]\r\nts = Float32.(collect(0.0:0.01:tspan[2]))\r\nprob = ODEProblem(dxdt_,x0,tspan,θ)\r\nsolve(prob,Vern9(),abstol=1e-10,reltol=1e-10)\r\nfunction predict_adjoint(θ)\r\n  Array(solve(prob,Vern9(),p=θ,saveat=ts,sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\r\nend\r\nfunction loss_adjoint(θ)\r\n  x = predict_adjoint(θ)\r\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(ann([t],θ)) for t in ts])/10\r\nend\r\nl = loss_adjoint(θ)\r\ncb = function (θ,l)\r\n  println(l)\r\n  p = plot(solve(remake(prob,p=θ),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\r\n  plot!(p,ts,[first(ann([t],θ)) for t in ts],label=\"u(t)\",lw=3)\r\n  display(p)\r\n  return false\r\nend\r\n# Display the ODE with the current parameter values.\r\ncb(θ,l)\r\nloss1 = loss_adjoint(θ)\r\nres1 = DiffEqFlux.sciml_train(loss_adjoint, θ, ADAM(0.005), cb = cb,maxiters=100)\r\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.u,\r\n                              BFGS(initial_stepnorm=0.01), cb = cb,maxiters=100,\r\n                              allow_f_increases = false)","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Now that the system is in a better behaved part of parameter space, we return to the original loss function to finish the optimization:","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"function loss_adjoint(θ)\r\n  x = predict_adjoint(θ)\r\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(ann([t],θ)) for t in ts])\r\nend\r\n\r\nres3 = DiffEqFlux.sciml_train(loss_adjoint, res2.u,\r\n                              BFGS(initial_stepnorm=0.01), cb = cb,maxiters=100,\r\n                              allow_f_increases = false)\r\n\r\nl = loss_adjoint(res3.u)\r\ncb(res3.u,l)\r\np = plot(solve(remake(prob,p=res3.u),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\r\nplot!(p,ts,[first(ann([t],res3.u)) for t in ts],label=\"u(t)\",lw=3)\r\nsavefig(\"optimal_control.png\")","category":"page"},{"location":"examples/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"(Image: )","category":"page"},{"location":"examples/mnist_neural_ode/#mnist","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Training a classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend\n\n# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)\n\ndown = Chain(Flux.flatten, Dense(784, 20, tanh)) |> gpu\n\nnn = Chain(Dense(20, 10, tanh),\n           Dense(10, 10, tanh),\n           Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Chain(Dense(20, 10)) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend\n\n# Build our overall model topology\nmodel = Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncb() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = cb)","category":"page"},{"location":"examples/mnist_neural_ode/#Step-by-Step-Description","page":"GPU-based MNIST Neural ODE Classifier","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#Load-Packages","page":"GPU-based MNIST Neural ODE Classifier","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets","category":"page"},{"location":"examples/mnist_neural_ode/#GPU","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"examples/mnist_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"GPU-based MNIST Neural ODE Classifier","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    imgs, labels_raw = MNIST.traindata();\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)","category":"page"},{"location":"examples/mnist_neural_ode/#Layers","page":"GPU-based MNIST Neural ODE Classifier","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down = Chain(Flux.flatten, Dense(784, 20, tanh)) |> gpu\n\nnn = Chain(Dense(20, 10, tanh),\n           Dense(10, 10, tanh),\n           Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Chain(Dense(20, 10)) |> gpu","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down: This layer downsamples our images into a 20 dimensional feature vector.         It takes a 28 x 28 image, flattens it, and then passes it through a fully connected         layer with tanh activation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn: A 3 layers Deep Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"fc: The final fully connected layer which maps our learned feature vector to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"|> gpu: An utility function which transfers our model to GPU, if it is available","category":"page"},{"location":"examples/mnist_neural_ode/#Array-Conversion","page":"GPU-based MNIST Neural ODE Classifier","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"When using NeuralODE, this function converts the ODESolution's DiffEqArray to a Matrix (CuArray), and reduces the matrix from 3 to 2 dimensions for use in the next layer.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) to Array(x).","category":"page"},{"location":"examples/mnist_neural_ode/#Build-Topology","page":"GPU-based MNIST Neural ODE Classifier","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Build our overall model topology\nmodel = Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Chain(down,\n                 nn,\n                 fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"examples/mnist_neural_ode/#Prediction","page":"GPU-based MNIST Neural ODE Classifier","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_neural_ode/#Accuracy","page":"GPU-based MNIST Neural ODE Classifier","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(m, train_dataloader)","category":"page"},{"location":"examples/mnist_neural_ode/#Training-Parameters","page":"GPU-based MNIST Neural ODE Classifier","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_neural_ode/#Loss","page":"GPU-based MNIST Neural ODE Classifier","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"examples/mnist_neural_ode/#Optimizer","page":"GPU-based MNIST Neural ODE Classifier","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"opt = ADAM(0.05)","category":"page"},{"location":"examples/mnist_neural_ode/#CallBack","page":"GPU-based MNIST Neural ODE Classifier","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"cb() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"examples/mnist_neural_ode/#Train","page":"GPU-based MNIST Neural ODE Classifier","title":"Train","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params( down, nn_ode.p, fc), zip( x_train, y_train ), opt, cb = cb)","category":"page"},{"location":"examples/mnist_neural_ode/#Expected-Output","page":"GPU-based MNIST Neural ODE Classifier","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Iter:   1 || Train Accuracy: 16.203 || Test Accuracy: 16.933\nIter:  11 || Train Accuracy: 64.406 || Test Accuracy: 64.900\nIter:  21 || Train Accuracy: 76.656 || Test Accuracy: 76.667\nIter:  31 || Train Accuracy: 81.758 || Test Accuracy: 81.683\nIter:  41 || Train Accuracy: 81.078 || Test Accuracy: 81.967\nIter:  51 || Train Accuracy: 83.953 || Test Accuracy: 84.417\nIter:  61 || Train Accuracy: 85.266 || Test Accuracy: 85.017\nIter:  71 || Train Accuracy: 85.938 || Test Accuracy: 86.400\nIter:  81 || Train Accuracy: 84.836 || Test Accuracy: 85.533\nIter:  91 || Train Accuracy: 86.148 || Test Accuracy: 86.583\nIter: 101 || Train Accuracy: 83.859 || Test Accuracy: 84.500\nIter: 111 || Train Accuracy: 86.227 || Test Accuracy: 86.617\nIter: 121 || Train Accuracy: 87.508 || Test Accuracy: 87.200\nIter: 131 || Train Accuracy: 86.227 || Test Accuracy: 85.917\nIter: 141 || Train Accuracy: 84.453 || Test Accuracy: 84.850\nIter: 151 || Train Accuracy: 86.063 || Test Accuracy: 85.650\nIter: 161 || Train Accuracy: 88.375 || Test Accuracy: 88.033\nIter: 171 || Train Accuracy: 87.398 || Test Accuracy: 87.683\nIter: 181 || Train Accuracy: 88.070 || Test Accuracy: 88.350\nIter: 191 || Train Accuracy: 86.836 || Test Accuracy: 87.150\nIter: 201 || Train Accuracy: 89.266 || Test Accuracy: 88.583\nIter: 211 || Train Accuracy: 86.633 || Test Accuracy: 85.550\nIter: 221 || Train Accuracy: 89.313 || Test Accuracy: 88.217\nIter: 231 || Train Accuracy: 88.641 || Test Accuracy: 89.417\nIter: 241 || Train Accuracy: 88.617 || Test Accuracy: 88.550\nIter: 251 || Train Accuracy: 88.211 || Test Accuracy: 87.950\nIter: 261 || Train Accuracy: 87.742 || Test Accuracy: 87.317\nIter: 271 || Train Accuracy: 89.070 || Test Accuracy: 89.217\nIter: 281 || Train Accuracy: 89.703 || Test Accuracy: 89.067\nIter: 291 || Train Accuracy: 88.484 || Test Accuracy: 88.250\nIter: 301 || Train Accuracy: 87.898 || Test Accuracy: 88.367\nIter: 311 || Train Accuracy: 88.438 || Test Accuracy: 88.633\nIter: 321 || Train Accuracy: 88.664 || Test Accuracy: 88.567\nIter: 331 || Train Accuracy: 89.906 || Test Accuracy: 89.883\nIter: 341 || Train Accuracy: 88.883 || Test Accuracy: 88.667\nIter: 351 || Train Accuracy: 89.609 || Test Accuracy: 89.283\nIter: 361 || Train Accuracy: 89.516 || Test Accuracy: 89.117\nIter: 371 || Train Accuracy: 89.898 || Test Accuracy: 89.633\nIter: 381 || Train Accuracy: 89.055 || Test Accuracy: 89.017\nIter: 391 || Train Accuracy: 89.445 || Test Accuracy: 89.467\nIter: 401 || Train Accuracy: 89.156 || Test Accuracy: 88.250\nIter: 411 || Train Accuracy: 88.977 || Test Accuracy: 89.083\nIter: 421 || Train Accuracy: 90.109 || Test Accuracy: 89.417","category":"page"},{"location":"FastChain/#FastChain","page":"FastChain","title":"FastChain","text":"","category":"section"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"The FastChain system is a Flux-like explicit parameter neural network architecture system for less overhead in smaller neural networks. It acts explicitly, avoiding internal references to reduce overhead, while using explicitly defined adjoints to fuse operations. For neural networks with layers of lengths >~200, these optimizations  are overshadowed by the cost of matrix multiplication. However, for smaller layer operations,  this architecture can reduce a lot of the overhead traditionally seen in neural network architectures and thus is recommended in a lot of scientific machine learning use cases.","category":"page"},{"location":"FastChain/#Basics","page":"FastChain","title":"Basics","text":"","category":"section"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"The basic principle is that FastChain is a collection of functions of two values, (x,p), and chains these functions to call one after the next. Each layer in this chain gets a pre-defined amount of parameters sent to it. For example,","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"f = FastChain((x,p) -> x.^3,\r\n              FastDense(2,50,tanh),\r\n              FastDense(50,2))","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"FastChain here has a 2*50 + 50 length parameter FastDense(2,50,tanh) function and a 50*2 + 2 parameter function FastDense(50,2). The first function gets the default number of parameters which is 0. Thus, f(x,p) is equivalent to the following code:","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"function f(x,p)\r\n  tmp1 = x.^3\r\n  len1 = paramlength(FastDense(2,50,tanh))\r\n  tmp2 = FastDense(2,50,tanh)(tmp1,@view p[1:len1])\r\n  tmp3 = FastDense(50,2)(tmp2,@view p[(len1+1):end])\r\nend","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"FastChain functions thus require that the vector of neural network parameters is passed to it on each call, making the setup explicit in the passed parameters.","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"To get initial parameters for the optimization of a function defined by a FastChain, one simply calls initial_params(f), which returns the concatenation of the initial parameters for each layer. Notice that since all parameters are explicit, constructing and reconstructing chains/layers can be a memory-free operation, since the only memory is the parameter vector itself, which is handled by the user.","category":"page"},{"location":"FastChain/#FastChain-Interface","page":"FastChain","title":"FastChain Interface","text":"","category":"section"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"The only requirement to be a layer in FastChain is to be a 2-argument function l(x,p) and define the following traits:","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"paramlength(::typeof(l)): The number of parameters from the parameter vector to allocate to this layer. Defaults to zero.\ninitial_params(::typeof(l)): The function for defining the initial parameters of the layer. Should output a vector of length matching paramlength. Defaults to Float32[].","category":"page"},{"location":"FastChain/#FastChain-Compatible-Layers","page":"FastChain","title":"FastChain-Compatible Layers","text":"","category":"section"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"The following pre-defined layers can be used with FastChain:","category":"page"},{"location":"FastChain/","page":"FastChain","title":"FastChain","text":"FastDense\r\nStaticDense","category":"page"},{"location":"FastChain/#DiffEqFlux.FastDense","page":"FastChain","title":"DiffEqFlux.FastDense","text":"FastDense(in,out,activation=identity;           bias = true, precache = false ,initW = Flux.glorot_uniform, initb = Flux.zeros32)\n\nA Dense layer activation.(W*x + b) with input size in and output size out. The activation function defaults to identity, meaning the layer is an affine function. Initial parameters are taken to match Flux.Dense. 'bias' represents b in the layer and it defaults to true.'precache' is used to preallocate memory for the intermediate variables calculated during each pass. This avoids heap allocations in each pass which would otherwise slow down the computation, it defaults to false.\n\nNote that this function has specializations on tanh for a slightly faster adjoint with Zygote.\n\n\n\n\n\n","category":"type"},{"location":"FastChain/#DiffEqFlux.StaticDense","page":"FastChain","title":"DiffEqFlux.StaticDense","text":"StaticDense(in,out,activation=identity;           initW = Flux.glorot_uniform, initb = Flux.zeros32)\n\nA Dense layer activation.(W*x + b) with input size in and output size out. The activation function defaults to identity, meaning the layer is an affine function. Initial parameters are taken to match Flux.Dense. The internal calculations are done with StaticArrays for extra speed for small linear algebra operations. Should only be used for input/output sizes of approximately 16 or less. 'bias' represents bias(b) in the dense layer and it defaults to true.\n\nNote that this function has specializations on tanh for a slightly faster adjoint with Zygote.\n\n\n\n\n\n","category":"type"},{"location":"examples/feedback_control/#Universal-Differential-Equations-for-Neural-Feedback-Control","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"","category":"section"},{"location":"examples/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"You can also mix a known differential equation and a neural differential equation, so that the parameters and the neural network are estimated simultaneously!","category":"page"},{"location":"examples/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"We will assume that we know the dynamics of the second equation (linear dynamics), and our goal is to find a neural network that is dependent on the current state of the dynamical system that will control the second equation to stay close to 1.","category":"page"},{"location":"examples/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = 1.1f0\ntspan = (0.0f0, 25.0f0)\ntsteps = 0.0f0:1.0:25.0f0\n\nmodel_univ = FastChain(FastDense(2, 16, tanh),\n                       FastDense(16, 16, tanh),\n                       FastDense(16, 1))\n\n# The model weights are destructured into a vector of parameters\np_model = initial_params(model_univ)\nn_weights = length(p_model)\n\n# Parameters of the second equation (linear dynamics)\np_system = Float32[0.5, -0.5]\n\np_all = [p_model; p_system]\nθ = Float32[u0; p_all]\n\nfunction dudt_univ!(du, u, p, t)\n    # Destructure the parameters\n    model_weights = p[1:n_weights]\n    α = p[end - 1]\n    β = p[end]\n\n    # The neural network outputs a control taken by the system\n    # The system then produces an output\n    model_control, system_output = u\n\n    # Dynamics of the control and system\n    dmodel_control = model_univ(u, model_weights)[1]\n    dsystem_output = α*system_output + β*model_control\n\n    # Update in place\n    du[1] = dmodel_control\n    du[2] = dsystem_output\nend\n\nprob_univ = ODEProblem(dudt_univ!, [0f0, u0], tspan, p_all)\nsol_univ = solve(prob_univ, Tsit5(),abstol = 1e-8, reltol = 1e-6)\n\nfunction predict_univ(θ)\n  return Array(solve(prob_univ, Tsit5(), u0=[0f0, θ[1]], p=θ[2:end],\n                              sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)),\n                              saveat = tsteps))\nend\n\nloss_univ(θ) = sum(abs2, predict_univ(θ)[2,:] .- 1)\nl = loss_univ(θ)","category":"page"},{"location":"examples/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"list_plots = []\niter = 0\ncallback = function (θ, l)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  println(l)\n\n  plt = plot(predict_univ(θ)', ylim = (0, 6))\n  push!(list_plots, plt)\n  display(plt)\n  return false\nend","category":"page"},{"location":"examples/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"result_univ = DiffEqFlux.sciml_train(loss_univ, θ,\n                                     cb = callback)","category":"page"},{"location":"examples/data_parallel/#Data-Parallel-Multithreaded,-Distributed,-and-Multi-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqFlux.jl allows for data-parallel batching optimally on one computer, across an entire compute cluster, and batching along GPUs. This can be done by parallelizing within an ODE solve or between the ODE solves. The automatic differentiation tooling is compatible with the parallelism. The following examples demonstrate training over a few different modes of parallelism. These examples are not exhaustive.","category":"page"},{"location":"examples/data_parallel/#Within-ODE-Multithreaded-and-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Within-ODE Multithreaded and GPU Batching","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"We end by noting that there is an alternative way of batching which can be more efficient in some cases like neural ODEs. With a neural networks, columns are treated independently (by the properties of matrix multiplication). Thus for example, with FastChain we can define an ODE:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using DiffEqFlux, DifferentialEquations\n\ndudt = FastChain(FastDense(2,50,tanh),FastDense(50,2))\np = initial_params(dudt)\nf(u,p,t) = dudt(u,p)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"and we can solve this ODE where the initial condition is a vector:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"u0 = Float32[2.; 0.]\nprob = ODEProblem(f,u0,(0f0,1f0),p)\nsolve(prob,Tsit5())","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"or we can solve this ODE where the initial condition is a matrix, where each column is an independent system:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"u0 = Float32.([0 1 2\n               0 0 0])\nprob = ODEProblem(f,u0,(0f0,1f0),p)\nsolve(prob,Tsit5())","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"On the CPU this will multithread across the system (due to BLAS) and on GPUs this will parallelize the operations across the GPU. To GPU this, you'd simply move the parameters and the initial condition to the GPU:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"xs = Float32.([0 1 2\n               0 0 0])\nprob = ODEProblem(f,gpu(u0),(0f0,1f0),gpu(p))\nsolve(prob,Tsit5())","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"This method of parallelism is optimal if all of the operations are linear algebra operations such as a neural ODE. Thus this method of parallelism is demonstrated in the MNIST tutorial.","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"However, this method of parallelism has many limitations. First of all, the ODE function is required to be written in a way that is independent across the columns. Not all ODEs are written like this, so one needs to be careful. But additionally, this method is ineffective if the ODE function has many serial operations, like u[1]*u[2] - u[3]. In such a case, this indexing behavior will dominate the runtime and cause the parallelism to sometimes even be detrimental.","category":"page"},{"location":"examples/data_parallel/#Out-of-ODE-Parallelism","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Out of ODE Parallelism","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Instead of parallelizing within an ODE solve, one can parallelize the solves to the ODE itself. While this will be less effective on very large ODEs, like big neural ODE image classifiers, this method be effective even if the ODE is small or the f function is not well-parallelized. This kind of parallelism is done via the DifferentialEquations.jl ensemble interface. The following examples showcase multithreaded, cluster, and (multi)GPU parallelism through this interface.","category":"page"},{"location":"examples/data_parallel/#Multithreaded-Batching-At-a-Glance","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multithreaded Batching At a Glance","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"The following is a full copy-paste example for the multithreading. Distributed and GPU minibatching are described below.","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using DifferentialEquations, DiffEqFlux\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem((u, p, t) -> 1.01u .* p, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\n# loss function\nloss_serial(θ)   = sum(abs2,1.0.-Array(model1(θ,EnsembleSerial())))\nloss_threaded(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleThreads())))\n\ncb = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nl1 = loss_serial(θ)\nres_serial = DiffEqFlux.sciml_train(loss_serial, θ, opt; cb = cb, maxiters=100)\nres_threads = DiffEqFlux.sciml_train(loss_threaded, θ, opt; cb = cb, maxiters=100)","category":"page"},{"location":"examples/data_parallel/#Multithreaded-Batching-In-Depth","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multithreaded Batching In-Depth","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"In order to make use of the ensemble interface, we need to build an EnsembleProblem. The prob_func is the function for determining the different DEProblems to solve. This is the place where we can randomly sample initial conditions or pull initial conditions from an array of batches in order to perform our study. To do this, we first define a prototype DEProblem. Here we use the following ODEProblem as our base:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"prob = ODEProblem((u, p, t) -> 1.01u .* p, [θ[1]], (0.0, 1.0), [θ[2]])","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"In the prob_func we define how to build a new problem based on the base problem. In this case, we want to change u0 by a constant, i.e. 0.5 .+ i/100 .* prob.u0 for different trajectories labelled by i. Thus we use the remake function from the problem interface to do so:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"function prob_func(prob, i, repeat)\n  remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\nend","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"We now build the EnsembleProblem with this basis:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Now to solve an ensemble problem, we need to choose an ensembling algorithm and choose the number of trajectories to solve. Here let's solve this in serial with 100 trajectories. Note that i will thus run from 1:100.","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleSerial(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"and thus running in multithreading would be:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleThreads(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"This whole mechanism is differentiable, so we then put it in a training loop and it soars. Note that you need to make sure that Julia's multithreading is enabled, which you can do via:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Threads.nthreads()","category":"page"},{"location":"examples/data_parallel/#Distributed-Batching-Across-a-Cluster","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Distributed Batching Across a Cluster","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Changing to distributed computing is very simple as well. The setup is all the same, except you utilize EnsembleDistributed as the ensembler:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleDistributed(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Note that for this to work you need to ensure that your processes are already started. For more information on setting up processes and utilizing a compute cluster, see the official distributed documentation. The key feature to recognize is that, due to the message passing required for cluster compute, one needs to ensure that all of the required functions are defined on the worker processes. The following is a full example of a distributed batching setup:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using Distributed\naddprocs(4)\n\n@everywhere begin\n  using DifferentialEquations, DiffEqFlux\n  function f(u,p,t)\n    1.01u .* p\n  end\nend\n\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem(f, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\ncb = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nloss_distributed(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleDistributed())))\nl1 = loss_distributed(θ)\nres_distributed = DiffEqFlux.sciml_train(loss_distributed, θ, opt; cb = cb, maxiters=100)","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"And note that only addprocs(4) needs to be changed in order to make this demo run across a cluster. For more information on adding processes to a cluster, check out ClusterManagers.jl.","category":"page"},{"location":"examples/data_parallel/#Minibatching-Across-GPUs-with-DiffEqGPU","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Minibatching Across GPUs with DiffEqGPU","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqGPU.jl allows for generating code parallelizes an ensemble on generated CUDA kernels. This method is efficient for sufficiently small (<100 ODE) problems where the significant computational cost is due to the large number of batch trajectories that need to be solved. This kernel-building process adds a few restrictions to the function, such as requiring it has no boundschecking or allocations. The following is an example of minibatch ensemble parallelism across a GPU:","category":"page"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using DifferentialEquations, DiffEqFlux\nfunction f(du,u,p,t)\n  @inbounds begin\n    du[1] = 1.01 * u[1] * p[1] * p[2]\n  end\nend\n\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem(f, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\ncb = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nloss_gpu(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleGPUArray())))\nl1 = loss_gpu(θ)\nres_gpu = DiffEqFlux.sciml_train(loss_gpu, θ, opt; cb = cb, maxiters=100)","category":"page"},{"location":"examples/data_parallel/#Multi-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multi-GPU Batching","text":"","category":"section"},{"location":"examples/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqGPU supports batching across multiple GPUs. See its README for details on setting it up.","category":"page"},{"location":"examples/minibatch/#Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"","category":"section"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"using DifferentialEquations, DiffEqFlux, Plots\nusing IterTools: ncycle \n\n\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k*(temp-temp_m) \n  end\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2)/8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend\n\n\nann = FastChain(FastDense(1,8,tanh), FastDense(8,1,tanh))\nθ = initial_params(ann)\n\nfunction dudt_(u,p,t)           \n    ann(u, p).* u\nend\n\nfunction predict_adjoint(time_batch)\n    _prob = remake(prob,u0=u0,p=θ)\n    Array(solve(_prob, Tsit5(), saveat = time_batch)) \nend\n\nfunction loss_adjoint(batch, time_batch)\n    pred = predict_adjoint(time_batch)\n    sum(abs2, batch - pred)#, pred\nend\n\n\nu0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 3.0f0)\n\nt = range(tspan[1], tspan[2], length=datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat=t))\n\nprob = ODEProblem{false}(dudt_, u0, tspan, θ)\n\nk = 10\ntrain_loader = Flux.Data.DataLoader((ode_data, t), batchsize = k)\n\nfor (x, y) in train_loader\n    @show x\n    @show y\nend\n\nnumEpochs = 300\nlosses=[]\ncb() = begin\n    l=loss_adjoint(ode_data, t)\n    push!(losses, l)\n    @show l\n    pred=predict_adjoint(t)\n    pl = scatter(t,ode_data[1,:],label=\"data\", color=:black, ylim=(150,200))\n    scatter!(pl,t,pred[1,:],label=\"prediction\", color=:darkgreen)\n    display(plot(pl))\n    false\nend \n\nopt=ADAM(0.05)\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader,numEpochs), opt, cb=Flux.throttle(cb, 10))\n\n#Now lets see how well it generalizes to new initial conditions \n\nstarting_temp=collect(10:30:250)\ntrue_prob_func(u0)=ODEProblem(true_sol, [u0], tspan)\ncolor_cycle=palette(:tab10)\npl=plot()\nfor (j,temp) in enumerate(starting_temp)\n    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0,10.0f0)), Tsit5(), saveat=0.0:0.5:10.0)\n    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0,10.0f0), θ))\n    scatter!(pl, ode_test_sol, var=(0,1), label=\"\", color=color_cycle[j])\n    plot!(pl, ode_nn_sol, var=(0,1), label=\"\", color=color_cycle[j], lw=2.0)\nend\ndisplay(pl) \ntitle!(\"Neural ODE for Newton's Law of Cooling: Test Data\")\nxlabel!(\"Time\")\nylabel!(\"Temp\") \n\n\n# How to use MLDataUtils \nusing MLDataUtils\ntrain_loader, _, _ = kfolds((ode_data, t))\n\n@info \"Now training using the MLDataUtils format\"\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(eachbatch(train_loader[1], k), numEpochs), opt, cb=Flux.throttle(cb, 10))","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"When training a neural network we need to find the gradient with respect to our data set. There are three main ways to partition our data when using a training algorithm like gradient descent: stochastic, batching and mini-batching. Stochastic gradient descent trains on a single random data point each epoch. This allows for the neural network to better converge to the global minimum even on noisy data but is computationally inefficient. Batch gradient descent trains on the whole data set each epoch and while computationally efficient is prone to converging to local minima. Mini-batching combines both of these advantages and by training on a small random \"mini-batch\" of the data each epoch can converge to the global minimum while remaining more computationally efficient than stochastic descent. Typically we do this by randomly selecting subsets of the data each epoch and use this subset to train on. We can also pre-batch the data by creating an iterator holding these randomly selected batches before beginning to train. The proper size for the batch can be determined experimentally. Let us see how to do this with Julia. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"For this example we will use a very simple ordinary differential equation, newtons law of cooling. We can represent this in Julia like so. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"using DifferentialEquations, DiffEqFlux, Plots\nusing IterTools: ncycle \n\n\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k*(temp-temp_m) \n  end\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2)/8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Now we define a neural-network using a linear approximation with 1 hidden layer of 8 neurons.  ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"ann = FastChain(FastDense(1,8,tanh), FastDense(8,1,tanh))\nθ = initial_params(ann)\n\nfunction dudt_(u,p,t)           \n    ann(u, p).* u\nend","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"From here we build a loss function around it. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"function predict_adjoint(time_batch)\n    _prob = remake(prob, u0=u0, p=θ)\n    Array(solve(_prob, Tsit5(), saveat = time_batch)) \nend\n\nfunction loss_adjoint(batch, time_batch)\n    pred = predict_adjoint(time_batch)\n    sum(abs2, batch - pred)#, pred\nend","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"To add support for batches of size k we use Flux.Data.DataLoader. To use this we pass in the ode_data and t as the 'x' and 'y' data to batch respectively. The parameter batchsize controls the size of our batches. We check our implementation by iterating over the batched data. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"u0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 3.0f0)\n\nt = range(tspan[1], tspan[2], length=datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat=t))\nprob = ODEProblem{false}(dudt_, u0, tspan, θ)\n\nk = 10\ntrain_loader = Flux.Data.DataLoader((ode_data, t), batchsize = k)\nfor (x, y) in train_loader\n    @show x\n    @show y\nend\n\n\n#x = Float32[200.0 199.55284 199.1077 198.66454 198.22334 197.78413 197.3469 196.9116 196.47826 196.04686]\n#y = Float32[0.0, 0.05172414, 0.10344828, 0.15517241, 0.20689656, 0.25862068, 0.31034482, 0.36206895, 0.41379312, 0.46551725]\n#x = Float32[195.61739 195.18983 194.76418 194.34044 193.9186 193.49864 193.08057 192.66435 192.25 191.8375]\n#y = Float32[0.51724136, 0.5689655, 0.62068963, 0.67241377, 0.7241379, 0.7758621, 0.82758623, 0.87931037, 0.9310345, 0.98275864]\n#x = Float32[191.42683 191.01802 190.61102 190.20586 189.8025 189.40094 189.00119 188.60321 188.20702 187.8126]\n#y = Float32[1.0344827, 1.0862069, 1.137931, 1.1896552, 1.2413793, 1.2931035, 1.3448275, 1.3965517, 1.4482758, 1.5]","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Now we train the neural network with a user defined call back function to display loss and the graphs with a maximum of 300 epochs. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"numEpochs = 300\nlosses=[]\ncb() = begin\n    l=loss_adjoint(ode_data, t)\n    push!(losses, l)\n    @show l\n    pred=predict_adjoint(t)\n    pl = scatter(t,ode_data[1,:],label=\"data\", color=:black, ylim=(150,200))\n    scatter!(pl,t,pred[1,:],label=\"prediction\", color=:darkgreen)\n    display(plot(pl))\n    false\nend \n\nopt=ADAM(0.05)\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader,numEpochs), opt, cb=Flux.throttle(cb, 10))","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Finally we can see how well our trained network will generalize to new initial conditions. ","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"starting_temp=collect(10:30:250)\ntrue_prob_func(u0)=ODEProblem(true_sol, [u0], tspan)\ncolor_cycle=palette(:tab10)\npl=plot()\nfor (j,temp) in enumerate(starting_temp)\n    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0,10.0f0)), Tsit5(), saveat=0.0:0.5:10.0)\n    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0,10.0f0), θ))\n    scatter!(pl, ode_test_sol, var=(0,1), label=\"\", color=color_cycle[j])\n    plot!(pl, ode_nn_sol, var=(0,1), label=\"\", color=color_cycle[j], lw=2.0)\nend\ndisplay(pl) \ntitle!(\"Neural ODE for Newton's Law of Cooling: Test Data\")\nxlabel!(\"Time\")\nylabel!(\"Temp\")","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"We can also minibatch using tools from MLDataUtils. To do this we need to slightly change our implementation and is shown below again with a batch size of k and the same number of epochs.","category":"page"},{"location":"examples/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"using MLDataUtils\ntrain_loader, _, _ = kfolds((ode_data, t))\n\n@info \"Now training using the MLDataUtils format\"\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(eachbatch(train_loader[1], k), numEpochs), opt, cb=Flux.throttle(cb, 10))","category":"page"},{"location":"layers/BasisLayers/#Classical-Basis-Layers","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"","category":"section"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"layers/BasisLayers/#DiffEqFlux.ChebyshevBasis","page":"Classical Basis Layers","title":"DiffEqFlux.ChebyshevBasis","text":"Constructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nChebyshevBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.SinBasis","page":"Classical Basis Layers","title":"DiffEqFlux.SinBasis","text":"Constructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nSinBasis(n)\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.CosBasis","page":"Classical Basis Layers","title":"DiffEqFlux.CosBasis","text":"Constructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nCosBasis(n)\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.FourierBasis","page":"Classical Basis Layers","title":"DiffEqFlux.FourierBasis","text":"Constructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nFourierBasis(n)\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.LegendreBasis","page":"Classical Basis Layers","title":"DiffEqFlux.LegendreBasis","text":"Constructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nLegendreBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.PolynomialBasis","page":"Classical Basis Layers","title":"DiffEqFlux.PolynomialBasis","text":"Constructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nPolynomialBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"examples/optimization_ode/#Optimization-of-Ordinary-Differential-Equations","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/optimization_ode/#Copy-Paste-Code","page":"Optimization of Ordinary Differential Equations","title":"Copy-Paste Code","text":"","category":"section"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"If you want to just get things running, try the following! Explanation will follow.","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"using DifferentialEquations, DiffEqFlux, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [α, β, δ, γ]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob = ODEProblem(lotka_volterra!, u0, tspan, p)\nsol = solve(prob, Tsit5())\n\n# Plot the solution\nusing Plots\nplot(sol)\nsavefig(\"LV_ode.png\")\n\nfunction loss(p)\n  sol = solve(prob, Tsit5(), p=p, saveat = tsteps)\n  loss = sum(abs2, sol.-1)\n  return loss, sol\nend\n\ncallback = function (p, l, pred)\n  display(l)\n  plt = plot(pred, ylim = (0, 6))\n  display(plt)\n  # Tell sciml_train to not halt the optimization. If return true, then\n  # optimization stops.\n  return false\nend\n\nresult_ode = DiffEqFlux.sciml_train(loss, p,\n                                    cb = callback,\n                                    maxiters = 100)\n# result_ode = DiffEqFlux.sciml_train(loss, p, ADAM(0.1), cb = callback)","category":"page"},{"location":"examples/optimization_ode/#Explanation","page":"Optimization of Ordinary Differential Equations","title":"Explanation","text":"","category":"section"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"First let's create a Lotka-Volterra ODE using DifferentialEquations.jl. For more details, see the DifferentialEquations.jl documentation. The Lotka-Volterra equations have the form:","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"beginaligned\nfracdxdt = alpha x - beta x y      \nfracdydt = -delta y + gamma x y    \nendaligned","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"using DifferentialEquations, DiffEqFlux, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [α, β, δ, γ]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob = ODEProblem(lotka_volterra!, u0, tspan, p)\nsol = solve(prob, Tsit5())\n\n# Plot the solution\nusing Plots\nplot(sol)\nsavefig(\"LV_ode.png\")","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: LV Solution Plot)","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"For this first example, we do not yet include a neural network. We take AD-compatible solve function function that takes the parameters and an initial condition and returns the solution of the differential equation. Next we choose a loss function. Our goal will be to find parameters that make the Lotka-Volterra solution constant x(t)=1, so we define our loss as the squared distance from 1. Note that when using sciml_train, the first return is the loss value, and the other returns are sent to the callback for monitoring convergence.","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"function loss(p)\n  sol = solve(prob, Tsit5(), p=p, saveat = tsteps)\n  loss = sum(abs2, sol.-1)\n  return loss, sol\nend","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Lastly, we use the sciml_train function to train the parameters using ADAM to arrive at parameters which optimize for our goal. sciml_train allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation:","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"callback = function (p, l, pred)\n  display(l)\n  plt = plot(pred, ylim = (0, 6))\n  display(plt)\n  # Tell sciml_train to not halt the optimization. If return true, then\n  # optimization stops.\n  return false\nend","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Let's optimize the model.","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"result_ode = DiffEqFlux.sciml_train(loss, p, cb = callback)","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"In just seconds we found parameters which give a relative loss of 1e-16! We can get the final loss with result_ode.minimum, and get the optimal parameters with result_ode.u. For example, we can plot the final outcome and show that we solved the control problem and successfully found parameters to make the ODE solution constant:","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"remade_solution = solve(remake(prob, p = result_ode.u), Tsit5(),      \n                        saveat = tsteps)\nplot(remade_solution, ylim = (0, 6))","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: Final plot)","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Note that this was done with the default optimizer. One can also pass an optimization method, like ADAM(0.1), and tweak settings like set maxiters=100 to force at most 100 iterations of the optimization. This looks like:","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"result_ode = DiffEqFlux.sciml_train(loss, p, ADAM(0.1), cb = callback, maxiters=100)","category":"page"},{"location":"examples/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"For more information on tweaking this functionality, see the sciml_train documentation","category":"page"},{"location":"GPUs/#Use-with-GPUs","page":"GPUs","title":"Use with GPUs","text":"","category":"section"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus, for example, we can define a neural ODE by hand that runs on the GPU (if no GPU is available, the calculation defaults back to the CPU):","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"using DifferentialEquations, Flux, Optim, DiffEqFlux, DiffEqSensitivity\n\nmodel_gpu = Chain(Dense(2, 50, tanh), Dense(50, 2)) |> gpu\np, re = Flux.destructure(model_gpu)\ndudt!(u, p, t) = re(p)(u)\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\nu0 = Float32[2.0; 0.0] |> gpu\nprob_gpu = ODEProblem(dudt!, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"Or we could directly use the neural ODE layer function, like:","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"prob_neuralode_gpu = NeuralODE(gpu(dudt2), tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"If one is using FastChain, then the computation takes place on the GPU with f(x,p) if x and p are on the GPU. This commonly looks like:","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"dudt2 = FastChain((x,p) -> x.^3,\n            FastDense(2,50,tanh),\n            FastDense(50,2))\n\nu0 = Float32[2.; 0.] |> gpu\np = initial_params(dudt2) |> gpu\n\ndudt2_(u, p, t) = dudt2(u,p)\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\nprob_gpu = ODEProblem(dudt2_, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"or via the NeuralODE struct:","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"prob_neuralode_gpu = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nprob_neuralode_gpu(u0,p)","category":"page"},{"location":"GPUs/#Neural-ODE-Example","page":"GPUs","title":"Neural ODE Example","text":"","category":"section"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"Here is the full neural ODE example. Note that we use the gpu function so that the same code works on CPUs and GPUs, dependent on using CUDA.","category":"page"},{"location":"GPUs/","page":"GPUs","title":"GPUs","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, CUDA, DiffEqSensitivity\nCUDA.allowscalar(false) # Makes sure no slow operations are occuring\n\n# Generate Data\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Make the data into a GPU-based array if the user has a GPU\node_data = gpu(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nu0 = Float32[2.0; 0.0] |> gpu\np = initial_params(dudt2) |> gpu\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  gpu(prob_neuralode(u0,p))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, Array(ode_data[1,:]), label = \"data\")\n  scatter!(plt, tsteps, Array(pred[1,:]), label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 300)","category":"page"},{"location":"examples/second_order_neural/#Neural-Second-Order-Ordinary-Differential-Equation","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"","category":"section"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"The neural ODE focuses and finding a neural network such that:","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^prime = NN(u)","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"However, in many cases in physics-based modeling, the key object is not the velocity but the acceleration: knowing the acceleration tells you the force field and thus the generating process for the dynamical system. Thus what we want to do is find the force, i.e.:","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^primeprime = NN(u)","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"(Note that in order to be the acceleration, we should divide the output of the neural network by the mass!)","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"An example of training a neural network on a second order ODE is as follows:","category":"page"},{"location":"examples/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"using DifferentialEquations, DiffEqFlux, RecursiveArrayTools\r\n\r\nu0 = Float32[0.; 2.]\r\ndu0 = Float32[0.; 0.]\r\ntspan = (0.0f0, 1.0f0)\r\nt = range(tspan[1], tspan[2], length=20)\r\n\r\nmodel = FastChain(FastDense(2, 50, tanh), FastDense(50, 2))\r\np = initial_params(model)\r\nff(du,u,p,t) = model(u,p)\r\nprob = SecondOrderODEProblem{false}(ff, du0, u0, tspan, p)\r\n\r\nfunction predict(p)\r\n    Array(solve(prob, Tsit5(), p=p, saveat=t))\r\nend\r\n\r\ncorrect_pos = Float32.(transpose(hcat(collect(0:0.05:1)[2:end], collect(2:-0.05:1)[2:end])))\r\n\r\nfunction loss_n_ode(p)\r\n    pred = predict(p)\r\n    sum(abs2, correct_pos .- pred[1:2, :]), pred\r\nend\r\n\r\ndata = Iterators.repeated((), 1000)\r\nopt = ADAM(0.01)\r\n\r\nl1 = loss_n_ode(p)\r\n\r\ncb = function (p,l,pred)\r\n    println(l)\r\n    l < 0.01\r\nend\r\n\r\nres = DiffEqFlux.sciml_train(loss_n_ode, p, opt, cb=cb, maxiters = 1000)","category":"page"}]
}
