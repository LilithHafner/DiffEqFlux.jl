var documenterSearchIndex = {"docs":
[{"location":"layers/SplineLayer/#Spline-Layer","page":"Spline Layer","title":"Spline Layer","text":"","category":"section"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Constructs a Spline Layer. At a high-level, it performs the following:","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Takes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"SplineLayer","category":"page"},{"location":"layers/SplineLayer/#DiffEqFlux.SplineLayer","page":"Spline Layer","title":"DiffEqFlux.SplineLayer","text":"Constructs a Spline Layer. At a high-level, it performs the following:\n\nTakes as input a one-dimensional training dataset, a time span, a time step and\n\nan interpolation method.\n\nDuring training, adjusts the values of the function at multiples of the time-step\n\nsuch that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.\n\nSplineLayer(time_span,time_step,spline_basis,saved_points=nothing)\n\nArguments:\n\ntime_span: Tuple of real numbers corresponding to the time span.\ntime_step: Real number corresponding to the time step.\nspline_basis: Interpolation method to be used yb the basis (current supported interpolation methods: ConstantInterpolation, LinearInterpolation, QuadraticInterpolation, QuadraticSpline, CubicSpline).\n'saved_points': values of the function at multiples of the time step. Initialized by default\n\nto a random vector sampled from the unit normal.\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#CNF-Layer-Functions","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"DeterministicCNF\nFFJORD\nFFJORDDistribution","category":"page"},{"location":"layers/CNFLayer/#DiffEqFlux.DeterministicCNF","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.DeterministicCNF","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a direct computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\n!!!note     This layer has been deprecated in favour of FFJORD. Use FFJORD with monte_carlo=false instead.\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nDeterministicCNF(model, tspan, basedist=nothing, monte_carlo=false, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a stochastic approach [2] for the computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nFFJORD(model, basedist=nothing, monte_carlo=false, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORDDistribution","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by rand or estimate densities by pdf or logpdf (from Distributions.jl).\n\nArguments:\n\nmodel: A FFJORD instance\nregularize: Whether we use regularization (default: false)\nmonte_carlo: Whether we use monte carlo (default: true)\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#Neural-Differential-Equation-Layer-Functions","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"Constructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nNeuralODE(model,tspan,alg=nothing,args...;kwargs...)\nNeuralODE(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"Constructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nNeuralDSDE(drift,diffusion,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\nNeuralDSDE(drift::FastChain,diffusion::FastChain,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\ndrift: A Chain or FastChain neural network that defines the drift function.\ndiffusion: A Chain or FastChain neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"Constructs a neural stochastic differential equation (neural SDE).\n\nNeuralSDE(drift,diffusion,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralSDE(drift::FastChain,diffusion::FastChain,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\ndrift: A Chain or FastChain neural network that defines the drift function.\ndiffusion: A Chain or FastChain neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x,1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"Constructs a neural differential-algebraic equation (neural DAE).\n\nNeuralDAE(model,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralDAE(model::FastChain,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"Constructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nNeuralODEMM(model,constraints_model,tspan,mass_matrix,alg=nothing,args...;kwargs...)\nNeuralODEMM(model::FastChain,tspan,mass_matrix,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nmass_matrix: The mass matrix associated with the DAE\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#Helper-Layer-Functions","page":"Neural Differential Equation Layers","title":"Helper Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"DimMover\nFluxBatchOrder","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.DimMover","page":"Neural Differential Equation Layers","title":"DiffEqFlux.DimMover","text":"Constructs a Dimension Mover Layer.\n\nDimMover(from, to)\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.FluxBatchOrder","page":"Neural Differential Equation Layers","title":"DiffEqFlux.FluxBatchOrder","text":"We can have Flux's conventional order (data, channel, batch) by using it as the last layer of Flux.Chain to swap the batch-index and the time-index of the Neural DE's output. considering that each time point is a channel.\n\nFluxBatchOrder = DimMover(-2, -1)\n\n\n\n\n\n","category":"function"},{"location":"examples/tensor_layer/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"x = - kx - αx³ - βx -γx³","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in DifferentialEquations:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using DiffEqFlux, Optimization, OptimizationFlux, DifferentialEquations, LinearAlgebra\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0,10.0)\n\nfunction dxdt_train(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\nend\n\nu0 = [1.0,0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train,u0,tspan)\ndata_train = Array(solve(prob_train,Tsit5(),saveat=ts))","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we also instantiate the model we are trying to learn, “informing” the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"f = x -> min(30one(x),x)\n\nfunction dxdt_pred(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\nend\n\nα = zeros(102)\n\nprob_pred = ODEProblem{true}(dxdt_pred,u0,tspan)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Note that we introduced a “cap” in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"function predict_adjoint(θ)\n  x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts,\n                  sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  loss = sum(norm.(x - data_train))\n  return loss\nend\n\niter = 0\nfunction callback(θ,l)\n  global iter\n  iter += 1\n  if iter%10 == 0\n    println(l)\n  end\n  return false\nend","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we train the network using two rounds of ADAM:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_adjoint(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, α)\nres1 = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 150)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2, ADAM(0.001), callback = callback,maxiters = 150)\nopt = res2.u","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"We plot the results, and we obtain a fairly accurate learned model:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(res1.u)\nplot(ts, data_train[1,:], label = \"X (ODE)\")\nplot!(ts, data_train[2,:], label = \"V (ODE)\")\nplot!(ts, data_pred[1,:], label = \"X (NN)\")\nplot!(ts, data_pred[2,:],label = \"V (NN)\")","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"utilities/Collocation/#Smoothed-Collocation","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Smoothed collocation, also referred to as the two-stage method, allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true (u',u) pairs, at which point u'-f(u,p,t) can be directly estimated as a loss to determine the correct parameters p. This method can be extremely fast and robust to noise, though, because it does not accumulate through time, is not as exact as other methods.","category":"page"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"collocate_data","category":"page"},{"location":"utilities/Collocation/#DiffEqFlux.collocate_data","page":"Smoothed Collocation","title":"DiffEqFlux.collocate_data","text":"u′,u = collocate_data(data,tpoints,kernel=SigmoidKernel())\nu′,u = collocate_data(data,tpoints,tpoints_sample,interp,args...)\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nFor kernels, the following exist:\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/\n\nAdditionally, we can use interpolation methods from DataInterpolations.jl to generate data from intermediate timesteps. In this case, pass any of the methods like QuadraticInterpolation as interp, and the timestamps to sample from as tpoints_sample.\n\n\n\n\n\n","category":"function"},{"location":"utilities/Collocation/#Kernel-Choice","page":"Smoothed Collocation","title":"Kernel Choice","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Note that the kernel choices of DataInterpolations.jl, such as CubicSpline(), are exact, i.e. go through the data points, while the smoothed kernels are regression splines. Thus CubicSpline() is preferred if the data is not too noisy or is relatively sparse. If data is sparse and very noisy, a BSpline()  can be the best regression spline, otherwise one of the other kernels such as as EpanechnikovKernel.","category":"page"},{"location":"utilities/Collocation/#Non-Allocating-Forward-Mode-L2-Collocation-Loss","page":"Smoothed Collocation","title":"Non-Allocating Forward-Mode L2 Collocation Loss","text":"","category":"section"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation:","category":"page"},{"location":"utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"using PreallocationTools\ndu = PreallocationTools.dualcache(similar(prob.u0))\npreview_est_sol = [@view estimated_solution[:,i] for i in 1:size(estimated_solution,2)]\npreview_est_deriv = [@view estimated_derivative[:,i] for i in 1:size(estimated_solution,2)]\n\nfunction construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)\n  function (p)\n      _du = PreallocationTools.get_tmp(du,p)\n      vecdu = vec(_du)\n      cost = zero(first(p))\n      for i in 1:length(preview_est_sol)\n        est_sol = preview_est_sol[i]\n        f(_du,est_sol,p,tpoints[i])\n        vecdu .= vec(preview_est_deriv[i]) .- vec(_du)\n        cost += sum(abs2,vecdu)\n      end\n      sqrt(cost)\n  end\nend\ncost_function = construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)","category":"page"},{"location":"examples/multiple_shooting/#Multiple-Shooting","page":"Multiple Shooting","title":"Multiple Shooting","text":"","category":"section"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"In Multiple Shooting, the training data is split into overlapping intervals. The solver is then trained on individual intervals. If the end conditions of any interval coincide with the initial conditions of the next immediate interval, then the joined/combined solution is the same as solving on the whole dataset (without splitting).","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"To ensure that the overlapping part of two consecutive intervals coincide, we add a penalizing term, continuity_term * absolute_value_of(prediction of last point of group i - prediction of first point of group i+1), to the loss.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Note that the continuity_term should have a large positive value to add high penalties in case the solver predicts discontinuous values.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The following is a working demo, using Multiple Shooting","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"using Lux, DiffEqFlux, Optimization, OptimizationPolyalgorithms, DifferentialEquations, Plots\nusing DiffEqFlux: group_ranges\n\nusing Random\nrng = Random.default_rng()\n\n# Define initial conditions and time steps\ndatasize = 30\nu0 = Float32[2.0, 0.0]\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\n\n# Get the data\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\n# Define the Neural Network\nnn = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 16, tanh),\n                  Lux.Dense(16, 2))\np_init, st = Lux.setup(rng, nn)\n\nneuralode = NeuralODE(nn, tspan, Tsit5(), saveat = tsteps)\nprob_node = ODEProblem((u,p,t)->nn(u,p,st)[1], u0, tspan, Lux.ComponentArray(p_init))\n\n\nfunction plot_multiple_shoot(plt, preds, group_size)\n\tstep = group_size-1\n\tranges = group_ranges(datasize, group_size)\n\n\tfor (i, rg) in enumerate(ranges)\n\t\tplot!(plt, tsteps[rg], preds[i][1,:], markershape=:circle, label=\"Group $(i)\")\n\tend\nend\n\n# Animate training, cannot make animation on CI server\n# anim = Plots.Animation()\niter = 0\ncallback = function (p, l, preds; doplot = false)\n  display(l)\n  global iter\n  iter += 1\n  if doplot && iter%1 == 0\n    # plot the original data\n    plt = scatter(tsteps, ode_data[1,:], label = \"Data\")\n\n    # plot the different predictions for individual shoot\n    plot_multiple_shoot(plt, preds, group_size)\n\n    frame(anim)\n    display(plot(plt))\n  end\n  return false\nend\n\n# Define parameters for Multiple Shooting\ngroup_size = 3\ncontinuity_term = 200\n\nfunction loss_function(data, pred)\n\treturn sum(abs2, data - pred)\nend\n\nfunction loss_multiple_shooting(p)\n    return multiple_shoot(p, ode_data, tsteps, prob_node, loss_function, Tsit5(),\n                          group_size; continuity_term)\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_multiple_shooting(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(p_init))\nres_ms = Optimization.solve(optprob, PolyOpt(),\n                                callback = callback)\n#gif(anim, \"multiple_shooting.gif\", fps=15)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here's the animation that we get from above when doplot=true and the animation code is uncommented:","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic) The connected lines show the predictions of each group (Notice that there are overlapping points as well. These are the points we are trying to coincide.)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here is an output with group_size = 30 (which is the same as solving on the whole interval without splitting also called single shooting)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic_single_shoot3)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"It is clear from the above picture, a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minimum.","category":"page"},{"location":"examples/neural_ode/#Neural-Ordinary-Differential-Equations","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"A neural ODE is an ODE where a neural network defines its derivative function. For example, with the multilayer perceptron neural network Lux.Chain(Lux.Dense(2, 50, tanh), Lux.Dense(50, 2)), we can define a differential equation which is u' = NN(u). This is done simply by the NeuralODE struct. Let's take a look at an example.","category":"page"},{"location":"examples/neural_ode/#Copy-Pasteable-Code","page":"Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using Lux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationOptimJL, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\n# Do not plot by default for the documentation\n# Users should change doplot=true to see the plots callbacks\ncallback = function (p, l, pred; doplot = false)\n  println(l)\n  # plot current prediction against data\n  if doplot\n    plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n    scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n    display(plot(plt))\n  end\n  return false\nend\n\npinit = Lux.ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...; doplot=true)\n\n# use Optimization.jl to solve the problem\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n                                        Optim.BFGS(initial_stepnorm=0.01),\n                                        callback=callback,\n                                        allow_f_increases = false)\n\ncallback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot=true)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"(Image: Neural ODE)","category":"page"},{"location":"examples/neural_ode/#Explanation","page":"Neural Ordinary Differential Equations","title":"Explanation","text":"","category":"section"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Let's get a time series array from a spiral ODE to train against.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using Lux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationOptimJL, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Now let's define a neural network with a NeuralODE layer. First, we define the layer. Here we're going to use Lux.Chain, which is a suitable neural network structure for NeuralODEs with separate handling of state variables:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Note that we can directly use Chains from Flux.jl as well, for example:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Chain(x -> x.^3,\n              Dense(2, 50, tanh),\n              Dense(50, 2))","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"In our model, we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters, which we will use to change the neural network iteratively in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We define a callback function. In this example, we set doplot = false because otherwise it would show every step and overflow the documentation, but for your use case set doplot=true to see a live animation of the training process!","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Callback function to observe training\ncallback = function (p, l, pred; doplot = false)\n  println(l)\n  # plot current prediction against data\n  if doplot\n    plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n    scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n    display(plot(plt))\n  end\n  return false\nend\n\npinit = Lux.ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then train the neural network to learn the ODE.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we can fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting). You can easily incorporate the procedure below to set up custom optimization problems. For more information on the usage of Optimization.jl, please consult this documentation.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"The x and p variables in the optimization function are different from x and p above. The optimization function runs over the space of parameters of the original problem, so x_optimization == p_original.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Train using the ADAM optimizer\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then complete the training using a different optimizer, starting from where ADAM stopped. We do allow_f_increases=false to make the optimization automatically halt when near the minimum.","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Retrain using the LBFGS optimizer\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n                                        Optim.BFGS(initial_stepnorm=0.01),\n                                        callback = callback,\n                                        allow_f_increases = false)","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"And then we use the callback with doplot=true to see the final plot:","category":"page"},{"location":"examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"callback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot=true)","category":"page"},{"location":"examples/GPUs/#Neural-ODEs-on-GPUs","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"","category":"section"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus, for example, we can define a neural ODE manually that runs on the GPU (if no GPU is available, the calculation defaults back to the CPU):","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using DifferentialEquations, Lux, SciMLSensitivity, ComponentArrays\nusing Random\nrng = Random.default_rng()\n\nmodel = Chain(Dense(2, 50, tanh), Dense(50, 2))\nps, st = Lux.setup(rng,model) \nps = ps |> ComponentArray |> gpu\nst = st |> gpu\ndudt(u, p, t) = model(u, p, st)[1]\n\n# Simulation interval and intermediary points\ntspan = (0f0, 10f0)\ntsteps = 0f0:1f-1:10f0\n\nu0 = Float32[2.0; 0.0] |> gpu\nprob_gpu = ODEProblem(dudt, u0, tspan, ps)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Or we could directly use the neural ODE layer function, like:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using DiffEqFlux: NeuralODE\nprob_neuralode_gpu = NeuralODE(model, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"If one is using Lux.Chain, then the computation takes place on the GPU with f(x,p,st) if x, p and st are on the GPU. This commonly looks like:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"import Lux\n\ndudt2 = Lux.Chain(x -> x.^3,\n            Lux.Dense(2,50,tanh),\n            Lux.Dense(50,2))\n\nu0 = Float32[2.; 0.] |> gpu\np, st = Lux.setup(rng, dudt2) |> gpu\n\ndudt2_(u, p, t) = dudt2(u,p,st)[1]\n\n# Simulation interval and intermediary points\ntspan = (0f0, 10f0)\ntsteps = 0f0:1f-1:10f0\n\nprob_gpu = ODEProblem(dudt2_, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"or via the NeuralODE struct:","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"prob_neuralode_gpu = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nprob_neuralode_gpu(u0,p,st)","category":"page"},{"location":"examples/GPUs/#Neural-ODE-Example","page":"Neural ODEs on GPUs","title":"Neural ODE Example","text":"","category":"section"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Here is the full neural ODE example. Note that we use the gpu function so that the same code works on CPUs and GPUs, dependent on using CUDA.","category":"page"},{"location":"examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using Lux, Optimization, OptimizationOptimisers, Zygote, OrdinaryDiffEq, \n      Plots, CUDA, SciMLSensitivity, Random, ComponentArrays\nimport DiffEqFlux: NeuralODE \n\nCUDA.allowscalar(false) # Makes sure no slow operations are occuring\n\n#rng for Lux.setup\nrng = Random.default_rng()\n# Generate Data\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Make the data into a GPU-based array if the user has a GPU\node_data = gpu(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\ndudt2 = Chain(x -> x.^3, Dense(2, 50, tanh), Dense(50, 2))\nu0 = Float32[2.0; 0.0] |> gpu\np, st = Lux.setup(rng, dudt2) \np = p |> ComponentArray |> gpu\nst = st |> gpu\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  gpu(first(prob_neuralode(u0,p,st)))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, Array(ode_data[1,:]), label = \"data\")\n  scatter!(plt, tsteps, Array(pred[1,:]), label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\nresult_neuralode = Optimization.solve(optprob,Adam(0.05),callback = callback,maxiters = 300)\n","category":"page"},{"location":"examples/normalizing_flows/#Continuous-Normalizing-Flows","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"examples/normalizing_flows/#Copy-Pasteable-Code","page":"Continuous Normalizing Flows","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Flux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationFlux,\n      OptimizationOptimJL, Distributions\n\nnn = Flux.Chain(\n    Flux.Dense(1, 3, tanh),\n    Flux.Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, Tsit5())\n\n# Training\ndata_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))\n\nfunction loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend\n\nfunction cb(p, l)\n    @info \"Training\" loss = loss(p)\n    false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ffjord_mdl.p)\n\nres1 = Optimization.solve(optprob,\n                          ADAM(0.1),\n                          maxiters = 100,\n                          callback=cb)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2,\n                          Optim.LBFGS(),\n                          allow_f_increases=false,\n                          callback=cb)\n\n# Evaluation\nusing Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u, monte_carlo=false)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)\n\n# Data Generation\nffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#Step-by-Step-Explanation","page":"Continuous Normalizing Flows","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Flux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationFlux,\n      OptimizationOptimJL, Distributions\n\nnn = Flux.Chain(\n    Flux.Dense(1, 3, tanh),\n    Flux.Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, Tsit5())","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"examples/normalizing_flows/#Training","page":"Continuous Normalizing Flows","title":"Training","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"First, let's get an array from a normal distribution as the training data. Note that we want the data in Float32 values to match how we have set up the neural network weights and the state space of the ODE.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"data_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now we define a loss function that we wish to minimize and a callback function to track loss improvements","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"function loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend\n\nfunction cb(p, l)\n    @info \"Training\" loss = loss(p)\n    false\nend","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ffjord_mdl.p)\n\nres1 = Optimization.solve(optprob,\n                          ADAM(0.1),\n                          maxiters = 100,\n                          callback=cb)","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then complete the training using a different optimizer, starting from where ADAM stopped.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"optprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2,\n                          Optim.LBFGS(),\n                          allow_f_increases=false,\n                          callback=cb)","category":"page"},{"location":"examples/normalizing_flows/#Evaluation","page":"Continuous Normalizing Flows","title":"Evaluation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"For evaluating the result, we can use totalvariation function from Distances.jl. First, we compute densities using actual distribution and FFJORD model. Then we use a distance function between these distributions.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u, monte_carlo=false)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)","category":"page"},{"location":"examples/normalizing_flows/#Data-Generation","page":"Continuous Normalizing Flows","title":"Data Generation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"What's more, we can generate new data by using FFJORD as a distribution in rand.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"ffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#References","page":"Continuous Normalizing Flows","title":"References","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"[1] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).","category":"page"},{"location":"examples/mnist_neural_ode/#mnist","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Training a classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with minibatching.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs)\n\t# Use MLDataUtils LabelEnc for natural onehot conversion\n  \tonehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n\t# Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n\t# Process images into (H,W,C,BS) batches\n\tx_train = Float32.(reshape(imgs,size(imgs,1),size(imgs,2),1,size(imgs,3))) |> gpu\n\tx_train = batchview(x_train,batchsize)\n\t# Onehot and batch the labels\n\ty_train = onehot(labels_raw) |> gpu\n\ty_train = batchview(y_train,batchsize)\n\treturn x_train, y_train\nend\n\n# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)\n\ndown = Flux.Chain(Flux.flatten, Flux.Dense(784, 20, tanh)) |> gpu\n\nnn = Flux.Chain(Flux.Dense(20, 10, tanh),\n           Flux.Dense(10, 10, tanh),\n           Flux.Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Flux.Chain(Flux.Dense(20, 10)) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend\n\n# Build our overall model topology\nmodel = Flux.Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncallback() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = callback)","category":"page"},{"location":"examples/mnist_neural_ode/#Step-by-Step-Description","page":"GPU-based MNIST Neural ODE Classifier","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_neural_ode/#Load-Packages","page":"GPU-based MNIST Neural ODE Classifier","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets","category":"page"},{"location":"examples/mnist_neural_ode/#GPU","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fall back to the CPU.","category":"page"},{"location":"examples/mnist_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"GPU-based MNIST Neural ODE Classifier","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function loadmnist(batchsize = bs)\n\t# Use MLDataUtils LabelEnc for natural onehot conversion\n  \tonehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n\t# Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n\t# Process images into (H,W,C,BS) batches\n\tx_train = Float32.(reshape(imgs,size(imgs,1),size(imgs,2),1,size(imgs,3))) |> gpu\n\tx_train = batchview(x_train,batchsize)\n\t# Onehot and batch the labels\n\ty_train = onehot(labels_raw) |> gpu\n\ty_train = batchview(y_train,batchsize)\n\treturn x_train, y_train\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)","category":"page"},{"location":"examples/mnist_neural_ode/#Layers","page":"GPU-based MNIST Neural ODE Classifier","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from the previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down = Flux.Chain(Flux.flatten, Flux.Dense(784, 20, tanh)) |> gpu\n\nnn = Flux.Chain(Flux.Dense(20, 10, tanh),\n           Flux.Dense(10, 10, tanh),\n           Flux.Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Flux.Chain(Flux.Dense(20, 10)) |> gpu","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down: This layer downsamples our images into a 20 dimensional feature vector.         It takes a 28 x 28 image, flattens it, and then passes it through a fully connected         layer with tanh activation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn: A 3 layers Deep Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"fc: The final fully connected layer which maps our learned feature vector to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"|> gpu: An utility function which transfers our model to GPU, if it is available","category":"page"},{"location":"examples/mnist_neural_ode/#Array-Conversion","page":"GPU-based MNIST Neural ODE Classifier","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"When using NeuralODE, this function converts the ODESolution's DiffEqArray to a Matrix (CuArray), and reduces the matrix from 3 to 2 dimensions for use in the next layer.","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"For CPU: If this function does not automatically fall back to CPU when no GPU is present, we can change gpu(x) to Array(x).","category":"page"},{"location":"examples/mnist_neural_ode/#Build-Topology","page":"GPU-based MNIST Neural ODE Classifier","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Next, we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Build our overall model topology\nmodel = Flux.Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Flux.Chain(down,\n                 nn,\n                 fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"examples/mnist_neural_ode/#Prediction","page":"GPU-based MNIST Neural ODE Classifier","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_neural_ode/#Accuracy","page":"GPU-based MNIST Neural ODE Classifier","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(m, train_dataloader)","category":"page"},{"location":"examples/mnist_neural_ode/#Training-Parameters","page":"GPU-based MNIST Neural ODE Classifier","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_neural_ode/#Loss","page":"GPU-based MNIST Neural ODE Classifier","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Cross Entropy is the loss function computed here, which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"examples/mnist_neural_ode/#Optimizer","page":"GPU-based MNIST Neural ODE Classifier","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"opt = ADAM(0.05)","category":"page"},{"location":"examples/mnist_neural_ode/#CallBack","page":"GPU-based MNIST Neural ODE Classifier","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"callback() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"examples/mnist_neural_ode/#Train","page":"GPU-based MNIST Neural ODE Classifier","title":"Train","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params( down, nn_ode.p, fc), zip( x_train, y_train ), opt, callback = callback)","category":"page"},{"location":"examples/mnist_neural_ode/#Expected-Output","page":"GPU-based MNIST Neural ODE Classifier","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Iter:   1 || Train Accuracy: 16.203 || Test Accuracy: 16.933\nIter:  11 || Train Accuracy: 64.406 || Test Accuracy: 64.900\nIter:  21 || Train Accuracy: 76.656 || Test Accuracy: 76.667\nIter:  31 || Train Accuracy: 81.758 || Test Accuracy: 81.683\nIter:  41 || Train Accuracy: 81.078 || Test Accuracy: 81.967\nIter:  51 || Train Accuracy: 83.953 || Test Accuracy: 84.417\nIter:  61 || Train Accuracy: 85.266 || Test Accuracy: 85.017\nIter:  71 || Train Accuracy: 85.938 || Test Accuracy: 86.400\nIter:  81 || Train Accuracy: 84.836 || Test Accuracy: 85.533\nIter:  91 || Train Accuracy: 86.148 || Test Accuracy: 86.583\nIter: 101 || Train Accuracy: 83.859 || Test Accuracy: 84.500\nIter: 111 || Train Accuracy: 86.227 || Test Accuracy: 86.617\nIter: 121 || Train Accuracy: 87.508 || Test Accuracy: 87.200\nIter: 131 || Train Accuracy: 86.227 || Test Accuracy: 85.917\nIter: 141 || Train Accuracy: 84.453 || Test Accuracy: 84.850\nIter: 151 || Train Accuracy: 86.063 || Test Accuracy: 85.650\nIter: 161 || Train Accuracy: 88.375 || Test Accuracy: 88.033\nIter: 171 || Train Accuracy: 87.398 || Test Accuracy: 87.683\nIter: 181 || Train Accuracy: 88.070 || Test Accuracy: 88.350\nIter: 191 || Train Accuracy: 86.836 || Test Accuracy: 87.150\nIter: 201 || Train Accuracy: 89.266 || Test Accuracy: 88.583\nIter: 211 || Train Accuracy: 86.633 || Test Accuracy: 85.550\nIter: 221 || Train Accuracy: 89.313 || Test Accuracy: 88.217\nIter: 231 || Train Accuracy: 88.641 || Test Accuracy: 89.417\nIter: 241 || Train Accuracy: 88.617 || Test Accuracy: 88.550\nIter: 251 || Train Accuracy: 88.211 || Test Accuracy: 87.950\nIter: 261 || Train Accuracy: 87.742 || Test Accuracy: 87.317\nIter: 271 || Train Accuracy: 89.070 || Test Accuracy: 89.217\nIter: 281 || Train Accuracy: 89.703 || Test Accuracy: 89.067\nIter: 291 || Train Accuracy: 88.484 || Test Accuracy: 88.250\nIter: 301 || Train Accuracy: 87.898 || Test Accuracy: 88.367\nIter: 311 || Train Accuracy: 88.438 || Test Accuracy: 88.633\nIter: 321 || Train Accuracy: 88.664 || Test Accuracy: 88.567\nIter: 331 || Train Accuracy: 89.906 || Test Accuracy: 89.883\nIter: 341 || Train Accuracy: 88.883 || Test Accuracy: 88.667\nIter: 351 || Train Accuracy: 89.609 || Test Accuracy: 89.283\nIter: 361 || Train Accuracy: 89.516 || Test Accuracy: 89.117\nIter: 371 || Train Accuracy: 89.898 || Test Accuracy: 89.633\nIter: 381 || Train Accuracy: 89.055 || Test Accuracy: 89.017\nIter: 391 || Train Accuracy: 89.445 || Test Accuracy: 89.467\nIter: 401 || Train Accuracy: 89.156 || Test Accuracy: 88.250\nIter: 411 || Train Accuracy: 88.977 || Test Accuracy: 89.083\nIter: 421 || Train Accuracy: 90.109 || Test Accuracy: 89.417","category":"page"},{"location":"examples/neural_sde/#Neural-Stochastic-Differential-Equations-With-Method-of-Moments","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"","category":"section"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"With neural stochastic differential equations, there is once again a helper form neural_dmsde which can be used for the multiplicative noise case (consult the layers API documentation, or this full example using the layer function).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"However, since there are far too many possible combinations for the API to support, often you will want to define neural differential equations for non-ODE systems from scratch. To get good performance for these systems, it is generally best to use TrackerAdjoint with non-mutating (out-of-place) forms. For example, the following defines a neural SDE with neural networks for both the drift and diffusion terms:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt(u, p, t) = model(u)\ng(u, p, t) = model2(u)\nprob = SDEProblem(dudt, g, x, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"where model and model2 are different neural networks. The same can apply to a neural delay differential equation. Its out-of-place formulation is f(u,h,p,t). Thus, for example, if we want to define a neural delay differential equation which uses the history value at p.tau in the past, we can define:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt!(u, h, p, t) = model([u; h(t - p.tau)])\nprob = DDEProblem(dudt_, u0, h, tspan, nothing)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"First, let's build training data from the same example as the neural ODE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"using Plots, Statistics\nusing Flux, Optimization, OptimizationFlux, DiffEqFlux, StochasticDiffEq, SciMLBase.EnsembleAnalysis\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0, 1.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"function trueSDEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nmp = Float32[0.2, 0.2]\nfunction true_noise_func(du, u, p, t)\n    du .= mp.*u\nend\n\nprob_truesde = SDEProblem(trueSDEfunc, true_noise_func, u0, tspan)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"For our dataset, we will use DifferentialEquations.jl's parallel ensemble interface to generate data from the average of 10,000 runs of the SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Take a typical sample from the mean\nensemble_prob = EnsembleProblem(prob_truesde)\nensemble_sol = solve(ensemble_prob, SOSRI(), trajectories = 10000)\nensemble_sum = EnsembleSummary(ensemble_sol)\n\nsde_data, sde_data_vars = Array.(timeseries_point_meanvar(ensemble_sol, tsteps))","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we build a neural SDE. For simplicity, we will use the NeuralDSDE neural SDE with diagonal noise layer function:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"drift_dudt = Flux.Chain(x -> x.^3,\n                       Flux.Dense(2, 50, tanh),\n                       Flux.Dense(50, 2))\np1, re1 = Flux.destructure(drift_dudt)\n\ndiffusion_dudt = Flux.Chain(Flux.Dense(2, 2))\np2, re2 = Flux.destructure(diffusion_dudt)\n\nneuralsde = NeuralDSDE(drift_dudt, diffusion_dudt, tspan, SOSRI(),\n                       saveat = tsteps, reltol = 1e-1, abstol = 1e-1)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Let's see what that looks like:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Get the prediction using the correct initial condition\nprediction0 = neuralsde(u0)\n\ndrift_(u, p, t) = re1(p[1:neuralsde.len])(u)\ndiffusion_(u, p, t) = re2(p[neuralsde.len+1:end])(u)\n\nprob_neuralsde = SDEProblem(drift_, diffusion_, u0,(0.0f0, 1.2f0), neuralsde.p)\n\nensemble_nprob = EnsembleProblem(prob_neuralsde)\nensemble_nsol = solve(ensemble_nprob, SOSRI(), trajectories = 100,\n                      saveat = tsteps)\nensemble_nsum = EnsembleSummary(ensemble_nsol)\n\nplt1 = plot(ensemble_nsum, title = \"Neural SDE: Before Training\")\nscatter!(plt1, tsteps, sde_data', lw = 3)\n\nscatter(tsteps, sde_data[1,:], label = \"data\")\nscatter!(tsteps, prediction0[1,:], label = \"prediction\")","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now just as with the neural ODE we define a loss function that calculates the mean and variance from n runs at each time point and uses the distance from the data values:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"function predict_neuralsde(p, u = u0)\n  return Array(neuralsde(u, p))\nend\n\nfunction loss_neuralsde(p; n = 100)\n  u = repeat(reshape(u0, :, 1), 1, n)\n  samples = predict_neuralsde(p, u)\n  means = mean(samples, dims = 2)\n  vars = var(samples, dims = 2, mean = means)[:, 1, :]\n  means = means[:, 1, :]\n  loss = sum(abs2, sde_data - means) + sum(abs2, sde_data_vars - vars)\n  return loss, means, vars\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"list_plots = []\niter = 0\n\n# Callback function to observe training\ncallback = function (p, loss, means, vars; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  # loss against current data\n  display(loss)\n\n  # plot current prediction against data\n  plt = Plots.scatter(tsteps, sde_data[1,:], yerror = sde_data_vars[1,:],\n                     ylim = (-4.0, 8.0), label = \"data\")\n  Plots.scatter!(plt, tsteps, means[1,:], ribbon = vars[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n\n  if doplot\n    display(plt)\n  end\n  return false\nend","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we train using this loss function. We can pre-train a little bit using a smaller n and then decrease it after it has had some time to adjust towards the right mean behavior:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"opt = ADAM(0.025)\n\n# First round of training with n = 10\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_neuralsde(x, n=10), adtype)\noptprob = Optimization.OptimizationProblem(optf, neuralsde.p)\nresult1 = Optimization.solve(optprob, opt,\n                                 callback = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"We resume the training with a larger n. (WARNING - this step is a couple of orders of magnitude longer than the previous one).","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"optf2 = Optimization.OptimizationFunction((x,p) -> loss_neuralsde(x, n=100), adtype)\noptprob2 = Optimization.OptimizationProblem(optf2, result1.u)\nresult2 = Optimization.solve(optprob2, opt,\n                                 callback = callback, maxiters = 100)","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"And now we plot the solution to an ensemble of the trained neural SDE:","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"_, means, vars = loss_neuralsde(result2.u, n = 1000)\n\nplt2 = Plots.scatter(tsteps, sde_data', yerror = sde_data_vars',\n                     label = \"data\", title = \"Neural SDE: After Training\",\n                     xlabel = \"Time\")\nplot!(plt2, tsteps, means', lw = 8, ribbon = vars', label = \"prediction\")\n\nplt = plot(plt1, plt2, layout = (2, 1))\nsavefig(plt, \"NN_sde_combined.png\"); nothing # sde","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"(Image: )","category":"page"},{"location":"examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Try this with GPUs as well!","category":"page"},{"location":"examples/collocation/#Smoothed-Collocation-for-Fast-Two-Stage-Training","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data. First the example and then an explanation.","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using Lux, DiffEqFlux, OrdinaryDiffEq, SciMLSensitivity, Optimization, OptimizationFlux, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)\nsavefig(\"colloc.png\")\nplot(tsteps,du')\nsavefig(\"colloc_du.png\")\n\ndudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du, _ = dudt2(@view(u[:,i]),p, st)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol)\nsavefig(\"colloc_trained.png\")\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)","category":"page"},{"location":"examples/collocation/#Generating-the-Collocation","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Generating the Collocation","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"The smoothed collocation is a spline fit of the data points which allows us to get an estimate of the approximate noiseless dynamics:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using Lux, DiffEqFlux, Optimization, OptimizationFlux, DifferentialEquations, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"We can then differentiate the smoothed function to get estimates of the derivative at each data point:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"plot(tsteps,du')","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Because we have (u',u) pairs, we can write a loss function that calculates the squared difference between f(u,p,t) and u' at each point, and find the parameters which minimize this difference:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"dudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du, _ = dudt2(@view(u[:,i]),p, st)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"While this doesn't look great, it has the characteristics of the full solution all throughout the timeseries, but it does have a drift. We can continue to optimize like this, or we can use this as the initial condition to the next phase of our fitting:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"This method then has a good global starting position, making it less prone to local minima, and this method is thus a great method to mix in with other fitting methods for neural ODEs.","category":"page"},{"location":"examples/hamiltonian_nn/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Networks introduced in [1] allow models to \"learn and respect exact conservation laws in an unsupervised manner\". In this example, we will train a model to learn the Hamiltonian for a 1D Spring mass system. This system is described by the equation:","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"mddot x + kx = 0","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Now we make some simplifying assumptions, and assign m = 1 and k = 1. Analytically solving this equation, we get x = sin(t). Hence, q = sin(t), and p = cos(t). Using these solutions, we generate our dataset and fit the NeuralHamiltonianDE to learn the dynamics of this system.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Flux, DiffEqFlux, DifferentialEquations, Statistics, Plots, ReverseDiff\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader((data, target); batchsize=256, shuffle=true)\n\nhnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()\n\nmodel = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/#Step-by-Step-Explanation","page":"Hamiltonian Neural Network","title":"Step by Step Explanation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/#Data-Generation","page":"Hamiltonian Neural Network","title":"Data Generation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"The HNN predicts the gradients (dot q dot p) given (q p). Hence, we generate the pairs (q p) using the equations given at the top. Additionally, to supervise the training, we also generate the gradients. Next, we use Flux DataLoader for automatically batching our dataset.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Flux, DiffEqFlux, DifferentialEquations, Statistics, Plots, ReverseDiff\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader((data, target); batchsize=256, shuffle=true)","category":"page"},{"location":"examples/hamiltonian_nn/#Training-the-HamiltonianNN","page":"Hamiltonian Neural Network","title":"Training the HamiltonianNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We parameterize the HamiltonianNN with a small MultiLayered Perceptron (HNN also works with the Fast* Layers provided in DiffEqFlux). HNNs are trained by optimizing the gradients of the Neural Network. Zygote currently doesn't support nesting itself, so we will be using ReverseDiff in the training loop to compute the gradients of the HNN Layer for Optimization.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"hnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()","category":"page"},{"location":"examples/hamiltonian_nn/#Solving-the-ODE-using-trained-HNN","page":"Hamiltonian Neural Network","title":"Solving the ODE using trained HNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In order to visualize the learned trajectories, we need to solve the ODE. We will use the NeuralHamiltonianDE layer, which is essentially a wrapper over HamiltonianNN layer, and solves the ODE.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"model = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"(Image: HNN Prediction)","category":"page"},{"location":"examples/hamiltonian_nn/#Expected-Output","page":"Hamiltonian Neural Network","title":"Expected Output","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Loss Neural Hamiltonian DE = 18.768814\nLoss Neural Hamiltonian DE = 0.022630047\nLoss Neural Hamiltonian DE = 0.015060622\nLoss Neural Hamiltonian DE = 0.013170851\nLoss Neural Hamiltonian DE = 0.011898238\nLoss Neural Hamiltonian DE = 0.009806873","category":"page"},{"location":"examples/hamiltonian_nn/#References","page":"Hamiltonian Neural Network","title":"References","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.","category":"page"},{"location":"layers/HamiltonianNN/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system.","category":"page"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"HamiltonianNN\nNeuralHamiltonianDE","category":"page"},{"location":"layers/HamiltonianNN/#DiffEqFlux.HamiltonianNN","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.HamiltonianNN","text":"Constructs a Hamiltonian Neural Network [1]. This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories. It takes as input a concatenated vector of length 2n containing the position (of size n) and momentum (of size n) of the particles. It then returns the time derivatives for position and momentum.\n\nnote: Note\nThis doesn't solve the Hamiltonian Problem. Use NeuralHamiltonianDE for such applications.\n\nnote: Note\nThis layer currently doesn't support GPU. The support will be added in future with some AD fixes.\n\nTo obtain the gradients to train this network, ReverseDiff.gradient is supposed to be used. This prevents the usage of DiffEqFlux.sciml_train or Flux.train. Follow this tutorial to see how to define a training loop to circumvent this issue.\n\nHamiltonianNN(model; p = nothing)\nHamiltonianNN(model::FastChain; p = initial_params(model))\n\nArguments:\n\nmodel: A Chain or FastChain neural network that returns the Hamiltonian of the          system.\np: The initial parameters of the neural network.\n\nReferences:\n\n[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.\n\n\n\n\n\n","category":"type"},{"location":"layers/HamiltonianNN/#DiffEqFlux.NeuralHamiltonianDE","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.NeuralHamiltonianDE","text":"Contructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network HamiltonianNN.\n\nNeuralHamiltonianDE(model, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain, FastChain or Hamiltonian Neural Network that predicts the          Hamiltonian of the system.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the           Common Solver Arguments           documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#Classical-Basis-Layers","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"","category":"section"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"layers/BasisLayers/#DiffEqFlux.ChebyshevBasis","page":"Classical Basis Layers","title":"DiffEqFlux.ChebyshevBasis","text":"Constructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nChebyshevBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.SinBasis","page":"Classical Basis Layers","title":"DiffEqFlux.SinBasis","text":"Constructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nSinBasis(n)\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.CosBasis","page":"Classical Basis Layers","title":"DiffEqFlux.CosBasis","text":"Constructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nCosBasis(n)\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.FourierBasis","page":"Classical Basis Layers","title":"DiffEqFlux.FourierBasis","text":"Constructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nFourierBasis(n)\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.LegendreBasis","page":"Classical Basis Layers","title":"DiffEqFlux.LegendreBasis","text":"Constructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nLegendreBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.PolynomialBasis","page":"Classical Basis Layers","title":"DiffEqFlux.PolynomialBasis","text":"Constructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nPolynomialBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Copy-Pasteable-Code","page":"Augmented Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend\n\ndiffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    return Chain((x, p=node.p) -> node(x, p),\n                 Array,\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend\n\nfunction plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend\n\nloss_node(x, y) = mean((model(x) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)\n\niter = 0\ncb = function()\n    global iter \n    iter += 1\n    if iter % 10 == 0\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend\n\nmodel, parameters = construct_model(1, 2, 64, 0)\nopt = ADAM(0.005)\n\nprintln(\"Training Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(parameters, model), dataloader, opt, cb = cb)\nend\n\nplt_node = plot_contour(model)\n\nmodel, parameters = construct_model(1, 2, 64, 1)\nopt = ADAM(0.005)\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(parameters, model), dataloader, opt, cb = cb)\nend\n\nplt_anode = plot_contour(model)","category":"page"},{"location":"examples/augmented_neural_ode/#Step-by-Step-Explanation","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loading-required-packages","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader","category":"page"},{"location":"examples/augmented_neural_ode/#Generating-a-toy-dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Models","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tutorial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on GPU, we need to manually transfer the models to GPU. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : (AugmentedNDELayer(node, augment_dim) |> gpu)\n    return Chain((x, p=node.p) -> node(x, p),\n                 Array,\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Plotting-the-Results","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define a utility to plot our model regression results as a heatmap.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Training-Parameters","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loss-Functions","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(x, y) = mean((model(x) .- y) .^ 2)","category":"page"},{"location":"examples/augmented_neural_ode/#Dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)","category":"page"},{"location":"examples/augmented_neural_ode/#Callback-Function","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally, we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"iter = 0\ncb = function()\n    global iter += 1\n    if iter % 10 == 1\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Optimizer","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use ADAM as the optimizer with a learning rate of 0.005","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = ADAM(0.005)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which are returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 0)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(model, parameters), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be the same as that of Neural ODE. Only in this case, we have augmented the input with a single zero. This makes the problem 3-dimensional, and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs, please refer to [1].","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 1)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(model, parameters), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"examples/augmented_neural_ode/#Expected-Output","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\nTraining Neural ODE\nIteration 10 || Loss = 0.9802582\nIteration 20 || Loss = 0.6727416\nIteration 30 || Loss = 0.5862373\nIteration 40 || Loss = 0.5278132\nIteration 50 || Loss = 0.4867624\nIteration 60 || Loss = 0.41630346\nIteration 70 || Loss = 0.3325938\nIteration 80 || Loss = 0.28235924\nIteration 90 || Loss = 0.24069068\nIteration 100 || Loss = 0.20503852\nIteration 110 || Loss = 0.17608969\nIteration 120 || Loss = 0.1491399\nIteration 130 || Loss = 0.12711425\nIteration 140 || Loss = 0.10686825\nIteration 150 || Loss = 0.089558244\n\nTraining Augmented Neural ODE\nIteration 10 || Loss = 1.3911372\nIteration 20 || Loss = 0.7694144\nIteration 30 || Loss = 0.5639633\nIteration 40 || Loss = 0.33187616\nIteration 50 || Loss = 0.14787851\nIteration 60 || Loss = 0.094676435\nIteration 70 || Loss = 0.07363529\nIteration 80 || Loss = 0.060333826\nIteration 90 || Loss = 0.04998395\nIteration 100 || Loss = 0.044843454\nIteration 110 || Loss = 0.042587914\nIteration 120 || Loss = 0.042706195\nIteration 130 || Loss = 0.040252227\nIteration 140 || Loss = 0.037686247\nIteration 150 || Loss = 0.036247417","category":"page"},{"location":"examples/augmented_neural_ode/#References","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.","category":"page"},{"location":"utilities/MultipleShooting/#Multiple-Shooting-Functionality","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"","category":"section"},{"location":"utilities/MultipleShooting/","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"multiple_shoot","category":"page"},{"location":"utilities/MultipleShooting/#DiffEqFlux.multiple_shoot","page":"Multiple Shooting Functionality","title":"DiffEqFlux.multiple_shoot","text":"Returns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nmultiple_shoot(p, ode_data, tsteps, prob, loss_function, solver, group_size;\n               continuity_term=100, kwargs...)\nmultiple_shoot(p, ode_data, tsteps, prob, loss_function, continuity_loss, solver, group_size;\n               continuity_term=100, kwargs...)\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data: Original Data to be modelled.\ntsteps: Timesteps on which ode_data was calculated.\nprob: ODE problem that the Neural Network attempts to solve.\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss   between them.   If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout different groups.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and   Common Solver Arguments   documentation for more details. Note: The parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\nReturns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nmultiple_shoot(p, ode_data_ensemble, tsteps, ensemble_prob, ensemble_alg, loss_function, solver,\n                group_size; continuity_term=100, trajectories)\nmultiple_shoot(p, ode_data_ensemble, tsteps, ensemble_prob, ensemble_alg, loss_function,\n                continuity_loss, solver, group_size; continuity_term=100, trajectories)\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data_ensemble: Original Data to be modelled. Batches (or equivalently \"trajectories\") are located in the third dimension.\ntsteps: Timesteps on which ode_data_ensemble was calculated.\nensemble_prob: Ensemble problem that the Neural Network attempts to solve.\nensemble_alg: Ensemble algorithm, e.g. EnsembleThreads()\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss   between them.   If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout\n\ndifferent groups.\n\ntrajectories: number of trajectories for ensemble_prob.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and   Common Solver Arguments   documentation for more details. Note: The parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\n","category":"function"},{"location":"#DiffEqFlux:-High-Level-Pre-Built-Architectures-for-Implicit-Deep-Learning","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux: High Level Pre-Built Architectures for Implicit Deep Learning","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"DiffEqFlux.jl is an implicit deep learning library built using the SciML ecosystem. It is a high-level interface that pulls together all the tools with heuristics and helper functions to make training such deep implicit layer models fast and easy.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"note: Note\nDiffEqFlux.jl is only for pre-built architectures and utility functions for deep implicit learning, mixing differential equations with machine learning. For details on automatic differentiation of equation solvers and adjoint techniques, and using these methods for doing things like calibrating models to data, nonlinear optimal control, and PDE-constrained optimization, see SciMLSensitivity.jl","category":"page"},{"location":"#Pre-Built-Architectures","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The approach of this package is the easy and efficient training of Universal Differential Equations. DiffEqFlux.jl provides architectures which match the interfaces of machine learning libraries such as Flux.jl and Lux.jl to make it easy to build continuous-time machine learning layers into larger machine learning applications.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The following layer functions exist:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Neural Ordinary Differential Equations (Neural ODEs)\nCollocation-Based Neural ODEs (Neural ODEs without a solver, by far the fastest way!)\nMultiple Shooting Neural Ordinary Differential Equations\nNeural Stochastic Differential Equations (Neural SDEs)\nNeural Differential-Algebriac Equations (Neural DAEs)\nNeural Delay Differential Equations (Neural DDEs)\nAugmented Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nContinuous Normalizing Flows (CNF) and FFJORD","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Examples of how to build architectures from scratch, with tutorials on things like Graph Neural ODEs, can be found in the SciMLSensitivity.jl documentation.","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"WIP:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Lagrangian Neural Networks\nGalerkin Neural ODEs","category":"page"},{"location":"#Citation","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Citation","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"#Reproducibility","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using Pkg # hide\nPkg.status(;mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"</details>","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"You can also download the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\",String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\",String))[\"name\"]\nlink = \"https://github.com/SciML/\"*name*\".jl/tree/gh-pages/v\"*version*\"/assets/Project.toml\"","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"\">project</a> file.","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Convolutional-Neural-ODE-MNIST-Classifier-on-GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Training a Convolutional Neural Net Classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"(Step-by-step description below)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend\n\n# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)\n\ndown = Flux.Chain(Flux.Conv((3, 3), 1=>64, relu, stride = 1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, relu, stride = 2, pad=1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Flux.Chain(Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Flux.Chain(Flux.GroupNorm(64, 64), x -> relu.(x), Flux.MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Flux.Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend\n\n# Build our over-all model topology\nmodel = Flux.Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncallback() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = callback)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Step-by-Step-Description","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Step-by-Step Description","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/#Load-Packages","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load Packages","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs","category":"page"},{"location":"examples/mnist_conv_neural_ode/#GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"GPU","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"A good trick used here:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"and then loaded from main:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Layers","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Layers","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down = Flux.Chain(Flux.Conv((3, 3), 1=>64, relu, stride = 1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, relu, stride = 2, pad=1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Flux.Chain(Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Flux.Chain(Flux.GroupNorm(64, 64), x -> relu.(x), Flux.MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Flux.Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down: This layer downsamples our images into 6 x 6 x 64 dimensional features.         It takes a 28 x 28 image, and passes it through a convolutional neural network         layer with relu activation","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn: A 2 layer Convolutional Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn_ode: ODE solver layer","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"fc: The final fully connected layer which maps our learned features to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"gpu: A utility function which transfers our model to GPU, if one is available","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Array-Conversion","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Array Conversion","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"When using NeuralODE, we can use the following function as a cheap conversion of DiffEqArray from the ODE solver into a Matrix that can be used in the following layer:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) with Array(x).","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Build-Topology","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Build Topology","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Build our over-all model topology\nmodel = Flux.Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Flux.Chain(down, nn, fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Prediction","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Prediction","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Accuracy","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Accuracy","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Training-Parameters","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Training Parameters","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Loss","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Loss","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Optimizer","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Optimizer","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"opt = ADAM(0.05)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#CallBack","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"CallBack","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"callback() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Train","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Train","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, callback = callback)","category":"page"},{"location":"examples/mnist_conv_neural_ode/#Expected-Output","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Expected Output","text":"","category":"section"},{"location":"examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Iter:   1 || Train Accuracy: 8.453 || Test Accuracy: 8.883\nIter:  11 || Train Accuracy: 14.773 || Test Accuracy: 14.967\nIter:  21 || Train Accuracy: 24.383 || Test Accuracy: 24.433\nIter:  31 || Train Accuracy: 38.820 || Test Accuracy: 38.000\nIter:  41 || Train Accuracy: 30.852 || Test Accuracy: 31.350\nIter:  51 || Train Accuracy: 29.852 || Test Accuracy: 29.433\nIter:  61 || Train Accuracy: 45.195 || Test Accuracy: 45.217\nIter:  71 || Train Accuracy: 70.336 || Test Accuracy: 68.850\nIter:  81 || Train Accuracy: 76.250 || Test Accuracy: 75.783\nIter:  91 || Train Accuracy: 80.867 || Test Accuracy: 81.017\nIter: 101 || Train Accuracy: 86.398 || Test Accuracy: 85.317\nIter: 111 || Train Accuracy: 90.852 || Test Accuracy: 90.650\nIter: 121 || Train Accuracy: 93.477 || Test Accuracy: 92.550\nIter: 131 || Train Accuracy: 93.320 || Test Accuracy: 92.483\nIter: 141 || Train Accuracy: 94.273 || Test Accuracy: 93.567\nIter: 151 || Train Accuracy: 94.531 || Test Accuracy: 93.583\nIter: 161 || Train Accuracy: 94.992 || Test Accuracy: 94.067\nIter: 171 || Train Accuracy: 95.398 || Test Accuracy: 94.883\nIter: 181 || Train Accuracy: 96.945 || Test Accuracy: 95.633\nIter: 191 || Train Accuracy: 96.430 || Test Accuracy: 95.750\nIter: 201 || Train Accuracy: 96.859 || Test Accuracy: 95.983\nIter: 211 || Train Accuracy: 97.359 || Test Accuracy: 96.500\nIter: 221 || Train Accuracy: 96.586 || Test Accuracy: 96.133\nIter: 231 || Train Accuracy: 96.992 || Test Accuracy: 95.833\nIter: 241 || Train Accuracy: 97.148 || Test Accuracy: 95.950\nIter: 251 || Train Accuracy: 96.422 || Test Accuracy: 95.950\nIter: 261 || Train Accuracy: 96.094 || Test Accuracy: 95.633\nIter: 271 || Train Accuracy: 96.719 || Test Accuracy: 95.767\nIter: 281 || Train Accuracy: 96.719 || Test Accuracy: 96.000\nIter: 291 || Train Accuracy: 96.609 || Test Accuracy: 95.817\nIter: 301 || Train Accuracy: 96.656 || Test Accuracy: 96.033\nIter: 311 || Train Accuracy: 97.594 || Test Accuracy: 96.500\nIter: 321 || Train Accuracy: 97.633 || Test Accuracy: 97.083\nIter: 331 || Train Accuracy: 98.008 || Test Accuracy: 97.067\nIter: 341 || Train Accuracy: 98.070 || Test Accuracy: 97.150\nIter: 351 || Train Accuracy: 97.875 || Test Accuracy: 97.050\nIter: 361 || Train Accuracy: 96.922 || Test Accuracy: 96.500\nIter: 371 || Train Accuracy: 97.188 || Test Accuracy: 96.650\nIter: 381 || Train Accuracy: 97.820 || Test Accuracy: 96.783\nIter: 391 || Train Accuracy: 98.156 || Test Accuracy: 97.567\nIter: 401 || Train Accuracy: 98.250 || Test Accuracy: 97.367\nIter: 411 || Train Accuracy: 97.969 || Test Accuracy: 97.267\nIter: 421 || Train Accuracy: 96.555 || Test Accuracy: 95.667","category":"page"},{"location":"layers/TensorLayer/#Tensor-Product-Layer","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, B_1 B_2  B_n, a data point x, computes zi = Wi  B_1(x1)  B_2(x2)    B_n(xn), where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"layers/TensorLayer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"Constructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nTensorLayer(model,out,p=nothing)\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initialized to standard normal by default.\n\n\n\n\n\n","category":"type"}]
}
