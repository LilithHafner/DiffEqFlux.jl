var documenterSearchIndex = {"docs":
[{"location":"layers/SplineLayer/#Spline-Layer","page":"Spline Layer","title":"Spline Layer","text":"","category":"section"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Constructs a Spline Layer. At a high-level, it performs the following:","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Takes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.","category":"page"},{"location":"layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"SplineLayer","category":"page"},{"location":"layers/SplineLayer/#DiffEqFlux.SplineLayer","page":"Spline Layer","title":"DiffEqFlux.SplineLayer","text":"Constructs a Spline Layer. At a high-level, it performs the following:\n\nTakes as input a one-dimensional training dataset, a time span, a time step and\n\nan interpolation method.\n\nDuring training, adjusts the values of the function at multiples of the time-step\n\nsuch that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.\n\nSplineLayer(time_span,time_step,spline_basis,saved_points=nothing)\n\nArguments:\n\ntime_span: Tuple of real numbers corresponding to the time span.\ntime_step: Real number corresponding to the time step.\nspline_basis: Interpolation method to be used yb the basis (current supported interpolation methods: ConstantInterpolation, LinearInterpolation, QuadraticInterpolation, QuadraticSpline, CubicSpline).\n'saved_points': values of the function at multiples of the time step. Initialized by default\n\nto a random vector sampled from the unit normal.\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#CNF-Layer-Functions","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"DeterministicCNF\nFFJORD\nFFJORDDistribution","category":"page"},{"location":"layers/CNFLayer/#DiffEqFlux.DeterministicCNF","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.DeterministicCNF","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a direct computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\n!!!note     This layer has been deprecated in favour of FFJORD. Use FFJORD with monte_carlo=false instead.\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nDeterministicCNF(model, tspan, basedist=nothing, monte_carlo=false, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a stochastic approach [2] for the computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nFFJORD(model, basedist=nothing, monte_carlo=false, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"layers/CNFLayer/#DiffEqFlux.FFJORDDistribution","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by rand or estimate densities by pdf or logpdf (from Distributions.jl).\n\nArguments:\n\nmodel: A FFJORD instance\nregularize: Whether we use regularization (default: false)\nmonte_carlo: Whether we use monte carlo (default: true)\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#Neural-Differential-Equation-Layer-Functions","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"Constructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nNeuralODE(model,tspan,alg=nothing,args...;kwargs...)\nNeuralODE(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"Constructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nNeuralDSDE(model1,model2,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\nNeuralDSDE(model1::FastChain,model2::FastChain,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"Constructs a neural stochastic differential equation (neural SDE).\n\nNeuralSDE(model1,model2,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralSDE(model1::FastChain,model2::FastChain,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x,1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralCDDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralCDDE","text":"Constructs a neural delay differential equation (neural DDE) with constant delays.\n\nNeuralCDDE(model,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralCDDE(model::FastChain,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size [x;x(t-lag_1);...;x(t-lag_n)] and produce and output shaped like x.\ntspan: The timespan to be solved on.\nhist: Defines the history function h(t) for values before the start of the integration.\nlags: Defines the lagged values that should be utilized in the neural network.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"Constructs a neural differential-algebraic equation (neural DAE).\n\nNeuralDAE(model,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralDAE(model::FastChain,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"Constructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nNeuralODEMM(model,constraints_model,tspan,mass_matrix,alg=nothing,args...;kwargs...)\nNeuralODEMM(model::FastChain,tspan,mass_matrix,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=DiffEqSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nmass_matrix: The mass matrix associated with the DAE\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/NeuralDELayers/#DiffEqFlux.AugmentedNDELayer","page":"Neural Differential Equation Layers","title":"DiffEqFlux.AugmentedNDELayer","text":"Constructs an Augmented Neural Differential Equation Layer.\n\nAugmentedNDELayer(nde, adim::Int)\n\nArguments:\n\nnde: Any Neural Differential Equation Layer\nadim: The number of dimensions the initial conditions should be lifted\n\nReferences:\n\n[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.\n\n\n\n\n\n","category":"type"},{"location":"Collocation/#Smoothed-Collocation","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Smoothed collocation, also referred to as the two-stage method, allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true (u',u) pairs, at which point u'-f(u,p,t) can be directly estimated as a loss to determine the correct parameters p. This method can be extremely fast and robust to noise, though, because it does not accumulate through time, is not as exact as other methods.","category":"page"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"collocate_data","category":"page"},{"location":"Collocation/#DiffEqFlux.collocate_data","page":"Smoothed Collocation","title":"DiffEqFlux.collocate_data","text":"u′,u = collocate_data(data,tpoints,kernel=SigmoidKernel())\nu′,u = collocate_data(data,tpoints,tpoints_sample,interp,args...)\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nFor kernels, the following exist:\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/\n\nAdditionally, we can use interpolation methods from DataInterpolations.jl to generate data from intermediate timesteps. In this case, pass any of the methods like QuadraticInterpolation as interp, and the timestamps to sample from as tpoints_sample.\n\n\n\n\n\n","category":"function"},{"location":"Collocation/#Kernel-Choice","page":"Smoothed Collocation","title":"Kernel Choice","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Note that the kernel choices of DataInterpolations.jl, such as CubicSpline(), are exact, i.e. go through the data points, while the smoothed kernels are regression splines. Thus CubicSpline() is preferred if the data is not too noisy or is relatively sparse. If data is sparse and very noisy, a BSpline()  can be the best regression spline, otherwise one of the other kernels such as as EpanechnikovKernel.","category":"page"},{"location":"Collocation/#Non-Allocating-Forward-Mode-L2-Collocation-Loss","page":"Smoothed Collocation","title":"Non-Allocating Forward-Mode L2 Collocation Loss","text":"","category":"section"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation:","category":"page"},{"location":"Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"using PreallocationTools\ndu = PreallocationTools.dualcache(similar(prob.u0))\npreview_est_sol = [@view estimated_solution[:,i] for i in 1:size(estimated_solution,2)]\npreview_est_deriv = [@view estimated_derivative[:,i] for i in 1:size(estimated_solution,2)]\n\nfunction construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)\n  function (p)\n      _du = PreallocationTools.get_tmp(du,p)\n      vecdu = vec(_du)\n      cost = zero(first(p))\n      for i in 1:length(preview_est_sol)\n        est_sol = preview_est_sol[i]\n        f(_du,est_sol,p,tpoints[i])\n        vecdu .= vec(preview_est_deriv[i]) .- vec(_du)\n        cost += sum(abs2,vecdu)\n      end\n      sqrt(cost)\n  end\nend\ncost_function = construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)","category":"page"},{"location":"Flux/#Use-with-Flux.jl","page":"Use with Flux Chain and train!","title":"Use with Flux.jl","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"All of the tools of DiffEqFlux.jl can be used with Flux.jl. A lot of the examples have been written to use FastChain and sciml_train, but in all cases this can be changed to the Chain and Flux.train! workflow.","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-Flux.train!","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with Flux.train!","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"This should work almost automatically by using solve. Here is an example of optimizing u0 and p.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nfunction predict_n_ode()\r\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\r\nend\r\n\r\nfunction loss_n_ode()\r\n    pred = predict_n_ode()\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss\r\nend\r\n\r\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\r\n\r\ncb = function (;doplot=false) #callback function to observe training\r\n  pred = predict_n_ode()\r\n  display(sum(abs2,ode_data .- pred))\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb()\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = Flux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = cb)","category":"page"},{"location":"Flux/#Using-Flux-Chain-neural-networks-with-sciml_train","page":"Use with Flux Chain and train!","title":"Using Flux Chain neural networks with sciml_train","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"While for simple neural networks we recommend using FastChain-based neural networks for speed and simplicity, Flux neural networks can be used with sciml_train by utilizing the Flux.destructure function. In this case, if dudt is a Flux chain, then:","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"p,re = Flux.destructure(chain)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"returns p which is the vector of parameters for the chain and re which is a function re(p) that reconstructs the neural network with new parameters p. Using this function we can thus build our neural differential equations in an explicit parameter style. For example, the neural ordinary differential equation example written out without using the NeuralODE helper would look like. Notice that in this example we will optimize both the neural network parameters p and the input initial condition u0. Notice that sciml_train works on a vector input, so we have to concatenate u0 and p and then in the loss function split to the pieces.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots\r\n\r\nu0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0,1.5f0)\r\n\r\nfunction trueODEfunc(du,u,p,t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\nt = range(tspan[1],tspan[2],length=datasize)\r\nprob = ODEProblem(trueODEfunc,u0,tspan)\r\node_data = Array(solve(prob,Tsit5(),saveat=t))\r\n\r\ndudt2 = Chain(x -> x.^3,\r\n             Dense(2,50,tanh),\r\n             Dense(50,2))\r\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\r\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\r\nprob = ODEProblem(dudt,u0,tspan)\r\n\r\nθ = [u0;p] # the parameter vector to optimize\r\n\r\nfunction predict_n_ode(θ)\r\n  Array(solve(prob,Tsit5(),u0=θ[1:2],p=θ[3:end],saveat=t))\r\nend\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2,ode_data .- pred)\r\n    loss,pred\r\nend\r\n\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ,l,pred;doplot=false) #callback function to observe training\r\n  display(l)\r\n  # plot current prediction against data\r\n  pl = scatter(t,ode_data[1,:],label=\"data\")\r\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\r\n  display(plot(pl))\r\n  return false\r\nend\r\n\r\n# Display the ODE with the initial parameter values.\r\ncb(θ,loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05), cb = cb, maxiters=100)\r\ncb(res1.minimizer,loss_n_ode(res1.minimizer)...;doplot=true)\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(), cb = cb)\r\ncb(res2.minimizer,loss_n_ode(res2.minimizer)...;doplot=true)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Notice that the advantage of this format is that we can use Optim's optimizers, like LBFGS with a full Chain object for all of Flux's neural networks, like convolutional neural networks.","category":"page"},{"location":"Flux/#Using-ComponentArrays-for-neural-network-layers","page":"Use with Flux Chain and train!","title":"Using ComponentArrays for neural network layers","text":"","category":"section"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"We can also create the dense layers from scratch using ComponentArrays.jl. Flux is used here just for the glorot_uniform function and the ADAM optimizer.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"using ComponentArrays, DiffEqFlux, Optim, OrdinaryDiffEq, Plots, UnPack\r\nusing Flux: glorot_uniform, ADAM","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Again, let's create the truth data.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"u0 = Float32[2.; 0.]\r\ndatasize = 30\r\ntspan = (0.0f0, 1.5f0)\r\n\r\nfunction trueODEfunc(du, u, p, t)\r\n    true_A = [-0.1 2.0; -2.0 -0.1]\r\n    du .= ((u.^3)'true_A)'\r\nend\r\n\r\nt = range(tspan[1], tspan[2], length = datasize)\r\nprob = ODEProblem(trueODEfunc, u0, tspan)\r\node_data = Array(solve(prob, Tsit5(), saveat = t))","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Now, we'll make a function that creates dense neural layer components. It is similar to Flux.Dense, except it doesn't handle the activation function. We'll do that separately.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"dense_layer(in, out) = ComponentArray{Float32}(weight=glorot_uniform(out, in), bias=zeros(out))","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"Our parameter vector will be a ComponentArray that holds the ODE initial conditions and the dense neural layers. This enables it to pass through the solver as a flat array while giving us the convenience of struct-like access to the components.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"layers = (L1=dense_layer(2, 50), L2=dense_layer(50, 2))\r\nθ = ComponentArray(u=u0, p=layers)\r\n\r\nfunction dudt(u, p, t)\r\n    @unpack L1, L2 = p\r\n    return L2.weight * tanh.(L1.weight * u.^3 .+ L1.bias) .+ L2.bias\r\nend\r\n\r\nprob = ODEProblem(dudt, u0, tspan)","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"As before, we'll define prediction and loss functions as well as a callback function to observe training.","category":"page"},{"location":"Flux/","page":"Use with Flux Chain and train!","title":"Use with Flux Chain and train!","text":"predict_n_ode(θ) = Array(solve(prob, Tsit5(), u0=θ.u, p=θ.p, saveat=t))\r\n\r\nfunction loss_n_ode(θ)\r\n    pred = predict_n_ode(θ)\r\n    loss = sum(abs2, ode_data .- pred)\r\n    return loss, pred\r\nend\r\nloss_n_ode(θ)\r\n\r\ncb = function (θ, loss, pred; doplot=false)\r\n    display(loss)\r\n    # plot current prediction against data\r\n    pl = scatter(t, ode_data[1,:], label = \"data\")\r\n    scatter!(pl, t, pred[1,:], label = \"prediction\")\r\n    display(plot(pl))\r\n    return false\r\nend\r\n\r\ncb(θ, loss_n_ode(θ)...)\r\n\r\ndata = Iterators.repeated((), 1000)\r\n\r\nres1 = DiffEqFlux.sciml_train(loss_n_ode, θ, ADAM(0.05); cb=cb, maxiters=100)\r\ncb(res1.minimizer, loss_n_ode(res1.minimizer)...; doplot=true)\r\n\r\nres2 = DiffEqFlux.sciml_train(loss_n_ode, res1.minimizer, LBFGS(); cb=cb)\r\ncb(res2.minimizer, loss_n_ode(res2.minimizer)...; doplot=true)","category":"page"},{"location":"examples/tensor_layer/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"x = - kx - αx³ - βx -γx³","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in DifferentialEquations:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using DiffEqFlux, DifferentialEquations, LinearAlgebra\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0,10.0)\n\nfunction dxdt_train(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\nend\n\nu0 = [1.0,0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train,u0,tspan,p=nothing)\ndata_train = Array(solve(prob_train,Tsit5(),saveat=ts))","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we also instantiate the model we are trying to learn, \"informing\" the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"f = x -> min(30one(x),x)\n\nfunction dxdt_pred(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\nend\n\nα = zeros(102)\n\nprob_pred = ODEProblem{true}(dxdt_pred,u0,tspan,p=nothing)","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Note that we introduced a \"cap\" in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"\nfunction predict_adjoint(θ)\n  x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts))\nend\n\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  loss = sum(norm.(x - data_train))\n  return loss\nend\n\nfunction cb(θ,l)\n  @show θ, l\n  return false\nend","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we train the network using two rounds of ADAM:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"res1 = DiffEqFlux.sciml_train(loss_adjoint, α, ADAM(0.05), cb = cb, maxiters = 150)\nres2 = DiffEqFlux.sciml_train(loss_adjoint, res1.u, ADAM(0.001), cb = cb,maxiters = 150)\nopt = res2.u","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"We plot the results and we obtain a fairly accurate learned model:","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(opt)\nplot(ts, data_train[1,:], label = \"X (ODE)\")\nplot!(ts, data_train[2,:], label = \"V (ODE)\")\nplot!(ts, data_pred[1,:], label = \"X (NN)\")\nplot!(ts, data_pred[2,:],label = \"V (NN)\")","category":"page"},{"location":"examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"examples/multiple_shooting/#Multiple-Shooting","page":"Multiple Shooting","title":"Multiple Shooting","text":"","category":"section"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"In Multiple Shooting, the training data is split into overlapping intervals. The solver is then trained on individual intervals. If the end conditions of any interval coincide with the initial conditions of the next immediate interval, then the joined/combined solution is same as solving on the whole dataset (without splitting).","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"To ensure that the overlapping part of two consecutive intervals coincide, we add a penalizing term, continuity_term * absolute_value_of(prediction of last point of group i - prediction of first point of group i+1), to the loss.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Note that the continuity_term should have a large positive value to add high penalties in case the solver predicts discontinuous values.","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The following is a working demo, using Multiple Shooting","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"using DiffEqFlux, DifferentialEquations, Plots\nusing DiffEqFlux: group_ranges\n\n# Define initial conditions and time steps\ndatasize = 30\nu0 = Float32[2.0, 0.0]\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\n\n# Get the data\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\n# Define the Neural Network\nnn = FastChain((x, p) -> x.^3,\n                  FastDense(2, 16, tanh),\n                  FastDense(16, 2))\np_init = initial_params(nn)\n\nneuralode = NeuralODE(nn, tspan, Tsit5(), saveat = tsteps)\nprob_node = ODEProblem((u,p,t)->nn(u,p), u0, tspan, p_init)\n\n\nfunction plot_multiple_shoot(plt, preds, group_size)\n\tstep = group_size-1\n\tranges = group_ranges(datasize, group_size)\n\n\tfor (i, rg) in enumerate(ranges)\n\t\tplot!(plt, tsteps[rg], preds[i][1,:], markershape=:circle, label=\"Group $(i)\")\n\tend\nend\n\n# Animate training\nanim = Animation()\ncallback = function (p, l, preds; doplot = true)\n  display(l)\n  if doplot\n\t# plot the original data\n\tplt = scatter(tsteps, ode_data[1,:], label = \"Data\")\n\n\t# plot the different predictions for individual shoot\n\tplot_multiple_shoot(plt, preds, group_size)\n\n    frame(anim)\n    display(plot(plt))\n  end\n  return false\nend\n\n# Define parameters for Multiple Shooting\ngroup_size = 3\ncontinuity_term = 200\n\nfunction loss_function(data, pred)\n\treturn sum(abs2, data - pred)\nend\n\nfunction loss_multiple_shooting(p)\n    return multiple_shoot(p, ode_data, tsteps, prob_node, loss_function, Tsit5(),\n                          group_size; continuity_term)\nend\n\nres_ms = DiffEqFlux.sciml_train(loss_multiple_shooting, p_init,\n                                cb = callback)\ngif(anim, \"multiple_shooting.gif\", fps=15)\n","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here's the animation that we get from above","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic) The connected lines show the predictions of each group (Notice that there are overlapping points as well. These are the points we are trying to coincide.)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here is an output with group_size = 30 (which is same as solving on the whole interval without splitting also called single shooting)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic_single_shoot3)","category":"page"},{"location":"examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"It is clear from the above picture, a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minima.","category":"page"},{"location":"examples/normalizing_flows/#Continuous-Normalizing-Flows-with-GalacticOptim.jl","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"examples/normalizing_flows/#Copy-Pasteable-Code","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, GalacticOptim, Distributions\n\nnn = Chain(\n    Dense(1, 3, tanh),\n    Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\nffjord_mdl = FFJORD(nn, tspan, Tsit5())\n\n# Training\ndata_dist = Normal(6.0f0, 0.7f0)\ntrain_data = rand(data_dist, 1, 100)\n\nfunction loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend\n\nadtype = GalacticOptim.AutoZygote()\nres1 = DiffEqFlux.sciml_train(loss, ffjord_mdl.p, ADAM(0.1), adtype; maxiters=100)\nres2 = DiffEqFlux.sciml_train(loss, res1.u, LBFGS(), adtype; allow_f_increases=false)\n\n# Evaluation\nusing Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)\n\n# Data Generation\nffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#Step-by-Step-Explanation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be:","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using DiffEqFlux, DifferentialEquations, GalacticOptim, Distributions\n\nnn = Chain(\n    Dense(1, 3, tanh),\n    Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\nffjord_mdl = FFJORD(nn, tspan, Tsit5())","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"examples/normalizing_flows/#Training","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Training","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"First, let's get an array from a normal distribution as the training data","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"data_dist = Normal(6.0f0, 0.7f0)\ntrain_data = rand(data_dist, 1, 100)","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Now we define a loss function that we wish to minimize","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"function loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"adtype = GalacticOptim.AutoZygote()\nres1 = DiffEqFlux.sciml_train(loss, ffjord_mdl.p, ADAM(0.1), adtype; maxiters=100)\n\n# output\n* Status: success\n\n* Candidate solution\n   u: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n   Minimum:   1.240627e+00\n\n* Found with\n   Algorithm:     ADAM\n   Initial Point: [9.33e-01, 1.13e+00, 2.92e-01,  ...]\n","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"We then complete the training using a different optimizer starting from where ADAM stopped.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"res2 = DiffEqFlux.sciml_train(loss, res1.u, LBFGS(), adtype; allow_f_increases=false)\n\n# output\n* Status: success\n\n* Candidate solution\n   u: [-1.06e+00, 2.24e+00, 8.77e-01,  ...]\n   Minimum:   1.157672e+00\n\n* Found with\n   Algorithm:     L-BFGS\n   Initial Point: [-1.88e+00, 2.44e+00, 2.01e-01,  ...]\n\n* Convergence measures\n   |x - x'|               = 0.00e+00 ≰ 0.0e+00\n   |x - x'|/|x'|          = 0.00e+00 ≰ 0.0e+00\n   |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n   |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n   |g(x)|                 = 4.09e-03 ≰ 1.0e-08\n\n* Work counters\n   Seconds run:   514  (vs limit Inf)\n   Iterations:    44\n   f(x) calls:    244\n   ∇f(x) calls:   244","category":"page"},{"location":"examples/normalizing_flows/#Evaluation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Evaluation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"For evaluating the result, we can use totalvariation function from Distances.jl. First, we compute densities using actual distribution and FFJORD model. then we use a distance function.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"using Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)","category":"page"},{"location":"examples/normalizing_flows/#Data-Generation","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Data Generation","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"What's more, we can generate new data by using FFJORD as a distribution in rand.","category":"page"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"ffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"examples/normalizing_flows/#References","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"References","text":"","category":"section"},{"location":"examples/normalizing_flows/","page":"Continuous Normalizing Flows with GalacticOptim.jl","title":"Continuous Normalizing Flows with GalacticOptim.jl","text":"[1] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).","category":"page"},{"location":"examples/collocation/#Smoothed-Collocation-for-Fast-Two-Stage-Training","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data. First the example and then an explanation.","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)\nsavefig(\"colloc.png\")\nplot(tsteps,du')\nsavefig(\"colloc_du.png\")\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du = dudt2(@view(u[:,i]),p)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit = initial_params(dudt2)\ncallback = function (p, l)\n  return false\nend\n\nresult_neuralode = DiffEqFlux.sciml_train(loss, pinit,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol = prob_neuralode(u0, result_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol)\nsavefig(\"colloc_trained.png\")\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\n@time numerical_neuralode = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.u,\n                                                ADAM(0.05), cb = callback,\n                                                maxiters = 300)\n\nnn_sol = prob_neuralode(u0, numerical_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)\nsavefig(\"post_trained.png\")","category":"page"},{"location":"examples/collocation/#Generating-the-Collocation","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Generating the Collocation","text":"","category":"section"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"The smoothed collocation is a spline fit of the datapoints which allows us to get a an estimate of the approximate noiseless dynamics:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using DiffEqFlux, DifferentialEquations, Plots\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"We can then differentiate the smoothed function to get estimates of the derivative at each datapoint:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"plot(tsteps,du')","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Because we have (u',u) pairs, we can write a loss function that calculates the squared difference between f(u,p,t) and u' at each point, and find the parameters which minimize this difference:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"dudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du = dudt2(@view(u[:,i]),p)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit = initial_params(dudt2)\ncallback = function (p, l)\n  return false\nend\n\nresult_neuralode = DiffEqFlux.sciml_train(loss, pinit,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol = prob_neuralode(u0, result_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"While this doesn't look great, it has the characteristics of the full solution all throughout the timeseries, but it does have a drift. We can continue to optimize like this, or we can use this as the initial condition to the next phase of our fitting:","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p))\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\n@time numerical_neuralode = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.u,\n                                                ADAM(0.05), cb = callback,\n                                                maxiters = 300)\n\nnn_sol = prob_neuralode(u0, numerical_neuralode.u)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"This method then has a good global starting position, making it less prone to local minima and is thus a great method to mix in with other fitting methods for neural ODEs.","category":"page"},{"location":"examples/hamiltonian_nn/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Networks introduced in [1] allow models to \"learn and respect exact conservation laws in an unsupervised manner\". In this example, we will train a model to learn the Hamiltonian for a 1D Spring mass system. This system is described by the equation:","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"mddot(x) + kx = 0","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Now we make some simplifying assumptions, and assign m = 1 and k = 1. Analytically solving this equation, we get x = sin(t). Hence, q = sin(t), and p = cos(t). Using these solutions we generate our dataset and fit the NeuralHamiltonianDE to learn the dynamics of this system.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using DiffEqFlux, DifferentialEquations, Statistics, Plots\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader(data, target; batchsize=256, shuffle=true)\n\nhnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()\n\nmodel = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/#Step-by-Step-Explanation","page":"Hamiltonian Neural Network","title":"Step by Step Explanation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/#Data-Generation","page":"Hamiltonian Neural Network","title":"Data Generation","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"The HNN predicts the gradients (dot(q) dot(p)) given (q p). Hence, we generate the pairs (q p) using the equations given at the top. Additionally to supervise the training we also generate the gradients. Next we use use Flux DataLoader for automatically batching our dataset.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"t = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader(data, target; batchsize=256, shuffle=true)","category":"page"},{"location":"examples/hamiltonian_nn/#Training-the-HamiltonianNN","page":"Hamiltonian Neural Network","title":"Training the HamiltonianNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We parameterize the HamiltonianNN with a small MultiLayered Perceptron (HNN also works with the Fast* Layers provided in DiffEqFlux). HNNs are trained by optimizing the gradients of the Neural Network. Zygote currently doesn't support nesting itself, so we will be using ReverseDiff in the training loop to compute the gradients of the HNN Layer for Optimization.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"hnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()","category":"page"},{"location":"examples/hamiltonian_nn/#Solving-the-ODE-using-trained-HNN","page":"Hamiltonian Neural Network","title":"Solving the ODE using trained HNN","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In order to visualize the learned trajectories, we need to solve the ODE. We will use the NeuralHamiltonianDE layer which is essentially a wrapper over HamiltonianNN layer and solves the ODE.","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"model = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"(Image: HNN Prediction)","category":"page"},{"location":"examples/hamiltonian_nn/#Expected-Output","page":"Hamiltonian Neural Network","title":"Expected Output","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Loss Neural Hamiltonian DE = 18.768814\nLoss Neural Hamiltonian DE = 0.022630047\nLoss Neural Hamiltonian DE = 0.015060622\nLoss Neural Hamiltonian DE = 0.013170851\nLoss Neural Hamiltonian DE = 0.011898238\nLoss Neural Hamiltonian DE = 0.009806873","category":"page"},{"location":"examples/hamiltonian_nn/#References","page":"Hamiltonian Neural Network","title":"References","text":"","category":"section"},{"location":"examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.","category":"page"},{"location":"layers/HamiltonianNN/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system.","category":"page"},{"location":"layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"HamiltonianNN\nNeuralHamiltonianDE","category":"page"},{"location":"layers/HamiltonianNN/#DiffEqFlux.HamiltonianNN","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.HamiltonianNN","text":"Constructs a Hamiltonian Neural Network [1]. This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories. It takes as input a concatenated vector of length 2n containing the position (of size n) and momentum (of size n) of the particles. It then returns the time derivatives for position and momentum.\n\nnote: Note\nThis doesn't solve the Hamiltonian Problem. Use NeuralHamiltonianDE for such applications.\n\nnote: Note\nThis layer currently doesn't support GPU. The support will be added in future with some AD fixes.\n\nTo obtain the gradients to train this network, ReverseDiff.gradient is supposed to be used. This prevents the usage of DiffEqFlux.sciml_train or Flux.train. Follow this tutorial to see how to define a training loop to circumvent this issue.\n\nHamiltonianNN(model; p = nothing)\nHamiltonianNN(model::FastChain; p = initial_params(model))\n\nArguments:\n\nmodel: A Chain or FastChain neural network that returns the Hamiltonian of the          system.\np: The initial parameters of the neural network.\n\nReferences:\n\n[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.\n\n\n\n\n\n","category":"type"},{"location":"layers/HamiltonianNN/#DiffEqFlux.NeuralHamiltonianDE","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.NeuralHamiltonianDE","text":"Contructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network HamiltonianNN.\n\nNeuralHamiltonianDE(model, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain, FastChain or Hamiltonian Neural Network that predicts the          Hamiltonian of the system.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the           Common Solver Arguments           documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#Classical-Basis-Layers","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"","category":"section"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"layers/BasisLayers/#DiffEqFlux.ChebyshevBasis","page":"Classical Basis Layers","title":"DiffEqFlux.ChebyshevBasis","text":"Constructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nChebyshevBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.SinBasis","page":"Classical Basis Layers","title":"DiffEqFlux.SinBasis","text":"Constructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nSinBasis(n)\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.CosBasis","page":"Classical Basis Layers","title":"DiffEqFlux.CosBasis","text":"Constructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nCosBasis(n)\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.FourierBasis","page":"Classical Basis Layers","title":"DiffEqFlux.FourierBasis","text":"Constructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nFourierBasis(n)\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.LegendreBasis","page":"Classical Basis Layers","title":"DiffEqFlux.LegendreBasis","text":"Constructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nLegendreBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"layers/BasisLayers/#DiffEqFlux.PolynomialBasis","page":"Classical Basis Layers","title":"DiffEqFlux.PolynomialBasis","text":"Constructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nPolynomialBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Copy-Pasteable-Code","page":"Augmented Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend\n\ndiffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend\n\nfunction plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend\n\nloss_node(x, y) = mean((model(x) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)\n\ncb = function()\n    global iter += 1\n    if iter % 10 == 0\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend\n\nmodel, parameters = construct_model(1, 2, 64, 0)\nopt = ADAM(0.005)\niter = 0\n\nprintln(\"Training Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_node = plot_contour(model)\n\nmodel, parameters = construct_model(1, 2, 64, 1)\nopt = ADAM(0.005)\niter = 0\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([parameters, model]), dataloader, opt, cb = cb)\nend\n\nplt_anode = plot_contour(model)","category":"page"},{"location":"examples/augmented_neural_ode/#Step-by-Step-Explanation","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loading-required-packages","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader","category":"page"},{"location":"examples/augmented_neural_ode/#Generating-a-toy-dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader(data |> gpu, labels |> gpu; batchsize=batch_size, shuffle=true,\n                      partial=false)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Models","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tutorial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on GPU, we need to manually transfer the models to GPU. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : (AugmentedNDELayer(node, augment_dim) |> gpu)\n    return Chain((x, p=node.p) -> node(x, p),\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Plotting-the-Results","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define an utility to plot our model regression results as a heatmap.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Training-Parameters","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"examples/augmented_neural_ode/#Loss-Functions","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(x, y) = mean((model(x) .- y) .^ 2)","category":"page"},{"location":"examples/augmented_neural_ode/#Dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)","category":"page"},{"location":"examples/augmented_neural_ode/#Callback-Function","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"cb = function()\n    global iter += 1\n    if iter % 10 == 1\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend","category":"page"},{"location":"examples/augmented_neural_ode/#Optimizer","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use ADAM as the optimizer with a learning rate of 0.005","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = ADAM(0.005)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which is returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 0)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be same as that of Neural ODE. Only in this case we have augmented the input with a single zero. This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs please refer to [1].","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 1)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params([model, parameters]), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"examples/augmented_neural_ode/#Expected-Output","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\nTraining Neural ODE\nIteration 10 || Loss = 0.9802582\nIteration 20 || Loss = 0.6727416\nIteration 30 || Loss = 0.5862373\nIteration 40 || Loss = 0.5278132\nIteration 50 || Loss = 0.4867624\nIteration 60 || Loss = 0.41630346\nIteration 70 || Loss = 0.3325938\nIteration 80 || Loss = 0.28235924\nIteration 90 || Loss = 0.24069068\nIteration 100 || Loss = 0.20503852\nIteration 110 || Loss = 0.17608969\nIteration 120 || Loss = 0.1491399\nIteration 130 || Loss = 0.12711425\nIteration 140 || Loss = 0.10686825\nIteration 150 || Loss = 0.089558244\n\nTraining Augmented Neural ODE\nIteration 10 || Loss = 1.3911372\nIteration 20 || Loss = 0.7694144\nIteration 30 || Loss = 0.5639633\nIteration 40 || Loss = 0.33187616\nIteration 50 || Loss = 0.14787851\nIteration 60 || Loss = 0.094676435\nIteration 70 || Loss = 0.07363529\nIteration 80 || Loss = 0.060333826\nIteration 90 || Loss = 0.04998395\nIteration 100 || Loss = 0.044843454\nIteration 110 || Loss = 0.042587914\nIteration 120 || Loss = 0.042706195\nIteration 130 || Loss = 0.040252227\nIteration 140 || Loss = 0.037686247\nIteration 150 || Loss = 0.036247417","category":"page"},{"location":"examples/augmented_neural_ode/#References","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.","category":"page"},{"location":"#DiffEqFlux:-High-Level-Scientific-Machine-Learning-(SciML)-Pre-Built-Architectures","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"DiffEqFlux.jl is a parameter estimation system for the SciML ecosystem. It is a high level interface that pulls together all of the tools with heuristics and helper functions to make solving inverse problems and inferring models as easy as possible without losing efficiency.","category":"page"},{"location":"#Applications","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Applications","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The approach of this package is the efficient training of Universal Differential Equations. Since this is a fairly general class of problems, the following applications are readily available as specific instances of this methodology, and are showcased in tutorials and layer functions:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Neural ODEs\nNeural SDEs\nNeural DAEs\nNeural DDEs\nAugmented Neural ODEs\nGraph Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nLagrangian Neural Networks\nContinuous Normalizing Flows (CNF) and FFJORD\nGalerkin Neural ODEs","category":"page"},{"location":"#Citation","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Citation","text":"","category":"section"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"layers/TensorLayer/#Tensor-Product-Layer","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, B_1 B_2  B_n, a data point x, computes zi = Wi  B_1(x1)  B_2(x2)    B_n(xn), where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"layers/TensorLayer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"Constructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nTensorLayer(model,out,p=nothing)\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initialized to standard normal by default.\n\n\n\n\n\n","category":"type"}]
}
